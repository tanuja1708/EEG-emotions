{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMd+D2HhmLxRxX7WaVRBkkD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanuja1708/EEG-emotions/blob/main/existed%20system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mspca"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX_KKs5q9ghK",
        "outputId": "219c104c-9927-4ef3-f970-cba7ee6b2fc5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mspca\n",
            "  Downloading mspca-0.0.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting PyWavelets (from mspca)\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets->mspca) (2.0.2)\n",
            "Downloading mspca-0.0.4-py3-none-any.whl (6.0 kB)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets, mspca\n",
            "Successfully installed PyWavelets-1.8.0 mspca-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5QcDxRM82N0",
        "outputId": "0bcd8851-539e-48ec-a4cb-efd95b1cb80c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Found files: ['1_20160518.mat', '14_20151205.mat', '5_20160406.mat', '6_20150507.mat', '8_20151103.mat', '13_20151115.mat', '2_20150915.mat', '4_20151111.mat', '11_20150916.mat', '10_20151014.mat', '15_20150508.mat', '9_20151028.mat', '12_20150725.mat', '3_20150919.mat', '7_20150715.mat']\n",
            "Processing 1_20160518.mat → cz_eeg1\n",
            "Processing 1_20160518.mat → cz_eeg2\n",
            "Processing 1_20160518.mat → cz_eeg3\n",
            "Processing 1_20160518.mat → cz_eeg4\n",
            "Processing 1_20160518.mat → cz_eeg5\n",
            "Processing 1_20160518.mat → cz_eeg6\n",
            "Processing 1_20160518.mat → cz_eeg7\n",
            "Processing 1_20160518.mat → cz_eeg8\n",
            "Processing 1_20160518.mat → cz_eeg9\n",
            "Processing 1_20160518.mat → cz_eeg10\n",
            "Processing 1_20160518.mat → cz_eeg11\n",
            "Processing 1_20160518.mat → cz_eeg12\n",
            "Processing 1_20160518.mat → cz_eeg13\n",
            "Processing 1_20160518.mat → cz_eeg14\n",
            "Processing 1_20160518.mat → cz_eeg15\n",
            "Processing 1_20160518.mat → cz_eeg16\n",
            "Processing 1_20160518.mat → cz_eeg17\n",
            "Processing 1_20160518.mat → cz_eeg18\n",
            "Processing 1_20160518.mat → cz_eeg19\n",
            "Processing 1_20160518.mat → cz_eeg20\n",
            "Processing 1_20160518.mat → cz_eeg21\n",
            "Processing 1_20160518.mat → cz_eeg22\n",
            "Processing 1_20160518.mat → cz_eeg23\n",
            "Processing 1_20160518.mat → cz_eeg24\n",
            "Saved locally: 1_20160518_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/1_20160518_MSPCA.npy\n",
            "Processing 14_20151205.mat → zjd_eeg1\n",
            "Processing 14_20151205.mat → zjd_eeg2\n",
            "Processing 14_20151205.mat → zjd_eeg3\n",
            "Processing 14_20151205.mat → zjd_eeg4\n",
            "Processing 14_20151205.mat → zjd_eeg5\n",
            "Processing 14_20151205.mat → zjd_eeg6\n",
            "Processing 14_20151205.mat → zjd_eeg7\n",
            "Processing 14_20151205.mat → zjd_eeg8\n",
            "Processing 14_20151205.mat → zjd_eeg9\n",
            "Processing 14_20151205.mat → zjd_eeg10\n",
            "Processing 14_20151205.mat → zjd_eeg11\n",
            "Processing 14_20151205.mat → zjd_eeg12\n",
            "Processing 14_20151205.mat → zjd_eeg13\n",
            "Processing 14_20151205.mat → zjd_eeg14\n",
            "Processing 14_20151205.mat → zjd_eeg15\n",
            "Processing 14_20151205.mat → zjd_eeg16\n",
            "Processing 14_20151205.mat → zjd_eeg17\n",
            "Processing 14_20151205.mat → zjd_eeg18\n",
            "Processing 14_20151205.mat → zjd_eeg19\n",
            "Processing 14_20151205.mat → zjd_eeg20\n",
            "Processing 14_20151205.mat → zjd_eeg21\n",
            "Processing 14_20151205.mat → zjd_eeg22\n",
            "Processing 14_20151205.mat → zjd_eeg23\n",
            "Processing 14_20151205.mat → zjd_eeg24\n",
            "Saved locally: 14_20151205_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/14_20151205_MSPCA.npy\n",
            "Processing 5_20160406.mat → ly_eeg1\n",
            "Processing 5_20160406.mat → ly_eeg2\n",
            "Processing 5_20160406.mat → ly_eeg3\n",
            "Processing 5_20160406.mat → ly_eeg4\n",
            "Processing 5_20160406.mat → ly_eeg5\n",
            "Processing 5_20160406.mat → ly_eeg6\n",
            "Processing 5_20160406.mat → ly_eeg7\n",
            "Processing 5_20160406.mat → ly_eeg8\n",
            "Processing 5_20160406.mat → ly_eeg9\n",
            "Processing 5_20160406.mat → ly_eeg10\n",
            "Processing 5_20160406.mat → ly_eeg11\n",
            "Processing 5_20160406.mat → ly_eeg12\n",
            "Processing 5_20160406.mat → ly_eeg13\n",
            "Processing 5_20160406.mat → ly_eeg14\n",
            "Processing 5_20160406.mat → ly_eeg15\n",
            "Processing 5_20160406.mat → ly_eeg16\n",
            "Processing 5_20160406.mat → ly_eeg17\n",
            "Processing 5_20160406.mat → ly_eeg18\n",
            "Processing 5_20160406.mat → ly_eeg19\n",
            "Processing 5_20160406.mat → ly_eeg20\n",
            "Processing 5_20160406.mat → ly_eeg21\n",
            "Processing 5_20160406.mat → ly_eeg22\n",
            "Processing 5_20160406.mat → ly_eeg23\n",
            "Processing 5_20160406.mat → ly_eeg24\n",
            "Saved locally: 5_20160406_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/5_20160406_MSPCA.npy\n",
            "Processing 6_20150507.mat → mhw_eeg1\n",
            "Processing 6_20150507.mat → mhw_eeg2\n",
            "Processing 6_20150507.mat → mhw_eeg3\n",
            "Processing 6_20150507.mat → mhw_eeg4\n",
            "Processing 6_20150507.mat → mhw_eeg5\n",
            "Processing 6_20150507.mat → mhw_eeg6\n",
            "Processing 6_20150507.mat → mhw_eeg7\n",
            "Processing 6_20150507.mat → mhw_eeg8\n",
            "Processing 6_20150507.mat → mhw_eeg9\n",
            "Processing 6_20150507.mat → mhw_eeg10\n",
            "Processing 6_20150507.mat → mhw_eeg11\n",
            "Processing 6_20150507.mat → mhw_eeg12\n",
            "Processing 6_20150507.mat → mhw_eeg13\n",
            "Processing 6_20150507.mat → mhw_eeg14\n",
            "Processing 6_20150507.mat → mhw_eeg15\n",
            "Processing 6_20150507.mat → mhw_eeg16\n",
            "Processing 6_20150507.mat → mhw_eeg17\n",
            "Processing 6_20150507.mat → mhw_eeg18\n",
            "Processing 6_20150507.mat → mhw_eeg19\n",
            "Processing 6_20150507.mat → mhw_eeg20\n",
            "Processing 6_20150507.mat → mhw_eeg21\n",
            "Processing 6_20150507.mat → mhw_eeg22\n",
            "Processing 6_20150507.mat → mhw_eeg23\n",
            "Processing 6_20150507.mat → mhw_eeg24\n",
            "Saved locally: 6_20150507_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/6_20150507_MSPCA.npy\n",
            "Processing 8_20151103.mat → qyt_eeg1\n",
            "Processing 8_20151103.mat → qyt_eeg2\n",
            "Processing 8_20151103.mat → qyt_eeg3\n",
            "Processing 8_20151103.mat → qyt_eeg4\n",
            "Processing 8_20151103.mat → qyt_eeg5\n",
            "Processing 8_20151103.mat → qyt_eeg6\n",
            "Processing 8_20151103.mat → qyt_eeg7\n",
            "Processing 8_20151103.mat → qyt_eeg8\n",
            "Processing 8_20151103.mat → qyt_eeg9\n",
            "Processing 8_20151103.mat → qyt_eeg10\n",
            "Processing 8_20151103.mat → qyt_eeg11\n",
            "Processing 8_20151103.mat → qyt_eeg12\n",
            "Processing 8_20151103.mat → qyt_eeg13\n",
            "Processing 8_20151103.mat → qyt_eeg14\n",
            "Processing 8_20151103.mat → qyt_eeg15\n",
            "Processing 8_20151103.mat → qyt_eeg16\n",
            "Processing 8_20151103.mat → qyt_eeg17\n",
            "Processing 8_20151103.mat → qyt_eeg18\n",
            "Processing 8_20151103.mat → qyt_eeg19\n",
            "Processing 8_20151103.mat → qyt_eeg20\n",
            "Processing 8_20151103.mat → qyt_eeg21\n",
            "Processing 8_20151103.mat → qyt_eeg22\n",
            "Processing 8_20151103.mat → qyt_eeg23\n",
            "Processing 8_20151103.mat → qyt_eeg24\n",
            "Saved locally: 8_20151103_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/8_20151103_MSPCA.npy\n",
            "Processing 13_20151115.mat → wq_eeg1\n",
            "Processing 13_20151115.mat → wq_eeg2\n",
            "Processing 13_20151115.mat → wq_eeg3\n",
            "Processing 13_20151115.mat → wq_eeg4\n",
            "Processing 13_20151115.mat → wq_eeg5\n",
            "Processing 13_20151115.mat → wq_eeg6\n",
            "Processing 13_20151115.mat → wq_eeg7\n",
            "Processing 13_20151115.mat → wq_eeg8\n",
            "Processing 13_20151115.mat → wq_eeg9\n",
            "Processing 13_20151115.mat → wq_eeg10\n",
            "Processing 13_20151115.mat → wq_eeg11\n",
            "Processing 13_20151115.mat → wq_eeg12\n",
            "Processing 13_20151115.mat → wq_eeg13\n",
            "Processing 13_20151115.mat → wq_eeg14\n",
            "Processing 13_20151115.mat → wq_eeg15\n",
            "Processing 13_20151115.mat → wq_eeg16\n",
            "Processing 13_20151115.mat → wq_eeg17\n",
            "Processing 13_20151115.mat → wq_eeg18\n",
            "Processing 13_20151115.mat → wq_eeg19\n",
            "Processing 13_20151115.mat → wq_eeg20\n",
            "Processing 13_20151115.mat → wq_eeg21\n",
            "Processing 13_20151115.mat → wq_eeg22\n",
            "Processing 13_20151115.mat → wq_eeg23\n",
            "Processing 13_20151115.mat → wq_eeg24\n",
            "Saved locally: 13_20151115_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/13_20151115_MSPCA.npy\n",
            "Processing 2_20150915.mat → ha_eeg1\n",
            "Processing 2_20150915.mat → ha_eeg2\n",
            "Processing 2_20150915.mat → ha_eeg3\n",
            "Processing 2_20150915.mat → ha_eeg4\n",
            "Processing 2_20150915.mat → ha_eeg5\n",
            "Processing 2_20150915.mat → ha_eeg6\n",
            "Processing 2_20150915.mat → ha_eeg7\n",
            "Processing 2_20150915.mat → ha_eeg8\n",
            "Processing 2_20150915.mat → ha_eeg9\n",
            "Processing 2_20150915.mat → ha_eeg10\n",
            "Processing 2_20150915.mat → ha_eeg11\n",
            "Processing 2_20150915.mat → ha_eeg12\n",
            "Processing 2_20150915.mat → ha_eeg13\n",
            "Processing 2_20150915.mat → ha_eeg14\n",
            "Processing 2_20150915.mat → ha_eeg15\n",
            "Processing 2_20150915.mat → ha_eeg16\n",
            "Processing 2_20150915.mat → ha_eeg17\n",
            "Processing 2_20150915.mat → ha_eeg18\n",
            "Processing 2_20150915.mat → ha_eeg19\n",
            "Processing 2_20150915.mat → ha_eeg20\n",
            "Processing 2_20150915.mat → ha_eeg21\n",
            "Processing 2_20150915.mat → ha_eeg22\n",
            "Processing 2_20150915.mat → ha_eeg23\n",
            "Processing 2_20150915.mat → ha_eeg24\n",
            "Saved locally: 2_20150915_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/2_20150915_MSPCA.npy\n",
            "Processing 4_20151111.mat → ldy_eeg1\n",
            "Processing 4_20151111.mat → ldy_eeg2\n",
            "Processing 4_20151111.mat → ldy_eeg3\n",
            "Processing 4_20151111.mat → ldy_eeg4\n",
            "Processing 4_20151111.mat → ldy_eeg5\n",
            "Processing 4_20151111.mat → ldy_eeg6\n",
            "Processing 4_20151111.mat → ldy_eeg7\n",
            "Processing 4_20151111.mat → ldy_eeg8\n",
            "Processing 4_20151111.mat → ldy_eeg9\n",
            "Processing 4_20151111.mat → ldy_eeg10\n",
            "Processing 4_20151111.mat → ldy_eeg11\n",
            "Processing 4_20151111.mat → ldy_eeg12\n",
            "Processing 4_20151111.mat → ldy_eeg13\n",
            "Processing 4_20151111.mat → ldy_eeg14\n",
            "Processing 4_20151111.mat → ldy_eeg15\n",
            "Processing 4_20151111.mat → ldy_eeg16\n",
            "Processing 4_20151111.mat → ldy_eeg17\n",
            "Processing 4_20151111.mat → ldy_eeg18\n",
            "Processing 4_20151111.mat → ldy_eeg19\n",
            "Processing 4_20151111.mat → ldy_eeg20\n",
            "Processing 4_20151111.mat → ldy_eeg21\n",
            "Processing 4_20151111.mat → ldy_eeg22\n",
            "Processing 4_20151111.mat → ldy_eeg23\n",
            "Processing 4_20151111.mat → ldy_eeg24\n",
            "Saved locally: 4_20151111_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/4_20151111_MSPCA.npy\n",
            "Processing 11_20150916.mat → whh_eeg1\n",
            "Processing 11_20150916.mat → whh_eeg2\n",
            "Processing 11_20150916.mat → whh_eeg3\n",
            "Processing 11_20150916.mat → whh_eeg4\n",
            "Processing 11_20150916.mat → whh_eeg5\n",
            "Processing 11_20150916.mat → whh_eeg6\n",
            "Processing 11_20150916.mat → whh_eeg7\n",
            "Processing 11_20150916.mat → whh_eeg8\n",
            "Processing 11_20150916.mat → whh_eeg9\n",
            "Processing 11_20150916.mat → whh_eeg10\n",
            "Processing 11_20150916.mat → whh_eeg11\n",
            "Processing 11_20150916.mat → whh_eeg12\n",
            "Processing 11_20150916.mat → whh_eeg13\n",
            "Processing 11_20150916.mat → whh_eeg14\n",
            "Processing 11_20150916.mat → whh_eeg15\n",
            "Processing 11_20150916.mat → whh_eeg16\n",
            "Processing 11_20150916.mat → whh_eeg17\n",
            "Processing 11_20150916.mat → whh_eeg18\n",
            "Processing 11_20150916.mat → whh_eeg19\n",
            "Processing 11_20150916.mat → whh_eeg20\n",
            "Processing 11_20150916.mat → whh_eeg21\n",
            "Processing 11_20150916.mat → whh_eeg22\n",
            "Processing 11_20150916.mat → whh_eeg23\n",
            "Processing 11_20150916.mat → whh_eeg24\n",
            "Saved locally: 11_20150916_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/11_20150916_MSPCA.npy\n",
            "Processing 10_20151014.mat → tyc_eeg1\n",
            "Processing 10_20151014.mat → tyc_eeg2\n",
            "Processing 10_20151014.mat → tyc_eeg3\n",
            "Processing 10_20151014.mat → tyc_eeg4\n",
            "Processing 10_20151014.mat → tyc_eeg5\n",
            "Processing 10_20151014.mat → tyc_eeg6\n",
            "Processing 10_20151014.mat → tyc_eeg7\n",
            "Processing 10_20151014.mat → tyc_eeg8\n",
            "Processing 10_20151014.mat → tyc_eeg9\n",
            "Processing 10_20151014.mat → tyc_eeg10\n",
            "Processing 10_20151014.mat → tyc_eeg11\n",
            "Processing 10_20151014.mat → tyc_eeg12\n",
            "Processing 10_20151014.mat → tyc_eeg13\n",
            "Processing 10_20151014.mat → tyc_eeg14\n",
            "Processing 10_20151014.mat → tyc_eeg15\n",
            "Processing 10_20151014.mat → tyc_eeg16\n",
            "Processing 10_20151014.mat → tyc_eeg17\n",
            "Processing 10_20151014.mat → tyc_eeg18\n",
            "Processing 10_20151014.mat → tyc_eeg19\n",
            "Processing 10_20151014.mat → tyc_eeg20\n",
            "Processing 10_20151014.mat → tyc_eeg21\n",
            "Processing 10_20151014.mat → tyc_eeg22\n",
            "Processing 10_20151014.mat → tyc_eeg23\n",
            "Processing 10_20151014.mat → tyc_eeg24\n",
            "Saved locally: 10_20151014_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/10_20151014_MSPCA.npy\n",
            "Processing 15_20150508.mat → zjy_eeg1\n",
            "Processing 15_20150508.mat → zjy_eeg2\n",
            "Processing 15_20150508.mat → zjy_eeg3\n",
            "Processing 15_20150508.mat → zjy_eeg4\n",
            "Processing 15_20150508.mat → zjy_eeg5\n",
            "Processing 15_20150508.mat → zjy_eeg6\n",
            "Processing 15_20150508.mat → zjy_eeg7\n",
            "Processing 15_20150508.mat → zjy_eeg8\n",
            "Processing 15_20150508.mat → zjy_eeg9\n",
            "Processing 15_20150508.mat → zjy_eeg10\n",
            "Processing 15_20150508.mat → zjy_eeg11\n",
            "Processing 15_20150508.mat → zjy_eeg12\n",
            "Processing 15_20150508.mat → zjy_eeg13\n",
            "Processing 15_20150508.mat → zjy_eeg14\n",
            "Processing 15_20150508.mat → zjy_eeg15\n",
            "Processing 15_20150508.mat → zjy_eeg16\n",
            "Processing 15_20150508.mat → zjy_eeg17\n",
            "Processing 15_20150508.mat → zjy_eeg18\n",
            "Processing 15_20150508.mat → zjy_eeg19\n",
            "Processing 15_20150508.mat → zjy_eeg20\n",
            "Processing 15_20150508.mat → zjy_eeg21\n",
            "Processing 15_20150508.mat → zjy_eeg22\n",
            "Processing 15_20150508.mat → zjy_eeg23\n",
            "Processing 15_20150508.mat → zjy_eeg24\n",
            "Saved locally: 15_20150508_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/15_20150508_MSPCA.npy\n",
            "Processing 9_20151028.mat → rx_eeg1\n",
            "Processing 9_20151028.mat → rx_eeg2\n",
            "Processing 9_20151028.mat → rx_eeg3\n",
            "Processing 9_20151028.mat → rx_eeg4\n",
            "Processing 9_20151028.mat → rx_eeg5\n",
            "Processing 9_20151028.mat → rx_eeg6\n",
            "Processing 9_20151028.mat → rx_eeg7\n",
            "Processing 9_20151028.mat → rx_eeg8\n",
            "Processing 9_20151028.mat → rx_eeg9\n",
            "Processing 9_20151028.mat → rx_eeg10\n",
            "Processing 9_20151028.mat → rx_eeg11\n",
            "Processing 9_20151028.mat → rx_eeg12\n",
            "Processing 9_20151028.mat → rx_eeg13\n",
            "Processing 9_20151028.mat → rx_eeg14\n",
            "Processing 9_20151028.mat → rx_eeg15\n",
            "Processing 9_20151028.mat → rx_eeg16\n",
            "Processing 9_20151028.mat → rx_eeg17\n",
            "Processing 9_20151028.mat → rx_eeg18\n",
            "Processing 9_20151028.mat → rx_eeg19\n",
            "Processing 9_20151028.mat → rx_eeg20\n",
            "Processing 9_20151028.mat → rx_eeg21\n",
            "Processing 9_20151028.mat → rx_eeg22\n",
            "Processing 9_20151028.mat → rx_eeg23\n",
            "Processing 9_20151028.mat → rx_eeg24\n",
            "Saved locally: 9_20151028_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/9_20151028_MSPCA.npy\n",
            "Processing 12_20150725.mat → wll_eeg1\n",
            "Processing 12_20150725.mat → wll_eeg2\n",
            "Processing 12_20150725.mat → wll_eeg3\n",
            "Processing 12_20150725.mat → wll_eeg4\n",
            "Processing 12_20150725.mat → wll_eeg5\n",
            "Processing 12_20150725.mat → wll_eeg6\n",
            "Processing 12_20150725.mat → wll_eeg7\n",
            "Processing 12_20150725.mat → wll_eeg8\n",
            "Processing 12_20150725.mat → wll_eeg9\n",
            "Processing 12_20150725.mat → wll_eeg10\n",
            "Processing 12_20150725.mat → wll_eeg11\n",
            "Processing 12_20150725.mat → wll_eeg12\n",
            "Processing 12_20150725.mat → wll_eeg13\n",
            "Processing 12_20150725.mat → wll_eeg14\n",
            "Processing 12_20150725.mat → wll_eeg15\n",
            "Processing 12_20150725.mat → wll_eeg16\n",
            "Processing 12_20150725.mat → wll_eeg17\n",
            "Processing 12_20150725.mat → wll_eeg18\n",
            "Processing 12_20150725.mat → wll_eeg19\n",
            "Processing 12_20150725.mat → wll_eeg20\n",
            "Processing 12_20150725.mat → wll_eeg21\n",
            "Processing 12_20150725.mat → wll_eeg22\n",
            "Processing 12_20150725.mat → wll_eeg23\n",
            "Processing 12_20150725.mat → wll_eeg24\n",
            "Saved locally: 12_20150725_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/12_20150725_MSPCA.npy\n",
            "Processing 3_20150919.mat → hql_eeg1\n",
            "Processing 3_20150919.mat → hql_eeg2\n",
            "Processing 3_20150919.mat → hql_eeg3\n",
            "Processing 3_20150919.mat → hql_eeg4\n",
            "Processing 3_20150919.mat → hql_eeg5\n",
            "Processing 3_20150919.mat → hql_eeg6\n",
            "Processing 3_20150919.mat → hql_eeg7\n",
            "Processing 3_20150919.mat → hql_eeg8\n",
            "Processing 3_20150919.mat → hql_eeg9\n",
            "Processing 3_20150919.mat → hql_eeg10\n",
            "Processing 3_20150919.mat → hql_eeg11\n",
            "Processing 3_20150919.mat → hql_eeg12\n",
            "Processing 3_20150919.mat → hql_eeg13\n",
            "Processing 3_20150919.mat → hql_eeg14\n",
            "Processing 3_20150919.mat → hql_eeg15\n",
            "Processing 3_20150919.mat → hql_eeg16\n",
            "Processing 3_20150919.mat → hql_eeg17\n",
            "Processing 3_20150919.mat → hql_eeg18\n",
            "Processing 3_20150919.mat → hql_eeg19\n",
            "Processing 3_20150919.mat → hql_eeg20\n",
            "Processing 3_20150919.mat → hql_eeg21\n",
            "Processing 3_20150919.mat → hql_eeg22\n",
            "Processing 3_20150919.mat → hql_eeg23\n",
            "Processing 3_20150919.mat → hql_eeg24\n",
            "Saved locally: 3_20150919_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/3_20150919_MSPCA.npy\n",
            "Processing 7_20150715.mat → mz_eeg1\n",
            "Processing 7_20150715.mat → mz_eeg2\n",
            "Processing 7_20150715.mat → mz_eeg3\n",
            "Processing 7_20150715.mat → mz_eeg4\n",
            "Processing 7_20150715.mat → mz_eeg5\n",
            "Processing 7_20150715.mat → mz_eeg6\n",
            "Processing 7_20150715.mat → mz_eeg7\n",
            "Processing 7_20150715.mat → mz_eeg8\n",
            "Processing 7_20150715.mat → mz_eeg9\n",
            "Processing 7_20150715.mat → mz_eeg10\n",
            "Processing 7_20150715.mat → mz_eeg11\n",
            "Processing 7_20150715.mat → mz_eeg12\n",
            "Processing 7_20150715.mat → mz_eeg13\n",
            "Processing 7_20150715.mat → mz_eeg14\n",
            "Processing 7_20150715.mat → mz_eeg15\n",
            "Processing 7_20150715.mat → mz_eeg16\n",
            "Processing 7_20150715.mat → mz_eeg17\n",
            "Processing 7_20150715.mat → mz_eeg18\n",
            "Processing 7_20150715.mat → mz_eeg19\n",
            "Processing 7_20150715.mat → mz_eeg20\n",
            "Processing 7_20150715.mat → mz_eeg21\n",
            "Processing 7_20150715.mat → mz_eeg22\n",
            "Processing 7_20150715.mat → mz_eeg23\n",
            "Processing 7_20150715.mat → mz_eeg24\n",
            "Saved locally: 7_20150715_MSPCA.npy\n",
            "Saved to Drive: /content/drive/MyDrive/MSPCA_outputs/7_20150715_MSPCA.npy\n"
          ]
        }
      ],
      "source": [
        "from mspca import mspca\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create output directory in Google Drive\n",
        "output_dir = '/content/drive/MyDrive/MSPCA_outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# List all .mat files in the current working directory\n",
        "mat_files = [f for f in os.listdir() if f.endswith('.mat')]\n",
        "print(\"Found files:\", mat_files)\n",
        "\n",
        "# Map for subject short names\n",
        "short_name_map = {\n",
        "    '1': 'cz', '2': 'ha', '3': 'hql', '4': 'ldy', '5': 'ly',\n",
        "    '6': 'mhw', '7': 'mz', '8': 'qyt', '9': 'rx', '10': 'tyc',\n",
        "    '11': 'whh', '12': 'wll', '13': 'wq', '14': 'zjd', '15': 'zjy'\n",
        "}\n",
        "\n",
        "for file in mat_files:\n",
        "    person_id = file.split('_')[0]\n",
        "    short_name = short_name_map.get(person_id)\n",
        "\n",
        "    if not short_name:\n",
        "        print(f\"Skipping {file} — unknown person ID.\")\n",
        "        continue\n",
        "\n",
        "    # Load .mat file\n",
        "    data = scipy.io.loadmat(file)\n",
        "    processed_data = {}\n",
        "\n",
        "    for j in range(24):  # Assuming 24 EEG channels\n",
        "        key = f'{short_name}_eeg{j + 1}'\n",
        "        if key in data:\n",
        "            array = data[key]\n",
        "            model = mspca.MultiscalePCA()\n",
        "            print(f'Processing {file} → {key}')\n",
        "            X_pred = model.fit_transform(array, wavelet_func='db4', threshold=0.3)\n",
        "            processed_data[key] = X_pred\n",
        "        else:\n",
        "            print(f'{key} not found in {file}, skipping.')\n",
        "\n",
        "    # Save to local (optional)\n",
        "    local_path = file.replace('.mat', '_MSPCA.npy')\n",
        "    np.save(local_path, processed_data)\n",
        "    print(f'Saved locally: {local_path}')\n",
        "\n",
        "    # Save to Google Drive\n",
        "    drive_path = os.path.join(output_dir, local_path)\n",
        "    np.save(drive_path, processed_data)\n",
        "    print(f'Saved to Drive: {drive_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define directory and label mapping\n",
        "mspca_dir = '/content/drive/MyDrive/MSPCA_outputs'\n",
        "channel_labels = [1, 2, 3, 0, 2, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2, 3, 2, 2, 3, 3, 0, 3, 0, 3]\n",
        "\n",
        "# Dictionary to hold labels for all files\n",
        "file_channel_labels = {}\n",
        "\n",
        "for file in os.listdir(mspca_dir):\n",
        "    if file.endswith('_MSPCA.npy'):\n",
        "        file_path = os.path.join(mspca_dir, file)\n",
        "        data = np.load(file_path, allow_pickle=True).item()\n",
        "        print(f\"\\n{file}:\")\n",
        "\n",
        "        for i in range(24):\n",
        "            for key in data:\n",
        "                if key.endswith(f'eeg{i+1}'):\n",
        "                    label = channel_labels[i]\n",
        "                    print(f\"  {key} → Label {label}\")\n",
        "                    file_channel_labels[key] = label\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0E_BNkqhjBd",
        "outputId": "f0603222-b185-4d30-a299-85eae5824c6a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1_20160518_MSPCA.npy:\n",
            "  cz_eeg1 → Label 1\n",
            "  cz_eeg2 → Label 2\n",
            "  cz_eeg3 → Label 3\n",
            "  cz_eeg4 → Label 0\n",
            "  cz_eeg5 → Label 2\n",
            "  cz_eeg6 → Label 0\n",
            "  cz_eeg7 → Label 0\n",
            "  cz_eeg8 → Label 1\n",
            "  cz_eeg9 → Label 0\n",
            "  cz_eeg10 → Label 1\n",
            "  cz_eeg11 → Label 2\n",
            "  cz_eeg12 → Label 1\n",
            "  cz_eeg13 → Label 1\n",
            "  cz_eeg14 → Label 1\n",
            "  cz_eeg15 → Label 2\n",
            "  cz_eeg16 → Label 3\n",
            "  cz_eeg17 → Label 2\n",
            "  cz_eeg18 → Label 2\n",
            "  cz_eeg19 → Label 3\n",
            "  cz_eeg20 → Label 3\n",
            "  cz_eeg21 → Label 0\n",
            "  cz_eeg22 → Label 3\n",
            "  cz_eeg23 → Label 0\n",
            "  cz_eeg24 → Label 3\n",
            "\n",
            "14_20151205_MSPCA.npy:\n",
            "  zjd_eeg1 → Label 1\n",
            "  zjd_eeg2 → Label 2\n",
            "  zjd_eeg3 → Label 3\n",
            "  zjd_eeg4 → Label 0\n",
            "  zjd_eeg5 → Label 2\n",
            "  zjd_eeg6 → Label 0\n",
            "  zjd_eeg7 → Label 0\n",
            "  zjd_eeg8 → Label 1\n",
            "  zjd_eeg9 → Label 0\n",
            "  zjd_eeg10 → Label 1\n",
            "  zjd_eeg11 → Label 2\n",
            "  zjd_eeg12 → Label 1\n",
            "  zjd_eeg13 → Label 1\n",
            "  zjd_eeg14 → Label 1\n",
            "  zjd_eeg15 → Label 2\n",
            "  zjd_eeg16 → Label 3\n",
            "  zjd_eeg17 → Label 2\n",
            "  zjd_eeg18 → Label 2\n",
            "  zjd_eeg19 → Label 3\n",
            "  zjd_eeg20 → Label 3\n",
            "  zjd_eeg21 → Label 0\n",
            "  zjd_eeg22 → Label 3\n",
            "  zjd_eeg23 → Label 0\n",
            "  zjd_eeg24 → Label 3\n",
            "\n",
            "5_20160406_MSPCA.npy:\n",
            "  ly_eeg1 → Label 1\n",
            "  ly_eeg2 → Label 2\n",
            "  ly_eeg3 → Label 3\n",
            "  ly_eeg4 → Label 0\n",
            "  ly_eeg5 → Label 2\n",
            "  ly_eeg6 → Label 0\n",
            "  ly_eeg7 → Label 0\n",
            "  ly_eeg8 → Label 1\n",
            "  ly_eeg9 → Label 0\n",
            "  ly_eeg10 → Label 1\n",
            "  ly_eeg11 → Label 2\n",
            "  ly_eeg12 → Label 1\n",
            "  ly_eeg13 → Label 1\n",
            "  ly_eeg14 → Label 1\n",
            "  ly_eeg15 → Label 2\n",
            "  ly_eeg16 → Label 3\n",
            "  ly_eeg17 → Label 2\n",
            "  ly_eeg18 → Label 2\n",
            "  ly_eeg19 → Label 3\n",
            "  ly_eeg20 → Label 3\n",
            "  ly_eeg21 → Label 0\n",
            "  ly_eeg22 → Label 3\n",
            "  ly_eeg23 → Label 0\n",
            "  ly_eeg24 → Label 3\n",
            "\n",
            "6_20150507_MSPCA.npy:\n",
            "  mhw_eeg1 → Label 1\n",
            "  mhw_eeg2 → Label 2\n",
            "  mhw_eeg3 → Label 3\n",
            "  mhw_eeg4 → Label 0\n",
            "  mhw_eeg5 → Label 2\n",
            "  mhw_eeg6 → Label 0\n",
            "  mhw_eeg7 → Label 0\n",
            "  mhw_eeg8 → Label 1\n",
            "  mhw_eeg9 → Label 0\n",
            "  mhw_eeg10 → Label 1\n",
            "  mhw_eeg11 → Label 2\n",
            "  mhw_eeg12 → Label 1\n",
            "  mhw_eeg13 → Label 1\n",
            "  mhw_eeg14 → Label 1\n",
            "  mhw_eeg15 → Label 2\n",
            "  mhw_eeg16 → Label 3\n",
            "  mhw_eeg17 → Label 2\n",
            "  mhw_eeg18 → Label 2\n",
            "  mhw_eeg19 → Label 3\n",
            "  mhw_eeg20 → Label 3\n",
            "  mhw_eeg21 → Label 0\n",
            "  mhw_eeg22 → Label 3\n",
            "  mhw_eeg23 → Label 0\n",
            "  mhw_eeg24 → Label 3\n",
            "\n",
            "8_20151103_MSPCA.npy:\n",
            "  qyt_eeg1 → Label 1\n",
            "  qyt_eeg2 → Label 2\n",
            "  qyt_eeg3 → Label 3\n",
            "  qyt_eeg4 → Label 0\n",
            "  qyt_eeg5 → Label 2\n",
            "  qyt_eeg6 → Label 0\n",
            "  qyt_eeg7 → Label 0\n",
            "  qyt_eeg8 → Label 1\n",
            "  qyt_eeg9 → Label 0\n",
            "  qyt_eeg10 → Label 1\n",
            "  qyt_eeg11 → Label 2\n",
            "  qyt_eeg12 → Label 1\n",
            "  qyt_eeg13 → Label 1\n",
            "  qyt_eeg14 → Label 1\n",
            "  qyt_eeg15 → Label 2\n",
            "  qyt_eeg16 → Label 3\n",
            "  qyt_eeg17 → Label 2\n",
            "  qyt_eeg18 → Label 2\n",
            "  qyt_eeg19 → Label 3\n",
            "  qyt_eeg20 → Label 3\n",
            "  qyt_eeg21 → Label 0\n",
            "  qyt_eeg22 → Label 3\n",
            "  qyt_eeg23 → Label 0\n",
            "  qyt_eeg24 → Label 3\n",
            "\n",
            "13_20151115_MSPCA.npy:\n",
            "  wq_eeg1 → Label 1\n",
            "  wq_eeg2 → Label 2\n",
            "  wq_eeg3 → Label 3\n",
            "  wq_eeg4 → Label 0\n",
            "  wq_eeg5 → Label 2\n",
            "  wq_eeg6 → Label 0\n",
            "  wq_eeg7 → Label 0\n",
            "  wq_eeg8 → Label 1\n",
            "  wq_eeg9 → Label 0\n",
            "  wq_eeg10 → Label 1\n",
            "  wq_eeg11 → Label 2\n",
            "  wq_eeg12 → Label 1\n",
            "  wq_eeg13 → Label 1\n",
            "  wq_eeg14 → Label 1\n",
            "  wq_eeg15 → Label 2\n",
            "  wq_eeg16 → Label 3\n",
            "  wq_eeg17 → Label 2\n",
            "  wq_eeg18 → Label 2\n",
            "  wq_eeg19 → Label 3\n",
            "  wq_eeg20 → Label 3\n",
            "  wq_eeg21 → Label 0\n",
            "  wq_eeg22 → Label 3\n",
            "  wq_eeg23 → Label 0\n",
            "  wq_eeg24 → Label 3\n",
            "\n",
            "2_20150915_MSPCA.npy:\n",
            "  ha_eeg1 → Label 1\n",
            "  ha_eeg2 → Label 2\n",
            "  ha_eeg3 → Label 3\n",
            "  ha_eeg4 → Label 0\n",
            "  ha_eeg5 → Label 2\n",
            "  ha_eeg6 → Label 0\n",
            "  ha_eeg7 → Label 0\n",
            "  ha_eeg8 → Label 1\n",
            "  ha_eeg9 → Label 0\n",
            "  ha_eeg10 → Label 1\n",
            "  ha_eeg11 → Label 2\n",
            "  ha_eeg12 → Label 1\n",
            "  ha_eeg13 → Label 1\n",
            "  ha_eeg14 → Label 1\n",
            "  ha_eeg15 → Label 2\n",
            "  ha_eeg16 → Label 3\n",
            "  ha_eeg17 → Label 2\n",
            "  ha_eeg18 → Label 2\n",
            "  ha_eeg19 → Label 3\n",
            "  ha_eeg20 → Label 3\n",
            "  ha_eeg21 → Label 0\n",
            "  ha_eeg22 → Label 3\n",
            "  ha_eeg23 → Label 0\n",
            "  ha_eeg24 → Label 3\n",
            "\n",
            "4_20151111_MSPCA.npy:\n",
            "  ldy_eeg1 → Label 1\n",
            "  ldy_eeg2 → Label 2\n",
            "  ldy_eeg3 → Label 3\n",
            "  ldy_eeg4 → Label 0\n",
            "  ldy_eeg5 → Label 2\n",
            "  ldy_eeg6 → Label 0\n",
            "  ldy_eeg7 → Label 0\n",
            "  ldy_eeg8 → Label 1\n",
            "  ldy_eeg9 → Label 0\n",
            "  ldy_eeg10 → Label 1\n",
            "  ldy_eeg11 → Label 2\n",
            "  ldy_eeg12 → Label 1\n",
            "  ldy_eeg13 → Label 1\n",
            "  ldy_eeg14 → Label 1\n",
            "  ldy_eeg15 → Label 2\n",
            "  ldy_eeg16 → Label 3\n",
            "  ldy_eeg17 → Label 2\n",
            "  ldy_eeg18 → Label 2\n",
            "  ldy_eeg19 → Label 3\n",
            "  ldy_eeg20 → Label 3\n",
            "  ldy_eeg21 → Label 0\n",
            "  ldy_eeg22 → Label 3\n",
            "  ldy_eeg23 → Label 0\n",
            "  ldy_eeg24 → Label 3\n",
            "\n",
            "11_20150916_MSPCA.npy:\n",
            "  whh_eeg1 → Label 1\n",
            "  whh_eeg2 → Label 2\n",
            "  whh_eeg3 → Label 3\n",
            "  whh_eeg4 → Label 0\n",
            "  whh_eeg5 → Label 2\n",
            "  whh_eeg6 → Label 0\n",
            "  whh_eeg7 → Label 0\n",
            "  whh_eeg8 → Label 1\n",
            "  whh_eeg9 → Label 0\n",
            "  whh_eeg10 → Label 1\n",
            "  whh_eeg11 → Label 2\n",
            "  whh_eeg12 → Label 1\n",
            "  whh_eeg13 → Label 1\n",
            "  whh_eeg14 → Label 1\n",
            "  whh_eeg15 → Label 2\n",
            "  whh_eeg16 → Label 3\n",
            "  whh_eeg17 → Label 2\n",
            "  whh_eeg18 → Label 2\n",
            "  whh_eeg19 → Label 3\n",
            "  whh_eeg20 → Label 3\n",
            "  whh_eeg21 → Label 0\n",
            "  whh_eeg22 → Label 3\n",
            "  whh_eeg23 → Label 0\n",
            "  whh_eeg24 → Label 3\n",
            "\n",
            "10_20151014_MSPCA.npy:\n",
            "  tyc_eeg1 → Label 1\n",
            "  tyc_eeg2 → Label 2\n",
            "  tyc_eeg3 → Label 3\n",
            "  tyc_eeg4 → Label 0\n",
            "  tyc_eeg5 → Label 2\n",
            "  tyc_eeg6 → Label 0\n",
            "  tyc_eeg7 → Label 0\n",
            "  tyc_eeg8 → Label 1\n",
            "  tyc_eeg9 → Label 0\n",
            "  tyc_eeg10 → Label 1\n",
            "  tyc_eeg11 → Label 2\n",
            "  tyc_eeg12 → Label 1\n",
            "  tyc_eeg13 → Label 1\n",
            "  tyc_eeg14 → Label 1\n",
            "  tyc_eeg15 → Label 2\n",
            "  tyc_eeg16 → Label 3\n",
            "  tyc_eeg17 → Label 2\n",
            "  tyc_eeg18 → Label 2\n",
            "  tyc_eeg19 → Label 3\n",
            "  tyc_eeg20 → Label 3\n",
            "  tyc_eeg21 → Label 0\n",
            "  tyc_eeg22 → Label 3\n",
            "  tyc_eeg23 → Label 0\n",
            "  tyc_eeg24 → Label 3\n",
            "\n",
            "15_20150508_MSPCA.npy:\n",
            "  zjy_eeg1 → Label 1\n",
            "  zjy_eeg2 → Label 2\n",
            "  zjy_eeg3 → Label 3\n",
            "  zjy_eeg4 → Label 0\n",
            "  zjy_eeg5 → Label 2\n",
            "  zjy_eeg6 → Label 0\n",
            "  zjy_eeg7 → Label 0\n",
            "  zjy_eeg8 → Label 1\n",
            "  zjy_eeg9 → Label 0\n",
            "  zjy_eeg10 → Label 1\n",
            "  zjy_eeg11 → Label 2\n",
            "  zjy_eeg12 → Label 1\n",
            "  zjy_eeg13 → Label 1\n",
            "  zjy_eeg14 → Label 1\n",
            "  zjy_eeg15 → Label 2\n",
            "  zjy_eeg16 → Label 3\n",
            "  zjy_eeg17 → Label 2\n",
            "  zjy_eeg18 → Label 2\n",
            "  zjy_eeg19 → Label 3\n",
            "  zjy_eeg20 → Label 3\n",
            "  zjy_eeg21 → Label 0\n",
            "  zjy_eeg22 → Label 3\n",
            "  zjy_eeg23 → Label 0\n",
            "  zjy_eeg24 → Label 3\n",
            "\n",
            "9_20151028_MSPCA.npy:\n",
            "  rx_eeg1 → Label 1\n",
            "  rx_eeg2 → Label 2\n",
            "  rx_eeg3 → Label 3\n",
            "  rx_eeg4 → Label 0\n",
            "  rx_eeg5 → Label 2\n",
            "  rx_eeg6 → Label 0\n",
            "  rx_eeg7 → Label 0\n",
            "  rx_eeg8 → Label 1\n",
            "  rx_eeg9 → Label 0\n",
            "  rx_eeg10 → Label 1\n",
            "  rx_eeg11 → Label 2\n",
            "  rx_eeg12 → Label 1\n",
            "  rx_eeg13 → Label 1\n",
            "  rx_eeg14 → Label 1\n",
            "  rx_eeg15 → Label 2\n",
            "  rx_eeg16 → Label 3\n",
            "  rx_eeg17 → Label 2\n",
            "  rx_eeg18 → Label 2\n",
            "  rx_eeg19 → Label 3\n",
            "  rx_eeg20 → Label 3\n",
            "  rx_eeg21 → Label 0\n",
            "  rx_eeg22 → Label 3\n",
            "  rx_eeg23 → Label 0\n",
            "  rx_eeg24 → Label 3\n",
            "\n",
            "12_20150725_MSPCA.npy:\n",
            "  wll_eeg1 → Label 1\n",
            "  wll_eeg2 → Label 2\n",
            "  wll_eeg3 → Label 3\n",
            "  wll_eeg4 → Label 0\n",
            "  wll_eeg5 → Label 2\n",
            "  wll_eeg6 → Label 0\n",
            "  wll_eeg7 → Label 0\n",
            "  wll_eeg8 → Label 1\n",
            "  wll_eeg9 → Label 0\n",
            "  wll_eeg10 → Label 1\n",
            "  wll_eeg11 → Label 2\n",
            "  wll_eeg12 → Label 1\n",
            "  wll_eeg13 → Label 1\n",
            "  wll_eeg14 → Label 1\n",
            "  wll_eeg15 → Label 2\n",
            "  wll_eeg16 → Label 3\n",
            "  wll_eeg17 → Label 2\n",
            "  wll_eeg18 → Label 2\n",
            "  wll_eeg19 → Label 3\n",
            "  wll_eeg20 → Label 3\n",
            "  wll_eeg21 → Label 0\n",
            "  wll_eeg22 → Label 3\n",
            "  wll_eeg23 → Label 0\n",
            "  wll_eeg24 → Label 3\n",
            "\n",
            "3_20150919_MSPCA.npy:\n",
            "  hql_eeg1 → Label 1\n",
            "  hql_eeg2 → Label 2\n",
            "  hql_eeg3 → Label 3\n",
            "  hql_eeg4 → Label 0\n",
            "  hql_eeg5 → Label 2\n",
            "  hql_eeg6 → Label 0\n",
            "  hql_eeg7 → Label 0\n",
            "  hql_eeg8 → Label 1\n",
            "  hql_eeg9 → Label 0\n",
            "  hql_eeg10 → Label 1\n",
            "  hql_eeg11 → Label 2\n",
            "  hql_eeg12 → Label 1\n",
            "  hql_eeg13 → Label 1\n",
            "  hql_eeg14 → Label 1\n",
            "  hql_eeg15 → Label 2\n",
            "  hql_eeg16 → Label 3\n",
            "  hql_eeg17 → Label 2\n",
            "  hql_eeg18 → Label 2\n",
            "  hql_eeg19 → Label 3\n",
            "  hql_eeg20 → Label 3\n",
            "  hql_eeg21 → Label 0\n",
            "  hql_eeg22 → Label 3\n",
            "  hql_eeg23 → Label 0\n",
            "  hql_eeg24 → Label 3\n",
            "\n",
            "7_20150715_MSPCA.npy:\n",
            "  mz_eeg1 → Label 1\n",
            "  mz_eeg2 → Label 2\n",
            "  mz_eeg3 → Label 3\n",
            "  mz_eeg4 → Label 0\n",
            "  mz_eeg5 → Label 2\n",
            "  mz_eeg6 → Label 0\n",
            "  mz_eeg7 → Label 0\n",
            "  mz_eeg8 → Label 1\n",
            "  mz_eeg9 → Label 0\n",
            "  mz_eeg10 → Label 1\n",
            "  mz_eeg11 → Label 2\n",
            "  mz_eeg12 → Label 1\n",
            "  mz_eeg13 → Label 1\n",
            "  mz_eeg14 → Label 1\n",
            "  mz_eeg15 → Label 2\n",
            "  mz_eeg16 → Label 3\n",
            "  mz_eeg17 → Label 2\n",
            "  mz_eeg18 → Label 2\n",
            "  mz_eeg19 → Label 3\n",
            "  mz_eeg20 → Label 3\n",
            "  mz_eeg21 → Label 0\n",
            "  mz_eeg22 → Label 3\n",
            "  mz_eeg23 → Label 0\n",
            "  mz_eeg24 → Label 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Label mapping for channels 1–24\n",
        "channel_labels = [1, 2, 3, 0, 2, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2, 3, 2, 2, 3, 3, 0, 3, 0, 3]\n",
        "\n",
        "mspca_dir = '/content/drive/MyDrive/MSPCA_outputs'\n",
        "\n",
        "for file in os.listdir(mspca_dir):\n",
        "    if not file.endswith('_MSPCA.npy'):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(mspca_dir, file)\n",
        "    data = np.load(file_path, allow_pickle=True).item()\n",
        "\n",
        "    # Create label dictionary { 'cz_eeg1': label1, ..., 'cz_eeg24': label24 }\n",
        "    labels = {}\n",
        "    for i in range(24):\n",
        "        eeg_key = None\n",
        "        for key in data:\n",
        "            if key.endswith(f'eeg{i+1}'):\n",
        "                eeg_key = key\n",
        "                break\n",
        "        if eeg_key:\n",
        "            labels[eeg_key] = channel_labels[i]\n",
        "\n",
        "    # Add labels to the same dictionary\n",
        "    data['labels'] = labels\n",
        "\n",
        "    # Save back to the same file\n",
        "    np.save(file_path, data)\n",
        "    print(f\"Updated: {file} with labels.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5ppW4sqhyyi",
        "outputId": "dae92e68-f932-4645-ccc2-3ee1464ca8e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated: 1_20160518_MSPCA.npy with labels.\n",
            "Updated: 14_20151205_MSPCA.npy with labels.\n",
            "Updated: 5_20160406_MSPCA.npy with labels.\n",
            "Updated: 6_20150507_MSPCA.npy with labels.\n",
            "Updated: 8_20151103_MSPCA.npy with labels.\n",
            "Updated: 13_20151115_MSPCA.npy with labels.\n",
            "Updated: 2_20150915_MSPCA.npy with labels.\n",
            "Updated: 4_20151111_MSPCA.npy with labels.\n",
            "Updated: 11_20150916_MSPCA.npy with labels.\n",
            "Updated: 10_20151014_MSPCA.npy with labels.\n",
            "Updated: 15_20150508_MSPCA.npy with labels.\n",
            "Updated: 9_20151028_MSPCA.npy with labels.\n",
            "Updated: 12_20150725_MSPCA.npy with labels.\n",
            "Updated: 3_20150919_MSPCA.npy with labels.\n",
            "Updated: 7_20150715_MSPCA.npy with labels.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "mspca_dir = '/content/drive/MyDrive/MSPCA_outputs'\n",
        "\n",
        "for file in os.listdir(mspca_dir):\n",
        "    if file.endswith('_MSPCA.npy'):\n",
        "        file_path = os.path.join(mspca_dir, file)\n",
        "        data = np.load(file_path, allow_pickle=True).item()\n",
        "        print(f\"\\nFile: {file}\")\n",
        "\n",
        "        for key in sorted(data.keys()):\n",
        "          value = data[key]\n",
        "          if isinstance(value, np.ndarray):\n",
        "            print(f\"  {key}: shape = {value.shape}\")\n",
        "        else:\n",
        "            print(f\"  {key}: type = {type(value)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_juEROeiMIv",
        "outputId": "839ba372-bc32-4904-dce5-d1c596125f09"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "File: 1_20160518_MSPCA.npy\n",
            "  cz_eeg1: shape = (62, 33601)\n",
            "  cz_eeg10: shape = (62, 33801)\n",
            "  cz_eeg11: shape = (62, 10001)\n",
            "  cz_eeg12: shape = (62, 22001)\n",
            "  cz_eeg13: shape = (62, 43401)\n",
            "  cz_eeg14: shape = (62, 33801)\n",
            "  cz_eeg15: shape = (62, 51801)\n",
            "  cz_eeg16: shape = (62, 28201)\n",
            "  cz_eeg17: shape = (62, 13601)\n",
            "  cz_eeg18: shape = (62, 35801)\n",
            "  cz_eeg19: shape = (62, 28001)\n",
            "  cz_eeg2: shape = (62, 19001)\n",
            "  cz_eeg20: shape = (62, 9601)\n",
            "  cz_eeg21: shape = (62, 22401)\n",
            "  cz_eeg22: shape = (62, 22401)\n",
            "  cz_eeg23: shape = (62, 35001)\n",
            "  cz_eeg24: shape = (62, 27401)\n",
            "  cz_eeg3: shape = (62, 39801)\n",
            "  cz_eeg4: shape = (62, 26001)\n",
            "  cz_eeg5: shape = (62, 17601)\n",
            "  cz_eeg6: shape = (62, 32401)\n",
            "  cz_eeg7: shape = (62, 30601)\n",
            "  cz_eeg8: shape = (62, 41801)\n",
            "  cz_eeg9: shape = (62, 29001)\n",
            "  labels: type = <class 'dict'>\n",
            "\n",
            "File: 14_20151205_MSPCA.npy\n",
            "  zjd_eeg1: shape = (62, 33601)\n",
            "  zjd_eeg10: shape = (62, 33801)\n",
            "  zjd_eeg11: shape = (62, 10001)\n",
            "  zjd_eeg12: shape = (62, 22001)\n",
            "  zjd_eeg13: shape = (62, 43401)\n",
            "  zjd_eeg14: shape = (62, 33801)\n",
            "  zjd_eeg15: shape = (62, 51801)\n",
            "  zjd_eeg16: shape = (62, 28201)\n",
            "  zjd_eeg17: shape = (62, 13601)\n",
            "  zjd_eeg18: shape = (62, 35801)\n",
            "  zjd_eeg19: shape = (62, 28001)\n",
            "  zjd_eeg2: shape = (62, 19001)\n",
            "  zjd_eeg20: shape = (62, 9601)\n",
            "  zjd_eeg21: shape = (62, 22401)\n",
            "  zjd_eeg22: shape = (62, 22401)\n",
            "  zjd_eeg23: shape = (62, 35001)\n",
            "  zjd_eeg24: shape = (62, 27401)\n",
            "  zjd_eeg3: shape = (62, 39801)\n",
            "  zjd_eeg4: shape = (62, 26001)\n",
            "  zjd_eeg5: shape = (62, 17601)\n",
            "  zjd_eeg6: shape = (62, 32401)\n",
            "  zjd_eeg7: shape = (62, 30601)\n",
            "  zjd_eeg8: shape = (62, 41801)\n",
            "  zjd_eeg9: shape = (62, 29001)\n",
            "  zjd_eeg9: type = <class 'numpy.ndarray'>\n",
            "\n",
            "File: 5_20160406_MSPCA.npy\n",
            "  ly_eeg1: shape = (62, 33601)\n",
            "  ly_eeg10: shape = (62, 33801)\n",
            "  ly_eeg11: shape = (62, 10001)\n",
            "  ly_eeg12: shape = (62, 22001)\n",
            "  ly_eeg13: shape = (62, 43401)\n",
            "  ly_eeg14: shape = (62, 33801)\n",
            "  ly_eeg15: shape = (62, 51801)\n",
            "  ly_eeg16: shape = (62, 28201)\n",
            "  ly_eeg17: shape = (62, 13601)\n",
            "  ly_eeg18: shape = (62, 35801)\n",
            "  ly_eeg19: shape = (62, 28001)\n",
            "  ly_eeg2: shape = (62, 19001)\n",
            "  ly_eeg20: shape = (62, 9601)\n",
            "  ly_eeg21: shape = (62, 22401)\n",
            "  ly_eeg22: shape = (62, 22401)\n",
            "  ly_eeg23: shape = (62, 35001)\n",
            "  ly_eeg24: shape = (62, 27401)\n",
            "  ly_eeg3: shape = (62, 39801)\n",
            "  ly_eeg4: shape = (62, 26001)\n",
            "  ly_eeg5: shape = (62, 17601)\n",
            "  ly_eeg6: shape = (62, 32401)\n",
            "  ly_eeg7: shape = (62, 30601)\n",
            "  ly_eeg8: shape = (62, 41801)\n",
            "  ly_eeg9: shape = (62, 29001)\n",
            "  ly_eeg9: type = <class 'numpy.ndarray'>\n",
            "\n",
            "File: 6_20150507_MSPCA.npy\n",
            "  mhw_eeg1: shape = (62, 33601)\n",
            "  mhw_eeg10: shape = (62, 33801)\n",
            "  mhw_eeg11: shape = (62, 10001)\n",
            "  mhw_eeg12: shape = (62, 22001)\n",
            "  mhw_eeg13: shape = (62, 43401)\n",
            "  mhw_eeg14: shape = (62, 33801)\n",
            "  mhw_eeg15: shape = (62, 51801)\n",
            "  mhw_eeg16: shape = (62, 28201)\n",
            "  mhw_eeg17: shape = (62, 13601)\n",
            "  mhw_eeg18: shape = (62, 35801)\n",
            "  mhw_eeg19: shape = (62, 28001)\n",
            "  mhw_eeg2: shape = (62, 19001)\n",
            "  mhw_eeg20: shape = (62, 9601)\n",
            "  mhw_eeg21: shape = (62, 22401)\n",
            "  mhw_eeg22: shape = (62, 22401)\n",
            "  mhw_eeg23: shape = (62, 35001)\n",
            "  mhw_eeg24: shape = (62, 27401)\n",
            "  mhw_eeg3: shape = (62, 39801)\n",
            "  mhw_eeg4: shape = (62, 26001)\n",
            "  mhw_eeg5: shape = (62, 17601)\n",
            "  mhw_eeg6: shape = (62, 32401)\n",
            "  mhw_eeg7: shape = (62, 30601)\n",
            "  mhw_eeg8: shape = (62, 41801)\n",
            "  mhw_eeg9: shape = (62, 29001)\n",
            "  mhw_eeg9: type = <class 'numpy.ndarray'>\n",
            "\n",
            "File: 8_20151103_MSPCA.npy\n",
            "  qyt_eeg1: shape = (62, 33601)\n",
            "  qyt_eeg10: shape = (62, 33801)\n",
            "  qyt_eeg11: shape = (62, 10001)\n",
            "  qyt_eeg12: shape = (62, 22001)\n",
            "  qyt_eeg13: shape = (62, 43401)\n",
            "  qyt_eeg14: shape = (62, 33801)\n",
            "  qyt_eeg15: shape = (62, 51801)\n",
            "  qyt_eeg16: shape = (62, 28201)\n",
            "  qyt_eeg17: shape = (62, 13601)\n",
            "  qyt_eeg18: shape = (62, 35801)\n",
            "  qyt_eeg19: shape = (62, 28001)\n",
            "  qyt_eeg2: shape = (62, 19001)\n",
            "  qyt_eeg20: shape = (62, 9601)\n",
            "  qyt_eeg21: shape = (62, 22401)\n",
            "  qyt_eeg22: shape = (62, 22401)\n",
            "  qyt_eeg23: shape = (62, 35001)\n",
            "  qyt_eeg24: shape = (62, 27401)\n",
            "  qyt_eeg3: shape = (62, 39801)\n",
            "  qyt_eeg4: shape = (62, 26001)\n",
            "  qyt_eeg5: shape = (62, 17601)\n",
            "  qyt_eeg6: shape = (62, 32401)\n",
            "  qyt_eeg7: shape = (62, 30601)\n",
            "  qyt_eeg8: shape = (62, 41801)\n",
            "  qyt_eeg9: shape = (62, 29001)\n",
            "  qyt_eeg9: type = <class 'numpy.ndarray'>\n",
            "\n",
            "File: 13_20151115_MSPCA.npy\n",
            "  wq_eeg1: shape = (62, 33601)\n",
            "  wq_eeg10: shape = (62, 33801)\n",
            "  wq_eeg11: shape = (62, 10001)\n",
            "  wq_eeg12: shape = (62, 22001)\n",
            "  wq_eeg13: shape = (62, 43401)\n",
            "  wq_eeg14: shape = (62, 33801)\n",
            "  wq_eeg15: shape = (62, 51801)\n",
            "  wq_eeg16: shape = (62, 28201)\n",
            "  wq_eeg17: shape = (62, 13601)\n",
            "  wq_eeg18: shape = (62, 35801)\n",
            "  wq_eeg19: shape = (62, 28001)\n",
            "  wq_eeg2: shape = (62, 19001)\n",
            "  wq_eeg20: shape = (62, 9601)\n",
            "  wq_eeg21: shape = (62, 22401)\n",
            "  wq_eeg22: shape = (62, 22401)\n",
            "  wq_eeg23: shape = (62, 35001)\n",
            "  wq_eeg24: shape = (62, 27401)\n",
            "  wq_eeg3: shape = (62, 39801)\n",
            "  wq_eeg4: shape = (62, 26001)\n",
            "  wq_eeg5: shape = (62, 17601)\n",
            "  wq_eeg6: shape = (62, 32401)\n",
            "  wq_eeg7: shape = (62, 30601)\n",
            "  wq_eeg8: shape = (62, 41801)\n",
            "  wq_eeg9: shape = (62, 29001)\n",
            "  wq_eeg9: type = <class 'numpy.ndarray'>\n",
            "\n",
            "File: 2_20150915_MSPCA.npy\n",
            "  ha_eeg1: shape = (62, 33601)\n",
            "  ha_eeg10: shape = (62, 33801)\n",
            "  ha_eeg11: shape = (62, 10001)\n",
            "  ha_eeg12: shape = (62, 22001)\n",
            "  ha_eeg13: shape = (62, 43401)\n",
            "  ha_eeg14: shape = (62, 33801)\n",
            "  ha_eeg15: shape = (62, 51801)\n",
            "  ha_eeg16: shape = (62, 28201)\n",
            "  ha_eeg17: shape = (62, 13601)\n",
            "  ha_eeg18: shape = (62, 35801)\n",
            "  ha_eeg19: shape = (62, 28001)\n",
            "  ha_eeg2: shape = (62, 19001)\n",
            "  ha_eeg20: shape = (62, 9601)\n",
            "  ha_eeg21: shape = (62, 22401)\n",
            "  ha_eeg22: shape = (62, 22401)\n",
            "  ha_eeg23: shape = (62, 35001)\n",
            "  ha_eeg24: shape = (62, 27401)\n",
            "  ha_eeg3: shape = (62, 39801)\n",
            "  ha_eeg4: shape = (62, 26001)\n",
            "  ha_eeg5: shape = (62, 17601)\n",
            "  ha_eeg6: shape = (62, 32401)\n",
            "  ha_eeg7: shape = (62, 30601)\n",
            "  ha_eeg8: shape = (62, 41801)\n",
            "  ha_eeg9: shape = (62, 29001)\n",
            "  labels: type = <class 'dict'>\n",
            "\n",
            "File: 4_20151111_MSPCA.npy\n",
            "  ldy_eeg1: shape = (62, 33601)\n",
            "  ldy_eeg10: shape = (62, 33801)\n",
            "  ldy_eeg11: shape = (62, 10001)\n",
            "  ldy_eeg12: shape = (62, 22001)\n",
            "  ldy_eeg13: shape = (62, 43401)\n",
            "  ldy_eeg14: shape = (62, 33801)\n",
            "  ldy_eeg15: shape = (62, 51801)\n",
            "  ldy_eeg16: shape = (62, 28201)\n",
            "  ldy_eeg17: shape = (62, 13601)\n",
            "  ldy_eeg18: shape = (62, 35801)\n",
            "  ldy_eeg19: shape = (62, 28001)\n",
            "  ldy_eeg2: shape = (62, 19001)\n",
            "  ldy_eeg20: shape = (62, 9601)\n",
            "  ldy_eeg21: shape = (62, 22401)\n",
            "  ldy_eeg22: shape = (62, 22401)\n",
            "  ldy_eeg23: shape = (62, 35001)\n",
            "  ldy_eeg24: shape = (62, 27401)\n",
            "  ldy_eeg3: shape = (62, 39801)\n",
            "  ldy_eeg4: shape = (62, 26001)\n",
            "  ldy_eeg5: shape = (62, 17601)\n",
            "  ldy_eeg6: shape = (62, 32401)\n",
            "  ldy_eeg7: shape = (62, 30601)\n",
            "  ldy_eeg8: shape = (62, 41801)\n",
            "  ldy_eeg9: shape = (62, 29001)\n",
            "  ldy_eeg9: type = <class 'numpy.ndarray'>\n",
            "\n",
            "File: 11_20150916_MSPCA.npy\n",
            "  whh_eeg1: shape = (62, 33601)\n",
            "  whh_eeg10: shape = (62, 33801)\n",
            "  whh_eeg11: shape = (62, 10001)\n",
            "  whh_eeg12: shape = (62, 22001)\n",
            "  whh_eeg13: shape = (62, 43401)\n",
            "  whh_eeg14: shape = (62, 33801)\n",
            "  whh_eeg15: shape = (62, 51801)\n",
            "  whh_eeg16: shape = (62, 28201)\n",
            "  whh_eeg17: shape = (62, 13601)\n",
            "  whh_eeg18: shape = (62, 35801)\n",
            "  whh_eeg19: shape = (62, 28001)\n",
            "  whh_eeg2: shape = (62, 19001)\n",
            "  whh_eeg20: shape = (62, 9601)\n",
            "  whh_eeg21: shape = (62, 22401)\n",
            "  whh_eeg22: shape = (62, 22401)\n",
            "  whh_eeg23: shape = (62, 35001)\n",
            "  whh_eeg24: shape = (62, 27401)\n",
            "  whh_eeg3: shape = (62, 39801)\n",
            "  whh_eeg4: shape = (62, 26001)\n",
            "  whh_eeg5: shape = (62, 17601)\n",
            "  whh_eeg6: shape = (62, 32401)\n",
            "  whh_eeg7: shape = (62, 30601)\n",
            "  whh_eeg8: shape = (62, 41801)\n",
            "  whh_eeg9: shape = (62, 29001)\n",
            "  whh_eeg9: type = <class 'numpy.ndarray'>\n",
            "\n",
            "File: 10_20151014_MSPCA.npy\n",
            "  tyc_eeg1: shape = (62, 33601)\n",
            "  tyc_eeg10: shape = (62, 33801)\n",
            "  tyc_eeg11: shape = (62, 10001)\n",
            "  tyc_eeg12: shape = (62, 22001)\n",
            "  tyc_eeg13: shape = (62, 43401)\n",
            "  tyc_eeg14: shape = (62, 33801)\n",
            "  tyc_eeg15: shape = (62, 51801)\n",
            "  tyc_eeg16: shape = (62, 28201)\n",
            "  tyc_eeg17: shape = (62, 13601)\n",
            "  tyc_eeg18: shape = (62, 35801)\n",
            "  tyc_eeg19: shape = (62, 28001)\n",
            "  tyc_eeg2: shape = (62, 19001)\n",
            "  tyc_eeg20: shape = (62, 9601)\n",
            "  tyc_eeg21: shape = (62, 22401)\n",
            "  tyc_eeg22: shape = (62, 22401)\n",
            "  tyc_eeg23: shape = (62, 35001)\n",
            "  tyc_eeg24: shape = (62, 27401)\n",
            "  tyc_eeg3: shape = (62, 39801)\n",
            "  tyc_eeg4: shape = (62, 26001)\n",
            "  tyc_eeg5: shape = (62, 17601)\n",
            "  tyc_eeg6: shape = (62, 32401)\n",
            "  tyc_eeg7: shape = (62, 30601)\n",
            "  tyc_eeg8: shape = (62, 41801)\n",
            "  tyc_eeg9: shape = (62, 29001)\n",
            "  tyc_eeg9: type = <class 'numpy.ndarray'>\n",
            "\n",
            "File: 15_20150508_MSPCA.npy\n",
            "  zjy_eeg1: shape = (62, 33601)\n",
            "  zjy_eeg10: shape = (62, 33801)\n",
            "  zjy_eeg11: shape = (62, 10001)\n",
            "  zjy_eeg12: shape = (62, 22001)\n",
            "  zjy_eeg13: shape = (62, 43401)\n",
            "  zjy_eeg14: shape = (62, 33801)\n",
            "  zjy_eeg15: shape = (62, 51801)\n",
            "  zjy_eeg16: shape = (62, 28201)\n",
            "  zjy_eeg17: shape = (62, 13601)\n",
            "  zjy_eeg18: shape = (62, 35801)\n",
            "  zjy_eeg19: shape = (62, 28001)\n",
            "  zjy_eeg2: shape = (62, 19001)\n",
            "  zjy_eeg20: shape = (62, 9601)\n",
            "  zjy_eeg21: shape = (62, 22401)\n",
            "  zjy_eeg22: shape = (62, 22401)\n",
            "  zjy_eeg23: shape = (62, 35001)\n",
            "  zjy_eeg24: shape = (62, 27401)\n",
            "  zjy_eeg3: shape = (62, 39801)\n",
            "  zjy_eeg4: shape = (62, 26001)\n",
            "  zjy_eeg5: shape = (62, 17601)\n",
            "  zjy_eeg6: shape = (62, 32401)\n",
            "  zjy_eeg7: shape = (62, 30601)\n",
            "  zjy_eeg8: shape = (62, 41801)\n",
            "  zjy_eeg9: shape = (62, 29001)\n",
            "  zjy_eeg9: type = <class 'numpy.ndarray'>\n",
            "\n",
            "File: 9_20151028_MSPCA.npy\n",
            "  rx_eeg1: shape = (62, 33601)\n",
            "  rx_eeg10: shape = (62, 33801)\n",
            "  rx_eeg11: shape = (62, 10001)\n",
            "  rx_eeg12: shape = (62, 22001)\n",
            "  rx_eeg13: shape = (62, 43401)\n",
            "  rx_eeg14: shape = (62, 33801)\n",
            "  rx_eeg15: shape = (62, 51801)\n",
            "  rx_eeg16: shape = (62, 28201)\n",
            "  rx_eeg17: shape = (62, 13601)\n",
            "  rx_eeg18: shape = (62, 35801)\n",
            "  rx_eeg19: shape = (62, 28001)\n",
            "  rx_eeg2: shape = (62, 19001)\n",
            "  rx_eeg20: shape = (62, 9601)\n",
            "  rx_eeg21: shape = (62, 22401)\n",
            "  rx_eeg22: shape = (62, 22401)\n",
            "  rx_eeg23: shape = (62, 35001)\n",
            "  rx_eeg24: shape = (62, 27401)\n",
            "  rx_eeg3: shape = (62, 39801)\n",
            "  rx_eeg4: shape = (62, 26001)\n",
            "  rx_eeg5: shape = (62, 17601)\n",
            "  rx_eeg6: shape = (62, 32401)\n",
            "  rx_eeg7: shape = (62, 30601)\n",
            "  rx_eeg8: shape = (62, 41801)\n",
            "  rx_eeg9: shape = (62, 29001)\n",
            "  rx_eeg9: type = <class 'numpy.ndarray'>\n",
            "\n",
            "File: 12_20150725_MSPCA.npy\n",
            "  wll_eeg1: shape = (62, 33601)\n",
            "  wll_eeg10: shape = (62, 33801)\n",
            "  wll_eeg11: shape = (62, 10001)\n",
            "  wll_eeg12: shape = (62, 22001)\n",
            "  wll_eeg13: shape = (62, 43401)\n",
            "  wll_eeg14: shape = (62, 33801)\n",
            "  wll_eeg15: shape = (62, 51801)\n",
            "  wll_eeg16: shape = (62, 28201)\n",
            "  wll_eeg17: shape = (62, 13601)\n",
            "  wll_eeg18: shape = (62, 35801)\n",
            "  wll_eeg19: shape = (62, 28001)\n",
            "  wll_eeg2: shape = (62, 19001)\n",
            "  wll_eeg20: shape = (62, 9601)\n",
            "  wll_eeg21: shape = (62, 22401)\n",
            "  wll_eeg22: shape = (62, 22401)\n",
            "  wll_eeg23: shape = (62, 35001)\n",
            "  wll_eeg24: shape = (62, 27401)\n",
            "  wll_eeg3: shape = (62, 39801)\n",
            "  wll_eeg4: shape = (62, 26001)\n",
            "  wll_eeg5: shape = (62, 17601)\n",
            "  wll_eeg6: shape = (62, 32401)\n",
            "  wll_eeg7: shape = (62, 30601)\n",
            "  wll_eeg8: shape = (62, 41801)\n",
            "  wll_eeg9: shape = (62, 29001)\n",
            "  wll_eeg9: type = <class 'numpy.ndarray'>\n",
            "\n",
            "File: 3_20150919_MSPCA.npy\n",
            "  hql_eeg1: shape = (62, 33601)\n",
            "  hql_eeg10: shape = (62, 33801)\n",
            "  hql_eeg11: shape = (62, 10001)\n",
            "  hql_eeg12: shape = (62, 22001)\n",
            "  hql_eeg13: shape = (62, 43401)\n",
            "  hql_eeg14: shape = (62, 33801)\n",
            "  hql_eeg15: shape = (62, 51801)\n",
            "  hql_eeg16: shape = (62, 28201)\n",
            "  hql_eeg17: shape = (62, 13601)\n",
            "  hql_eeg18: shape = (62, 35801)\n",
            "  hql_eeg19: shape = (62, 28001)\n",
            "  hql_eeg2: shape = (62, 19001)\n",
            "  hql_eeg20: shape = (62, 9601)\n",
            "  hql_eeg21: shape = (62, 22401)\n",
            "  hql_eeg22: shape = (62, 22401)\n",
            "  hql_eeg23: shape = (62, 35001)\n",
            "  hql_eeg24: shape = (62, 27401)\n",
            "  hql_eeg3: shape = (62, 39801)\n",
            "  hql_eeg4: shape = (62, 26001)\n",
            "  hql_eeg5: shape = (62, 17601)\n",
            "  hql_eeg6: shape = (62, 32401)\n",
            "  hql_eeg7: shape = (62, 30601)\n",
            "  hql_eeg8: shape = (62, 41801)\n",
            "  hql_eeg9: shape = (62, 29001)\n",
            "  labels: type = <class 'dict'>\n",
            "\n",
            "File: 7_20150715_MSPCA.npy\n",
            "  mz_eeg1: shape = (62, 33601)\n",
            "  mz_eeg10: shape = (62, 33801)\n",
            "  mz_eeg11: shape = (62, 10001)\n",
            "  mz_eeg12: shape = (62, 22001)\n",
            "  mz_eeg13: shape = (62, 43401)\n",
            "  mz_eeg14: shape = (62, 33801)\n",
            "  mz_eeg15: shape = (62, 51801)\n",
            "  mz_eeg16: shape = (62, 28201)\n",
            "  mz_eeg17: shape = (62, 13601)\n",
            "  mz_eeg18: shape = (62, 35801)\n",
            "  mz_eeg19: shape = (62, 28001)\n",
            "  mz_eeg2: shape = (62, 19001)\n",
            "  mz_eeg20: shape = (62, 9601)\n",
            "  mz_eeg21: shape = (62, 22401)\n",
            "  mz_eeg22: shape = (62, 22401)\n",
            "  mz_eeg23: shape = (62, 35001)\n",
            "  mz_eeg24: shape = (62, 27401)\n",
            "  mz_eeg3: shape = (62, 39801)\n",
            "  mz_eeg4: shape = (62, 26001)\n",
            "  mz_eeg5: shape = (62, 17601)\n",
            "  mz_eeg6: shape = (62, 32401)\n",
            "  mz_eeg7: shape = (62, 30601)\n",
            "  mz_eeg8: shape = (62, 41801)\n",
            "  mz_eeg9: shape = (62, 29001)\n",
            "  mz_eeg9: type = <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "file_path = '/content/drive/MyDrive/MSPCA_outputs/2_20150915_MSPCA.npy'\n",
        "data = np.load(file_path, allow_pickle=True).item()\n",
        "\n",
        "print(\"Keys:\", data.keys())\n",
        "print(\"Labels:\", data['labels'])  # This should print the label dictionary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9PYYsOKirMO",
        "outputId": "7f8081db-fdbe-49f2-bd15-7dfe6d16019e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: dict_keys(['ha_eeg1', 'ha_eeg2', 'ha_eeg3', 'ha_eeg4', 'ha_eeg5', 'ha_eeg6', 'ha_eeg7', 'ha_eeg8', 'ha_eeg9', 'ha_eeg10', 'ha_eeg11', 'ha_eeg12', 'ha_eeg13', 'ha_eeg14', 'ha_eeg15', 'ha_eeg16', 'ha_eeg17', 'ha_eeg18', 'ha_eeg19', 'ha_eeg20', 'ha_eeg21', 'ha_eeg22', 'ha_eeg23', 'ha_eeg24', 'labels'])\n",
            "Labels: {'ha_eeg1': 1, 'ha_eeg2': 2, 'ha_eeg3': 3, 'ha_eeg4': 0, 'ha_eeg5': 2, 'ha_eeg6': 0, 'ha_eeg7': 0, 'ha_eeg8': 1, 'ha_eeg9': 0, 'ha_eeg10': 1, 'ha_eeg11': 2, 'ha_eeg12': 1, 'ha_eeg13': 1, 'ha_eeg14': 1, 'ha_eeg15': 2, 'ha_eeg16': 3, 'ha_eeg17': 2, 'ha_eeg18': 2, 'ha_eeg19': 3, 'ha_eeg20': 3, 'ha_eeg21': 0, 'ha_eeg22': 3, 'ha_eeg23': 0, 'ha_eeg24': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Constants\n",
        "mspca_dir = '/content/drive/MyDrive/MSPCA_outputs'\n",
        "output_dir = '/content/drive/MyDrive/SODP_SDC_outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def compute_sodp(eeg_signal):\n",
        "    \"\"\"Compute x(n) = EEG(n+1) - EEG(n), y(n) = EEG(n+2) - EEG(n+1)\"\"\"\n",
        "    x = eeg_signal[1:-1] - eeg_signal[:-2]\n",
        "    y = eeg_signal[2:] - eeg_signal[1:-1]\n",
        "    return x, y\n",
        "\n",
        "def compute_sdc(x, y):\n",
        "    \"\"\"Compute SDC as sqrt(x^2 + y^2) and return total sum\"\"\"\n",
        "    return np.sum(np.sqrt(x**2 + y**2))\n",
        "\n",
        "# Iterate over all MSPCA files\n",
        "for file in os.listdir(mspca_dir):\n",
        "    if not file.endswith('_MSPCA.npy'):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(mspca_dir, file)\n",
        "    data = np.load(file_path, allow_pickle=True).item()\n",
        "\n",
        "    sodp_sdc_features = []\n",
        "\n",
        "    # Iterate through the EEG data and compute SODP and SDC for each channel\n",
        "    for key, eeg_data in sorted(data.items()):\n",
        "        # Skip 'labels' key, as it's not EEG data\n",
        "        if key == 'labels':\n",
        "            continue\n",
        "\n",
        "        # eeg_data: Check if it's a 1D or 2D array (signal or windows of signals)\n",
        "        if isinstance(eeg_data, np.ndarray):\n",
        "            if eeg_data.ndim == 1:\n",
        "                # Single signal (1D array)\n",
        "                x, y = compute_sodp(eeg_data)\n",
        "                sdc = compute_sdc(x, y)\n",
        "                sodp_sdc_features.append(sdc)\n",
        "\n",
        "            elif eeg_data.ndim == 2:\n",
        "                # Multiple windows of signals (2D array)\n",
        "                for window in eeg_data:\n",
        "                    x, y = compute_sodp(window)\n",
        "                    sdc = compute_sdc(x, y)\n",
        "                    sodp_sdc_features.append(sdc)\n",
        "        else:\n",
        "            print(f\"Skipping {key}: Expected NumPy array, found {type(eeg_data)}\")\n",
        "\n",
        "    # Convert to array: shape = (num_samples,) → SDC values\n",
        "    features_array = np.array(sodp_sdc_features)\n",
        "    save_path = os.path.join(output_dir, file.replace('_MSPCA.npy', '_SODP_SDC.npy'))\n",
        "    np.save(save_path, features_array)\n",
        "    print(f\"Saved SODP+SDC for {file} → {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J53Df9NnjvmA",
        "outputId": "248e3285-e022-4bb5-993b-780c0a8516e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved SODP+SDC for 1_20160518_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/1_20160518_SODP_SDC.npy\n",
            "Saved SODP+SDC for 14_20151205_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/14_20151205_SODP_SDC.npy\n",
            "Saved SODP+SDC for 5_20160406_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/5_20160406_SODP_SDC.npy\n",
            "Saved SODP+SDC for 6_20150507_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/6_20150507_SODP_SDC.npy\n",
            "Saved SODP+SDC for 8_20151103_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/8_20151103_SODP_SDC.npy\n",
            "Saved SODP+SDC for 13_20151115_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/13_20151115_SODP_SDC.npy\n",
            "Saved SODP+SDC for 2_20150915_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/2_20150915_SODP_SDC.npy\n",
            "Saved SODP+SDC for 4_20151111_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/4_20151111_SODP_SDC.npy\n",
            "Saved SODP+SDC for 11_20150916_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/11_20150916_SODP_SDC.npy\n",
            "Saved SODP+SDC for 10_20151014_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/10_20151014_SODP_SDC.npy\n",
            "Saved SODP+SDC for 15_20150508_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/15_20150508_SODP_SDC.npy\n",
            "Saved SODP+SDC for 9_20151028_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/9_20151028_SODP_SDC.npy\n",
            "Saved SODP+SDC for 12_20150725_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/12_20150725_SODP_SDC.npy\n",
            "Saved SODP+SDC for 3_20150919_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/3_20150919_SODP_SDC.npy\n",
            "Saved SODP+SDC for 7_20150715_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_outputs/7_20150715_SODP_SDC.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from scipy.interpolate import RegularGridInterpolator\n",
        "\n",
        "# Constants\n",
        "mspca_dir = '/content/drive/MyDrive/SODP_SDC_outputs'\n",
        "output_dir = '/content/drive/MyDrive/Brain_Cognition_Atlas'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "h, w = 8, 9  # Dimensions for the 2D image\n",
        "\n",
        "def project_to_2d_map(sdc_values):\n",
        "    \"\"\"Project SDC values onto a 8x9 image using RegularGridInterpolator\"\"\"\n",
        "    # Initialize an 8x9 grid (image)\n",
        "    image = np.zeros((h, w))\n",
        "\n",
        "    # Example electrode positions based on a typical 8x9 grid mapping\n",
        "    electrode_positions = [\n",
        "        (0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8),\n",
        "        (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8),\n",
        "        (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8),\n",
        "        (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8),\n",
        "        (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8),\n",
        "        (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8),\n",
        "        (6, 0), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6), (6, 7), (6, 8),\n",
        "        (7, 0), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7), (7, 8),\n",
        "    ]\n",
        "\n",
        "    # Assign SDC values to the corresponding electrodes on the grid\n",
        "    for idx, pos in enumerate(electrode_positions):\n",
        "        if idx < len(sdc_values):  # Ensure we do not exceed the number of available SDC values\n",
        "            image[pos] = sdc_values[idx]\n",
        "\n",
        "    # Set up grid for interpolation\n",
        "    x = np.linspace(0, w - 1, w)\n",
        "    y = np.linspace(0, h - 1, h)\n",
        "\n",
        "    # Use RegularGridInterpolator for bilinear interpolation\n",
        "    interpolator = RegularGridInterpolator((y, x), image, method='linear', bounds_error=False, fill_value=0)\n",
        "\n",
        "    # Create grid of points to interpolate (full grid)\n",
        "    grid_y, grid_x = np.meshgrid(np.arange(h), np.arange(w), indexing=\"ij\")\n",
        "    points = np.vstack((grid_y.ravel(), grid_x.ravel())).T\n",
        "\n",
        "    # Interpolate the full grid\n",
        "    interpolated_image = interpolator(points).reshape(h, w)\n",
        "\n",
        "    return interpolated_image\n",
        "\n",
        "def sliding_window_process(sdc_data, window_size=4, step_size=4):\n",
        "    \"\"\"Apply sliding window to SDC data and project onto 2D maps\"\"\"\n",
        "    num_windows = (sdc_data.shape[0] - window_size) // step_size + 1\n",
        "    all_maps = []\n",
        "\n",
        "    for i in range(num_windows):\n",
        "        # Extract a window of data\n",
        "        window_sdc_values = sdc_data[i*step_size : i*step_size + window_size]\n",
        "\n",
        "        # Project the window's SDC values to a 2D map\n",
        "        projected_map = project_to_2d_map(window_sdc_values)\n",
        "        all_maps.append(projected_map)\n",
        "\n",
        "    # Stack 2D maps into a 3D array\n",
        "    stacked_maps = np.stack(all_maps, axis=-1)  # Shape (8, 9, d) where d is the number of windows\n",
        "    return stacked_maps\n",
        "\n",
        "# Iterate over all SODP+SDC files\n",
        "for file in os.listdir(mspca_dir):\n",
        "    if not file.endswith('_SODP_SDC.npy'):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(mspca_dir, file)\n",
        "    sdc_data = np.load(file_path)\n",
        "\n",
        "    # Process each file and create a 3D brain-cognition atlas\n",
        "    stacked_maps = sliding_window_process(sdc_data)\n",
        "\n",
        "    # Save the resulting 3D array (brain-cognition atlas)\n",
        "    save_path = os.path.join(output_dir, file.replace('_SODP_SDC.npy', '_Brain_Cognition_Atlas.npy'))\n",
        "    np.save(save_path, stacked_maps)\n",
        "    print(f\"Saved Brain-Cognition Atlas for {file} → {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF5BVkYLlXJx",
        "outputId": "7866bf45-4028-40f4-dd9c-1f515e543243"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Brain-Cognition Atlas for 1_20160518_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/1_20160518_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 14_20151205_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/14_20151205_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 5_20160406_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/5_20160406_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 6_20150507_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/6_20150507_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 8_20151103_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/8_20151103_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 13_20151115_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/13_20151115_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 2_20150915_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/2_20150915_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 4_20151111_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/4_20151111_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 11_20150916_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/11_20150916_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 10_20151014_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/10_20151014_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 15_20150508_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/15_20150508_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 9_20151028_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/9_20151028_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 12_20150725_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/12_20150725_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 3_20150919_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/3_20150919_Brain_Cognition_Atlas.npy\n",
            "Saved Brain-Cognition Atlas for 7_20150715_SODP_SDC.npy → /content/drive/MyDrive/Brain_Cognition_Atlas/7_20150715_Brain_Cognition_Atlas.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define the folder path\n",
        "output_dir = '/content/drive/MyDrive/Brain_Cognition_Atlas'\n",
        "\n",
        "# Loop through all files in the directory\n",
        "for fname in sorted(os.listdir(output_dir)):\n",
        "    if fname.endswith(\".npy\"):\n",
        "        full_path = os.path.join(output_dir, fname)\n",
        "\n",
        "        # Load the .npy file\n",
        "        data = np.load(full_path)\n",
        "\n",
        "        # Print the shape of the data\n",
        "        print(f\"{fname}: {data.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh-3tbM9FoZU",
        "outputId": "fe314c65-7007-45bf-e19f-366acee8bd02"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10_20151014_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "11_20150916_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "12_20150725_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "13_20151115_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "14_20151205_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "15_20150508_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "1_20160518_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "2_20150915_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "3_20150919_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "4_20151111_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "5_20160406_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "6_20150507_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "7_20150715_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "8_20151103_Brain_Cognition_Atlas.npy: (8, 9, 372)\n",
            "9_20151028_Brain_Cognition_Atlas.npy: (8, 9, 372)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the saved Brain-Cognition Atlas (3D array)\n",
        "atlas_path = '/content/drive/MyDrive/Brain_Cognition_Atlas/7_20150715_Brain_Cognition_Atlas.npy'\n",
        "brain_atlas = np.load(atlas_path)\n",
        "\n",
        "# Select a time window (e.g., the 10th time window)\n",
        "time_window_idx = 9  # Indexing starts from 0, so this is the 10th window\n",
        "\n",
        "# Get the 2D projection (8x9 grid) for the selected time window\n",
        "image = brain_atlas[:, :, time_window_idx]\n",
        "\n",
        "# Plot the 2D image\n",
        "plt.imshow(image, cmap='viridis', interpolation='bilinear')\n",
        "plt.colorbar(label='SDC Value')\n",
        "plt.title(f'Time Window {time_window_idx + 1}')\n",
        "plt.axis('off')  # Turn off the axis for a cleaner view\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "gnk6hl5xl5qx",
        "outputId": "117b4d06-fc33-4b7f-a1d2-7c564ca54da7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of the Brain-Cognition Atlas for {file}: {stacked_maps.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cAE53QFmxEL",
        "outputId": "1da3046b-1a8a-4c82-d91c-ebb2ee65b7b9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the Brain-Cognition Atlas for 7_20150715_SODP_SDC.npy: (8, 9, 372)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# 1. Dataset Loader\n",
        "class BrainCognitionAtlasDataset(Dataset):\n",
        "    def __init__(self, folder_path):\n",
        "        self.samples = []\n",
        "        for fname in sorted(os.listdir(folder_path)):\n",
        "            if fname.endswith(\".npy\"):\n",
        "                full_path = os.path.join(folder_path, fname)\n",
        "                data = np.load(full_path)  # shape: (8, 9, 372)\n",
        "                for t in range(data.shape[-1]):  # treat each time slice as one sample\n",
        "                    slice_ = data[:, :, t]  # shape (8, 9)\n",
        "                    self.samples.append(slice_)\n",
        "        self.samples = np.stack(self.samples).astype(np.float32)  # shape: (N, 8, 9)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        return torch.tensor(sample).unsqueeze(0)  # shape: (1, 8, 9)\n",
        "\n",
        "# 2. CBAM Block\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.shared(self.avg_pool(x))\n",
        "        max_out = self.shared(self.max_pool(x))\n",
        "        return x * self.sigmoid(avg_out + max_out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg = torch.mean(x, dim=1, keepdim=True)\n",
        "        max, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg, max], dim=1)\n",
        "        return x * self.sigmoid(self.conv(x_cat))\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(in_channels)\n",
        "        self.sa = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ca(x)\n",
        "        x = self.sa(x)\n",
        "        return x\n",
        "\n",
        "# 3. Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # First deconvolution to increase size (100 -> 64, 1x1 -> 7x7)\n",
        "            nn.ConvTranspose2d(100, 64, kernel_size=(3, 3), stride=2, padding=1),  # Output: (64, 7, 7)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Second deconvolution (64 -> 1, 7x7 -> 8x9)\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=2, padding=1),    # Output: (1, 8, 9)\n",
        "            nn.Tanh()  # Normalize to [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Output: (32, 8, 9)\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Output: (64, 8, 9)\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.AdaptiveAvgPool2d((4, 4)),  # Forces shape to (64, 4, 4)\n",
        "            CBAM(64)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 4 * 4, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 5. Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = BrainCognitionAtlasDataset(\"/content/drive/MyDrive/Brain_Cognition_Atlas\")\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# 6. Training Loop\n",
        "# Training Loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for i, real in enumerate(dataloader):\n",
        "        real = real.to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "        real_labels = torch.ones(real.size(0), 1).to(device)\n",
        "        fake_labels = torch.zeros(real.size(0), 1).to(device)\n",
        "\n",
        "        output_real = discriminator(real)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        # Generate fake images using the generator\n",
        "        z = torch.randn(real.size(0), 100, 1, 1).to(device)  # Latent vector\n",
        "        fake = generator(z)\n",
        "        output_fake = discriminator(fake.detach())\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        # Combine losses and backpropagate\n",
        "        loss_d = loss_real + loss_fake\n",
        "        loss_d.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        output = discriminator(fake)\n",
        "        loss_g = criterion(output, real_labels)\n",
        "        loss_g.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR0JASqT7WJt",
        "outputId": "e8708f9b-8a62-4a0d-ac30-4626de8e92ac"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss D: 0.0891, Loss G: 2.4717\n",
            "Epoch [2/100], Loss D: 0.0076, Loss G: 4.8927\n",
            "Epoch [3/100], Loss D: 0.0021, Loss G: 6.1474\n",
            "Epoch [4/100], Loss D: 0.0009, Loss G: 6.9741\n",
            "Epoch [5/100], Loss D: 0.0005, Loss G: 7.5455\n",
            "Epoch [6/100], Loss D: 0.0003, Loss G: 8.0416\n",
            "Epoch [7/100], Loss D: 0.0002, Loss G: 8.4428\n",
            "Epoch [8/100], Loss D: 0.0001, Loss G: 8.8134\n",
            "Epoch [9/100], Loss D: 0.0001, Loss G: 9.1462\n",
            "Epoch [10/100], Loss D: 0.0001, Loss G: 9.4455\n",
            "Epoch [11/100], Loss D: 0.0001, Loss G: 9.7230\n",
            "Epoch [12/100], Loss D: 0.0000, Loss G: 9.9955\n",
            "Epoch [13/100], Loss D: 0.0000, Loss G: 10.2481\n",
            "Epoch [14/100], Loss D: 0.0000, Loss G: 10.4903\n",
            "Epoch [15/100], Loss D: 0.0000, Loss G: 10.7297\n",
            "Epoch [16/100], Loss D: 0.0000, Loss G: 10.9584\n",
            "Epoch [17/100], Loss D: 0.0000, Loss G: 11.1833\n",
            "Epoch [18/100], Loss D: 0.0000, Loss G: 11.4024\n",
            "Epoch [19/100], Loss D: 0.0000, Loss G: 11.6179\n",
            "Epoch [20/100], Loss D: 0.0000, Loss G: 11.8317\n",
            "Epoch [21/100], Loss D: 0.0000, Loss G: 12.0433\n",
            "Epoch [22/100], Loss D: 0.0000, Loss G: 12.2505\n",
            "Epoch [23/100], Loss D: 0.0000, Loss G: 12.4539\n",
            "Epoch [24/100], Loss D: 0.0000, Loss G: 12.6595\n",
            "Epoch [25/100], Loss D: 0.0000, Loss G: 12.8606\n",
            "Epoch [26/100], Loss D: 0.0000, Loss G: 13.0622\n",
            "Epoch [27/100], Loss D: 0.0000, Loss G: 13.2637\n",
            "Epoch [28/100], Loss D: 0.0000, Loss G: 13.4602\n",
            "Epoch [29/100], Loss D: 0.0000, Loss G: 13.6584\n",
            "Epoch [30/100], Loss D: 0.0000, Loss G: 13.8564\n",
            "Epoch [31/100], Loss D: 0.0000, Loss G: 14.0414\n",
            "Epoch [32/100], Loss D: 0.0000, Loss G: 14.2408\n",
            "Epoch [33/100], Loss D: 0.0000, Loss G: 14.4389\n",
            "Epoch [34/100], Loss D: 0.0000, Loss G: 14.6349\n",
            "Epoch [35/100], Loss D: 0.0000, Loss G: 14.8311\n",
            "Epoch [36/100], Loss D: 0.0000, Loss G: 15.0257\n",
            "Epoch [37/100], Loss D: 0.0000, Loss G: 15.2169\n",
            "Epoch [38/100], Loss D: 0.0000, Loss G: 15.4062\n",
            "Epoch [39/100], Loss D: 0.0000, Loss G: 15.5971\n",
            "Epoch [40/100], Loss D: 0.0000, Loss G: 15.7886\n",
            "Epoch [41/100], Loss D: 0.0000, Loss G: 15.9803\n",
            "Epoch [42/100], Loss D: 0.0000, Loss G: 16.1708\n",
            "Epoch [43/100], Loss D: 0.0000, Loss G: 16.3614\n",
            "Epoch [44/100], Loss D: 0.0000, Loss G: 16.5511\n",
            "Epoch [45/100], Loss D: 0.0000, Loss G: 16.7399\n",
            "Epoch [46/100], Loss D: 0.0000, Loss G: 16.9286\n",
            "Epoch [47/100], Loss D: 0.0000, Loss G: 17.1167\n",
            "Epoch [48/100], Loss D: 0.0000, Loss G: 17.3045\n",
            "Epoch [49/100], Loss D: 0.0000, Loss G: 17.4916\n",
            "Epoch [50/100], Loss D: 0.0000, Loss G: 17.6780\n",
            "Epoch [51/100], Loss D: 0.0000, Loss G: 17.8638\n",
            "Epoch [52/100], Loss D: 0.0000, Loss G: 18.0490\n",
            "Epoch [53/100], Loss D: 0.0000, Loss G: 18.2337\n",
            "Epoch [54/100], Loss D: 0.0000, Loss G: 18.4176\n",
            "Epoch [55/100], Loss D: 0.0000, Loss G: 18.6007\n",
            "Epoch [56/100], Loss D: 0.0000, Loss G: 18.7830\n",
            "Epoch [57/100], Loss D: 0.0000, Loss G: 18.9644\n",
            "Epoch [58/100], Loss D: 0.0000, Loss G: 19.1446\n",
            "Epoch [59/100], Loss D: 0.0000, Loss G: 19.3238\n",
            "Epoch [60/100], Loss D: 0.0000, Loss G: 19.5019\n",
            "Epoch [61/100], Loss D: 0.0000, Loss G: 19.6785\n",
            "Epoch [62/100], Loss D: 0.0000, Loss G: 19.8524\n",
            "Epoch [63/100], Loss D: 0.0000, Loss G: 20.0263\n",
            "Epoch [64/100], Loss D: 0.0000, Loss G: 20.1976\n",
            "Epoch [65/100], Loss D: 0.0000, Loss G: 20.3674\n",
            "Epoch [66/100], Loss D: 0.0000, Loss G: 20.5355\n",
            "Epoch [67/100], Loss D: 0.0000, Loss G: 20.7016\n",
            "Epoch [68/100], Loss D: 0.0000, Loss G: 20.8650\n",
            "Epoch [69/100], Loss D: 0.0000, Loss G: 21.0263\n",
            "Epoch [70/100], Loss D: 0.0000, Loss G: 21.1856\n",
            "Epoch [71/100], Loss D: 0.0000, Loss G: 21.3414\n",
            "Epoch [72/100], Loss D: 0.0000, Loss G: 21.4951\n",
            "Epoch [73/100], Loss D: 0.0000, Loss G: 21.6459\n",
            "Epoch [74/100], Loss D: 0.0000, Loss G: 21.7937\n",
            "Epoch [75/100], Loss D: 0.0000, Loss G: 21.9383\n",
            "Epoch [76/100], Loss D: 0.0000, Loss G: 22.0796\n",
            "Epoch [77/100], Loss D: 0.0000, Loss G: 22.2180\n",
            "Epoch [78/100], Loss D: 0.0000, Loss G: 22.3530\n",
            "Epoch [79/100], Loss D: 0.0000, Loss G: 22.4843\n",
            "Epoch [80/100], Loss D: 0.0000, Loss G: 22.6125\n",
            "Epoch [81/100], Loss D: 0.0000, Loss G: 22.7365\n",
            "Epoch [82/100], Loss D: 0.0000, Loss G: 22.8572\n",
            "Epoch [83/100], Loss D: 0.0000, Loss G: 22.9745\n",
            "Epoch [84/100], Loss D: 0.0000, Loss G: 23.0876\n",
            "Epoch [85/100], Loss D: 0.0000, Loss G: 23.1972\n",
            "Epoch [86/100], Loss D: 0.0000, Loss G: 23.3033\n",
            "Epoch [87/100], Loss D: 0.0000, Loss G: 23.4057\n",
            "Epoch [88/100], Loss D: 0.0000, Loss G: 23.5046\n",
            "Epoch [89/100], Loss D: 0.0000, Loss G: 23.6000\n",
            "Epoch [90/100], Loss D: 0.0000, Loss G: 23.6920\n",
            "Epoch [91/100], Loss D: 0.0000, Loss G: 23.7812\n",
            "Epoch [92/100], Loss D: 0.0000, Loss G: 23.8666\n",
            "Epoch [93/100], Loss D: 0.0000, Loss G: 23.9496\n",
            "Epoch [94/100], Loss D: 0.0000, Loss G: 24.0296\n",
            "Epoch [95/100], Loss D: 0.0000, Loss G: 24.1070\n",
            "Epoch [96/100], Loss D: 0.0000, Loss G: 24.1820\n",
            "Epoch [97/100], Loss D: 0.0000, Loss G: 24.2550\n",
            "Epoch [98/100], Loss D: 0.0000, Loss G: 24.3257\n",
            "Epoch [99/100], Loss D: 0.0000, Loss G: 24.3948\n",
            "Epoch [100/100], Loss D: 0.0000, Loss G: 24.4620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained models after the last epoch (or periodically)\n",
        "torch.save(generator.state_dict(), 'generator.pth')\n",
        "torch.save(discriminator.state_dict(), 'discriminator.pth')\n",
        "\n",
        "print(\"Models saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcT2TVoN9Gob",
        "outputId": "4e833f54-fd4c-4c49-fc60-ef0a395dcfd1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100, 64, kernel_size=4, stride=2, padding=1),  # Output: (64, 4, 4)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),  # Output: (64, 8, 8)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 8, kernel_size=4, stride=2, padding=1),   # Output: (8, 16, 16)\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(8, 8, kernel_size=4, stride=2, padding=1),    # Output: (8, 32, 32)\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(8, 8, kernel_size=4, stride=2, padding=1),    # Output: (8, 9, 372)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(8, 64, kernel_size=4, stride=2, padding=1),  # Output: (64, 4, 4)\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Output: (128, 2, 2)\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Output: (256, 1, 1)\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 1 * 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Instantiate generator and discriminator\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizers\n",
        "lr = 0.0002\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    for i, real_data in enumerate(train_loader):\n",
        "        batch_size = real_data.size(0)\n",
        "        real_data = real_data.to(device)\n",
        "\n",
        "        # Create labels\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        # Real data\n",
        "        output_real = discriminator(real_data)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        # Fake data\n",
        "        z = torch.randn(batch_size, 100, 1, 1).to(device)  # Latent vector\n",
        "        fake_data = generator(z)\n",
        "        output_fake = discriminator(fake_data.detach())\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        loss_d = loss_real + loss_fake\n",
        "\n",
        "        optimizer_d.zero_grad()\n",
        "        loss_d.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train Generator\n",
        "        output_fake = discriminator(fake_data)\n",
        "        loss_g = criterion(output_fake, real_labels)\n",
        "\n",
        "        optimizer_g.zero_grad()\n",
        "        loss_g.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "    print(f'Epoch [{epoch}/{epochs}], Loss D: {loss_d.item()}, Loss G: {loss_g.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "u9Cz9SYo9HDh",
        "outputId": "b47a895d-c9d1-42b8-eac5-79f77ff86796"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [64, 8, 4, 4], expected input[16, 1, 8, 9] to have 8 channels, but got 1 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-ca48ea882419>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Train Discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Real data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0moutput_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mloss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-ca48ea882419>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Instantiate generator and discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 8, 4, 4], expected input[16, 1, 8, 9] to have 8 channels, but got 1 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Channel Attention (Ca)\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return x * self.sigmoid(out)\n",
        "\n",
        "# Spatial Attention (Sa)\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        return x * self.sigmoid(self.conv(x_cat))\n",
        "\n",
        "# CBAM Block that combines Channel and Spatial Attention\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_attention = ChannelAttention(in_channels)\n",
        "        self.spatial_attention = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_attention(x)\n",
        "        x = self.spatial_attention(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "wOsVWe_gIYu_"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Initial dense layer to shape latent vector into the desired size\n",
        "            nn.ConvTranspose2d(100, 64, kernel_size=(4, 4), stride=2, padding=1),  # Output: (64, 7, 7)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Upsample\n",
        "            nn.ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=2, padding=1),  # Output: (64, 14, 14)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # More upsampling\n",
        "            nn.ConvTranspose2d(64, 8, kernel_size=(4, 4), stride=2, padding=1),    # Output: (8, 28, 28)\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final layer to match (8, 9, 372)\n",
        "            nn.ConvTranspose2d(8, 8, kernel_size=(4, 4), stride=2, padding=1),    # Output: (8, 9, 372)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n"
      ],
      "metadata": {
        "id": "I190HDciJJyn"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriminatorWithCBAM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(8, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.AdaptiveAvgPool2d((4, 4)),\n",
        "        )\n",
        "        self.cbam = CBAM(128)  # Add CBAM block to focus on important regions\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 4 * 4, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.cbam(x)  # Apply CBAM to enhance features before classification\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "ClSgqLc0IfoP"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "epochs = 50\n",
        "latent_dim = 100  # Latent vector dimension\n",
        "\n",
        "# Initialize the generator and discriminator\n",
        "generator = Generator().to(device)\n",
        "discriminator = DiscriminatorWithCBAM().to(device)\n",
        "\n",
        "# Optimizers for Generator and Discriminator\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Loss function: Binary Cross Entropy Loss\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Dummy dataset (replace with actual dataset)\n",
        "# Assume we have a DataLoader `train_loader` for the real EEG data\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for real_data in train_loader:\n",
        "        # Move data to device (GPU/CPU)\n",
        "        real_data = real_data.to(device)\n",
        "\n",
        "        # Train Discriminator: Maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        optimizer_d.zero_grad()\n",
        "\n",
        "        # Real data\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        output_real = discriminator(real_data)\n",
        "        loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "        # Fake data\n",
        "        z = torch.randn(batch_size, latent_dim, 1, 1).to(device)  # Latent vector\n",
        "        fake_data = generator(z)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "        output_fake = discriminator(fake_data.detach())  # Detach to avoid updating the generator\n",
        "        loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "        # Total Discriminator loss\n",
        "        loss_d = loss_real + loss_fake\n",
        "        loss_d.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train Generator: Maximize log(D(G(z)))\n",
        "        optimizer_g.zero_grad()\n",
        "\n",
        "        output_fake = discriminator(fake_data)\n",
        "        loss_g = criterion(output_fake, real_labels)  # We want fake data to be classified as real\n",
        "        loss_g.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "    # Print the progress\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {loss_d.item()} | G Loss: {loss_g.item()}\")\n",
        "\n",
        "    # Optionally, save the generated images after every few epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        save_generated_images(epoch + 1, fake_data)  # Save function for monitoring\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "CrboQMexJpkG",
        "outputId": "e62e6138-cb3f-4c36-fec9-bac4b2019931"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [64, 8, 3, 3], expected input[16, 1, 8, 9] to have 8 channels, but got 1 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-d9f783f9fa59>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Real data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mreal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moutput_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mloss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-a24309d1f34b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply CBAM to enhance features before classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 8, 3, 3], expected input[16, 1, 8, 9] to have 8 channels, but got 1 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.randn(batch_size, 100, 1, 1).to(device)  # Latent vector\n",
        "generated_data = generator(z)\n",
        "print(generated_data.shape)  # Should print [batch_size, 8, 9, 372]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaHk_wD_KrGN",
        "outputId": "2e35b377-e335-4cbf-e49e-b61acc4b9833"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 8, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(real_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WjS7ow8KwJX",
        "outputId": "1f5ed8b8-4ac9-444b-de87-1f086e4f3c8b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 8, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for checking Generator output\n",
        "z = torch.randn(batch_size, 100, 1, 1).to(device)  # Latent vector\n",
        "generated_data = generator(z)\n",
        "print(f\"Generated data shape: {generated_data.shape}\")  # Expected [batch_size, 8, 9, 372]\n",
        "\n",
        "# Example for checking real data\n",
        "print(f\"Real data shape: {real_data.shape}\")  # Expected [batch_size, 8, 9, 372]\n",
        "\n",
        "# Pass real data through Discriminator\n",
        "output_real = discriminator(real_data)\n",
        "print(f\"Discriminator output (real): {output_real.shape}\")  # Expected [batch_size, 1]\n",
        "\n",
        "# Pass generated data through Discriminator\n",
        "output_fake = discriminator(generated_data)\n",
        "print(f\"Discriminator output (fake): {output_fake.shape}\")  # Expected [batch_size, 1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "pWRyY28pLH6X",
        "outputId": "b5706f52-0ac2-49b4-d97f-8966146297f8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated data shape: torch.Size([16, 8, 16, 16])\n",
            "Real data shape: torch.Size([16, 1, 8, 9])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [64, 8, 3, 3], expected input[16, 1, 8, 9] to have 8 channels, but got 1 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-d1be05e08ee8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Pass real data through Discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0moutput_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Discriminator output (real): {output_real.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Expected [batch_size, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-a24309d1f34b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply CBAM to enhance features before classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 8, 3, 3], expected input[16, 1, 8, 9] to have 8 channels, but got 1 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSR1NWsSVE1L",
        "outputId": "145cca74-1c45-41c1-f01c-69c9297951ed"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Dataset Class\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class BrainCognitionAtlasDataset(Dataset):\n",
        "    def __init__(self, folder_path):\n",
        "        self.folder_path = folder_path\n",
        "        self.file_list = [f for f in os.listdir(folder_path) if f.endswith(\".npy\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = os.path.join(self.folder_path, self.file_list[idx])\n",
        "        data = np.load(file_path)  # Shape: (8, 9, 372)\n",
        "        data = torch.tensor(data, dtype=torch.float32)\n",
        "        data = data.unsqueeze(0)  # Shape becomes (1, 8, 9, 372)\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "5ugK-CG9VmmI"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Load Data\n",
        "dataset = BrainCognitionAtlasDataset(\"/content/drive/MyDrive/Brain_Cognition_Atlas\")\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "id": "5qLg3aXpVn7x"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Define GAN\n",
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=100):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose3d(latent_dim, 64, (4, 4, 4), stride=2),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose3d(64, 32, (4, 4, 4), stride=2),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose3d(32, 1, (3, 3, 3), stride=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)  # Output shape should be (B, 1, 8, 9, 372)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv3d(1, 32, (3, 3, 3), stride=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv3d(32, 64, (3, 3, 3), stride=2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*1*2*46, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "AhevKtNaVqM5"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists('/content/drive/MyDrive/Brain_Cognition_Atlas'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXziFNITWPu9",
        "outputId": "4ec08904-c629-4317-e63f-170be666ecaa"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BrainCognitionAtlasDataset(Dataset):\n",
        "    def __init__(self, folder_path):\n",
        "        self.file_paths = [os.path.join(folder_path, f) for f in sorted(os.listdir(folder_path)) if f.endswith(\".npy\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = np.load(self.file_paths[idx])  # (8, 9, 372)\n",
        "        data = torch.tensor(data, dtype=torch.float32)\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "HURpsjfkWgWO"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=100):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 8*9*372),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.model(z)\n",
        "        return x.view(-1, 1, 8, 9, 372)  # add channel dimension if needed\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(8*9*372, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "mNIELKp0WhnR"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "batch_size = 16\n",
        "epochs = 1000\n",
        "lr = 0.0002\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Dataset\n",
        "dataset = BrainCognitionAtlasDataset('/content/drive/MyDrive/Brain_Cognition_Atlas')\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Models\n",
        "G = Generator(latent_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "# Optimizers & Loss\n",
        "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr)\n",
        "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    for real_data in dataloader:\n",
        "        real_data = real_data.to(device)  # [B, 8, 9, 372]\n",
        "        real_data = real_data.view(real_data.size(0), -1)\n",
        "\n",
        "        batch_size = real_data.size(0)\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        z = torch.randn(batch_size, latent_dim).to(device)\n",
        "        fake_data = G(z).view(batch_size, -1)\n",
        "\n",
        "        d_loss_real = criterion(D(real_data), real_labels)\n",
        "        d_loss_fake = criterion(D(fake_data.detach()), fake_labels)\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Train Generator\n",
        "        z = torch.randn(batch_size, latent_dim).to(device)\n",
        "        fake_data = G(z).view(batch_size, -1)\n",
        "        g_loss = criterion(D(fake_data), real_labels)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}]  D_loss: {d_loss.item():.4f}  G_loss: {g_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AUC8dkaWjZ9",
        "outputId": "3041d52f-2d83-44ea-c6b3-0f2734d13783"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000]  D_loss: 47.0255  G_loss: 0.9501\n",
            "Epoch [2/1000]  D_loss: 0.7324  G_loss: 1.2319\n",
            "Epoch [3/1000]  D_loss: 0.2795  G_loss: 1.8837\n",
            "Epoch [4/1000]  D_loss: 0.4173  G_loss: 1.8011\n",
            "Epoch [5/1000]  D_loss: 0.6330  G_loss: 2.4005\n",
            "Epoch [6/1000]  D_loss: 0.3630  G_loss: 3.4801\n",
            "Epoch [7/1000]  D_loss: 0.0994  G_loss: 3.0317\n",
            "Epoch [8/1000]  D_loss: 0.4050  G_loss: 3.9798\n",
            "Epoch [9/1000]  D_loss: 0.1957  G_loss: 3.3987\n",
            "Epoch [10/1000]  D_loss: 0.1907  G_loss: 2.9871\n",
            "Epoch [11/1000]  D_loss: 0.0364  G_loss: 4.0255\n",
            "Epoch [12/1000]  D_loss: 0.1870  G_loss: 2.4927\n",
            "Epoch [13/1000]  D_loss: 0.0867  G_loss: 4.6675\n",
            "Epoch [14/1000]  D_loss: 0.0881  G_loss: 4.8789\n",
            "Epoch [15/1000]  D_loss: 0.4269  G_loss: 5.2022\n",
            "Epoch [16/1000]  D_loss: 0.1136  G_loss: 5.8717\n",
            "Epoch [17/1000]  D_loss: 0.0285  G_loss: 4.7485\n",
            "Epoch [18/1000]  D_loss: 0.0388  G_loss: 6.0284\n",
            "Epoch [19/1000]  D_loss: 0.0219  G_loss: 5.0854\n",
            "Epoch [20/1000]  D_loss: 0.0102  G_loss: 6.9606\n",
            "Epoch [21/1000]  D_loss: 0.0133  G_loss: 6.1421\n",
            "Epoch [22/1000]  D_loss: 0.0077  G_loss: 7.1455\n",
            "Epoch [23/1000]  D_loss: 0.0082  G_loss: 6.9562\n",
            "Epoch [24/1000]  D_loss: 0.1222  G_loss: 6.4558\n",
            "Epoch [25/1000]  D_loss: 0.0217  G_loss: 6.9438\n",
            "Epoch [26/1000]  D_loss: 0.0670  G_loss: 7.1467\n",
            "Epoch [27/1000]  D_loss: 0.0079  G_loss: 6.9757\n",
            "Epoch [28/1000]  D_loss: 0.0841  G_loss: 7.4960\n",
            "Epoch [29/1000]  D_loss: 0.0033  G_loss: 6.2162\n",
            "Epoch [30/1000]  D_loss: 0.0061  G_loss: 7.0079\n",
            "Epoch [31/1000]  D_loss: 0.0033  G_loss: 7.2852\n",
            "Epoch [32/1000]  D_loss: 0.0046  G_loss: 8.0239\n",
            "Epoch [33/1000]  D_loss: 0.0047  G_loss: 8.0722\n",
            "Epoch [34/1000]  D_loss: 0.0208  G_loss: 8.6017\n",
            "Epoch [35/1000]  D_loss: 0.0115  G_loss: 7.6915\n",
            "Epoch [36/1000]  D_loss: 0.0019  G_loss: 6.5885\n",
            "Epoch [37/1000]  D_loss: 0.0022  G_loss: 8.2798\n",
            "Epoch [38/1000]  D_loss: 0.0043  G_loss: 8.6054\n",
            "Epoch [39/1000]  D_loss: 0.0226  G_loss: 7.3416\n",
            "Epoch [40/1000]  D_loss: 0.0368  G_loss: 8.1248\n",
            "Epoch [41/1000]  D_loss: 0.0014  G_loss: 8.0996\n",
            "Epoch [42/1000]  D_loss: 0.0118  G_loss: 7.3406\n",
            "Epoch [43/1000]  D_loss: 0.0032  G_loss: 7.4524\n",
            "Epoch [44/1000]  D_loss: 0.0442  G_loss: 8.6985\n",
            "Epoch [45/1000]  D_loss: 0.0061  G_loss: 7.5771\n",
            "Epoch [46/1000]  D_loss: 0.0042  G_loss: 7.4102\n",
            "Epoch [47/1000]  D_loss: 0.0018  G_loss: 7.8351\n",
            "Epoch [48/1000]  D_loss: 0.0014  G_loss: 7.2319\n",
            "Epoch [49/1000]  D_loss: 0.0569  G_loss: 6.8489\n",
            "Epoch [50/1000]  D_loss: 0.0052  G_loss: 7.7331\n",
            "Epoch [51/1000]  D_loss: 0.0021  G_loss: 6.6586\n",
            "Epoch [52/1000]  D_loss: 0.0037  G_loss: 7.8831\n",
            "Epoch [53/1000]  D_loss: 0.0166  G_loss: 7.8462\n",
            "Epoch [54/1000]  D_loss: 0.0044  G_loss: 7.5903\n",
            "Epoch [55/1000]  D_loss: 0.0043  G_loss: 8.0451\n",
            "Epoch [56/1000]  D_loss: 0.0194  G_loss: 7.8153\n",
            "Epoch [57/1000]  D_loss: 0.0721  G_loss: 7.3977\n",
            "Epoch [58/1000]  D_loss: 0.0028  G_loss: 8.0164\n",
            "Epoch [59/1000]  D_loss: 0.0476  G_loss: 6.3753\n",
            "Epoch [60/1000]  D_loss: 0.0022  G_loss: 7.3269\n",
            "Epoch [61/1000]  D_loss: 0.0057  G_loss: 7.4989\n",
            "Epoch [62/1000]  D_loss: 0.0033  G_loss: 7.8068\n",
            "Epoch [63/1000]  D_loss: 0.0015  G_loss: 8.2177\n",
            "Epoch [64/1000]  D_loss: 0.0058  G_loss: 8.0370\n",
            "Epoch [65/1000]  D_loss: 0.0029  G_loss: 8.1629\n",
            "Epoch [66/1000]  D_loss: 0.0417  G_loss: 8.0433\n",
            "Epoch [67/1000]  D_loss: 0.0044  G_loss: 6.7730\n",
            "Epoch [68/1000]  D_loss: 0.0322  G_loss: 7.7144\n",
            "Epoch [69/1000]  D_loss: 0.2355  G_loss: 6.9823\n",
            "Epoch [70/1000]  D_loss: 0.0056  G_loss: 6.1659\n",
            "Epoch [71/1000]  D_loss: 0.0060  G_loss: 6.8924\n",
            "Epoch [72/1000]  D_loss: 0.0102  G_loss: 8.2515\n",
            "Epoch [73/1000]  D_loss: 0.0029  G_loss: 7.8937\n",
            "Epoch [74/1000]  D_loss: 0.0104  G_loss: 8.0482\n",
            "Epoch [75/1000]  D_loss: 0.0118  G_loss: 7.6916\n",
            "Epoch [76/1000]  D_loss: 0.0025  G_loss: 7.6374\n",
            "Epoch [77/1000]  D_loss: 0.0058  G_loss: 7.4643\n",
            "Epoch [78/1000]  D_loss: 0.0017  G_loss: 8.1597\n",
            "Epoch [79/1000]  D_loss: 0.0034  G_loss: 8.5588\n",
            "Epoch [80/1000]  D_loss: 0.0648  G_loss: 7.4030\n",
            "Epoch [81/1000]  D_loss: 0.0020  G_loss: 8.2083\n",
            "Epoch [82/1000]  D_loss: 0.0066  G_loss: 7.7377\n",
            "Epoch [83/1000]  D_loss: 0.0225  G_loss: 7.6767\n",
            "Epoch [84/1000]  D_loss: 0.0857  G_loss: 6.1443\n",
            "Epoch [85/1000]  D_loss: 0.0100  G_loss: 6.8898\n",
            "Epoch [86/1000]  D_loss: 0.0114  G_loss: 7.6715\n",
            "Epoch [87/1000]  D_loss: 0.0020  G_loss: 9.0058\n",
            "Epoch [88/1000]  D_loss: 0.0029  G_loss: 8.7705\n",
            "Epoch [89/1000]  D_loss: 0.0297  G_loss: 7.5097\n",
            "Epoch [90/1000]  D_loss: 0.0143  G_loss: 7.6550\n",
            "Epoch [91/1000]  D_loss: 0.0018  G_loss: 7.5483\n",
            "Epoch [92/1000]  D_loss: 0.0005  G_loss: 7.8076\n",
            "Epoch [93/1000]  D_loss: 0.0027  G_loss: 8.0760\n",
            "Epoch [94/1000]  D_loss: 0.0174  G_loss: 8.1222\n",
            "Epoch [95/1000]  D_loss: 0.0045  G_loss: 7.3671\n",
            "Epoch [96/1000]  D_loss: 0.0040  G_loss: 7.8482\n",
            "Epoch [97/1000]  D_loss: 0.0141  G_loss: 8.3659\n",
            "Epoch [98/1000]  D_loss: 0.0387  G_loss: 8.2251\n",
            "Epoch [99/1000]  D_loss: 0.0035  G_loss: 6.4871\n",
            "Epoch [100/1000]  D_loss: 0.0043  G_loss: 7.6775\n",
            "Epoch [101/1000]  D_loss: 0.0031  G_loss: 7.7988\n",
            "Epoch [102/1000]  D_loss: 0.0014  G_loss: 7.9147\n",
            "Epoch [103/1000]  D_loss: 0.0084  G_loss: 6.7597\n",
            "Epoch [104/1000]  D_loss: 0.0103  G_loss: 7.6982\n",
            "Epoch [105/1000]  D_loss: 0.0016  G_loss: 7.6215\n",
            "Epoch [106/1000]  D_loss: 0.0085  G_loss: 7.9802\n",
            "Epoch [107/1000]  D_loss: 0.0668  G_loss: 7.9130\n",
            "Epoch [108/1000]  D_loss: 0.0033  G_loss: 6.7905\n",
            "Epoch [109/1000]  D_loss: 0.0016  G_loss: 6.5748\n",
            "Epoch [110/1000]  D_loss: 0.0063  G_loss: 6.8994\n",
            "Epoch [111/1000]  D_loss: 0.0093  G_loss: 7.1641\n",
            "Epoch [112/1000]  D_loss: 0.0084  G_loss: 8.5849\n",
            "Epoch [113/1000]  D_loss: 0.0025  G_loss: 7.1366\n",
            "Epoch [114/1000]  D_loss: 0.0367  G_loss: 8.0280\n",
            "Epoch [115/1000]  D_loss: 0.0042  G_loss: 6.5680\n",
            "Epoch [116/1000]  D_loss: 0.0031  G_loss: 6.6779\n",
            "Epoch [117/1000]  D_loss: 0.0034  G_loss: 7.0698\n",
            "Epoch [118/1000]  D_loss: 0.0054  G_loss: 6.9966\n",
            "Epoch [119/1000]  D_loss: 0.0032  G_loss: 6.0933\n",
            "Epoch [120/1000]  D_loss: 0.1508  G_loss: 6.9143\n",
            "Epoch [121/1000]  D_loss: 0.0025  G_loss: 6.5377\n",
            "Epoch [122/1000]  D_loss: 0.0280  G_loss: 7.7996\n",
            "Epoch [123/1000]  D_loss: 0.0060  G_loss: 6.7864\n",
            "Epoch [124/1000]  D_loss: 0.0015  G_loss: 6.5396\n",
            "Epoch [125/1000]  D_loss: 0.0133  G_loss: 7.9783\n",
            "Epoch [126/1000]  D_loss: 0.0135  G_loss: 7.1729\n",
            "Epoch [127/1000]  D_loss: 0.0033  G_loss: 6.8459\n",
            "Epoch [128/1000]  D_loss: 0.0018  G_loss: 6.9787\n",
            "Epoch [129/1000]  D_loss: 0.1690  G_loss: 8.2427\n",
            "Epoch [130/1000]  D_loss: 0.0106  G_loss: 8.0072\n",
            "Epoch [131/1000]  D_loss: 0.0019  G_loss: 8.0510\n",
            "Epoch [132/1000]  D_loss: 0.0067  G_loss: 6.9374\n",
            "Epoch [133/1000]  D_loss: 0.0424  G_loss: 6.7074\n",
            "Epoch [134/1000]  D_loss: 0.0030  G_loss: 6.6728\n",
            "Epoch [135/1000]  D_loss: 0.0045  G_loss: 5.8208\n",
            "Epoch [136/1000]  D_loss: 0.0267  G_loss: 5.5803\n",
            "Epoch [137/1000]  D_loss: 0.0016  G_loss: 7.2355\n",
            "Epoch [138/1000]  D_loss: 0.0081  G_loss: 6.0924\n",
            "Epoch [139/1000]  D_loss: 0.0061  G_loss: 7.2762\n",
            "Epoch [140/1000]  D_loss: 0.0165  G_loss: 6.0739\n",
            "Epoch [141/1000]  D_loss: 0.0070  G_loss: 7.7852\n",
            "Epoch [142/1000]  D_loss: 0.0010  G_loss: 7.6080\n",
            "Epoch [143/1000]  D_loss: 0.0873  G_loss: 7.5149\n",
            "Epoch [144/1000]  D_loss: 0.0370  G_loss: 7.5513\n",
            "Epoch [145/1000]  D_loss: 0.1557  G_loss: 6.1391\n",
            "Epoch [146/1000]  D_loss: 0.0062  G_loss: 5.8484\n",
            "Epoch [147/1000]  D_loss: 0.0625  G_loss: 8.4428\n",
            "Epoch [148/1000]  D_loss: 0.2188  G_loss: 7.4498\n",
            "Epoch [149/1000]  D_loss: 0.0290  G_loss: 7.8031\n",
            "Epoch [150/1000]  D_loss: 0.2146  G_loss: 7.5336\n",
            "Epoch [151/1000]  D_loss: 0.3939  G_loss: 6.9301\n",
            "Epoch [152/1000]  D_loss: 0.0083  G_loss: 6.4461\n",
            "Epoch [153/1000]  D_loss: 0.0124  G_loss: 6.3624\n",
            "Epoch [154/1000]  D_loss: 0.0365  G_loss: 5.1845\n",
            "Epoch [155/1000]  D_loss: 0.0172  G_loss: 6.7511\n",
            "Epoch [156/1000]  D_loss: 0.0178  G_loss: 7.4389\n",
            "Epoch [157/1000]  D_loss: 0.2712  G_loss: 7.5756\n",
            "Epoch [158/1000]  D_loss: 0.4094  G_loss: 8.6417\n",
            "Epoch [159/1000]  D_loss: 0.0032  G_loss: 5.4239\n",
            "Epoch [160/1000]  D_loss: 0.0056  G_loss: 7.8330\n",
            "Epoch [161/1000]  D_loss: 0.0062  G_loss: 6.9217\n",
            "Epoch [162/1000]  D_loss: 0.0046  G_loss: 8.6755\n",
            "Epoch [163/1000]  D_loss: 0.0277  G_loss: 8.2841\n",
            "Epoch [164/1000]  D_loss: 0.1518  G_loss: 7.0778\n",
            "Epoch [165/1000]  D_loss: 0.4183  G_loss: 6.8308\n",
            "Epoch [166/1000]  D_loss: 0.0059  G_loss: 7.5956\n",
            "Epoch [167/1000]  D_loss: 0.1068  G_loss: 8.0411\n",
            "Epoch [168/1000]  D_loss: 0.0613  G_loss: 7.0164\n",
            "Epoch [169/1000]  D_loss: 0.0044  G_loss: 7.7725\n",
            "Epoch [170/1000]  D_loss: 0.0053  G_loss: 7.2106\n",
            "Epoch [171/1000]  D_loss: 0.0255  G_loss: 7.4684\n",
            "Epoch [172/1000]  D_loss: 0.0257  G_loss: 7.0409\n",
            "Epoch [173/1000]  D_loss: 0.0016  G_loss: 7.2736\n",
            "Epoch [174/1000]  D_loss: 0.0022  G_loss: 7.1615\n",
            "Epoch [175/1000]  D_loss: 0.0127  G_loss: 7.0584\n",
            "Epoch [176/1000]  D_loss: 0.0043  G_loss: 6.5466\n",
            "Epoch [177/1000]  D_loss: 0.0882  G_loss: 7.6987\n",
            "Epoch [178/1000]  D_loss: 0.0371  G_loss: 6.5150\n",
            "Epoch [179/1000]  D_loss: 0.0757  G_loss: 7.2857\n",
            "Epoch [180/1000]  D_loss: 0.0187  G_loss: 6.9558\n",
            "Epoch [181/1000]  D_loss: 0.0277  G_loss: 7.0893\n",
            "Epoch [182/1000]  D_loss: 0.0011  G_loss: 6.4827\n",
            "Epoch [183/1000]  D_loss: 0.1012  G_loss: 7.0347\n",
            "Epoch [184/1000]  D_loss: 0.0139  G_loss: 7.5814\n",
            "Epoch [185/1000]  D_loss: 0.0706  G_loss: 7.1658\n",
            "Epoch [186/1000]  D_loss: 0.0108  G_loss: 7.8930\n",
            "Epoch [187/1000]  D_loss: 0.0689  G_loss: 5.9521\n",
            "Epoch [188/1000]  D_loss: 0.0147  G_loss: 7.0661\n",
            "Epoch [189/1000]  D_loss: 0.0041  G_loss: 7.9490\n",
            "Epoch [190/1000]  D_loss: 0.0150  G_loss: 7.5082\n",
            "Epoch [191/1000]  D_loss: 0.0033  G_loss: 6.5159\n",
            "Epoch [192/1000]  D_loss: 0.8335  G_loss: 6.8125\n",
            "Epoch [193/1000]  D_loss: 0.0143  G_loss: 6.7352\n",
            "Epoch [194/1000]  D_loss: 0.1397  G_loss: 6.2482\n",
            "Epoch [195/1000]  D_loss: 0.0094  G_loss: 7.3561\n",
            "Epoch [196/1000]  D_loss: 0.0316  G_loss: 5.5548\n",
            "Epoch [197/1000]  D_loss: 0.0046  G_loss: 6.6773\n",
            "Epoch [198/1000]  D_loss: 0.0050  G_loss: 9.1244\n",
            "Epoch [199/1000]  D_loss: 0.0039  G_loss: 6.2978\n",
            "Epoch [200/1000]  D_loss: 0.0023  G_loss: 7.8648\n",
            "Epoch [201/1000]  D_loss: 0.1893  G_loss: 7.3305\n",
            "Epoch [202/1000]  D_loss: 0.1139  G_loss: 6.5086\n",
            "Epoch [203/1000]  D_loss: 0.0653  G_loss: 5.9243\n",
            "Epoch [204/1000]  D_loss: 0.0079  G_loss: 7.5258\n",
            "Epoch [205/1000]  D_loss: 0.0789  G_loss: 6.9319\n",
            "Epoch [206/1000]  D_loss: 0.0042  G_loss: 5.3759\n",
            "Epoch [207/1000]  D_loss: 0.5741  G_loss: 7.3923\n",
            "Epoch [208/1000]  D_loss: 0.3766  G_loss: 7.3782\n",
            "Epoch [209/1000]  D_loss: 0.0962  G_loss: 7.2361\n",
            "Epoch [210/1000]  D_loss: 0.0549  G_loss: 7.9934\n",
            "Epoch [211/1000]  D_loss: 0.0050  G_loss: 7.2292\n",
            "Epoch [212/1000]  D_loss: 0.3535  G_loss: 7.1730\n",
            "Epoch [213/1000]  D_loss: 0.2704  G_loss: 7.6012\n",
            "Epoch [214/1000]  D_loss: 0.0923  G_loss: 6.6515\n",
            "Epoch [215/1000]  D_loss: 0.0502  G_loss: 7.3956\n",
            "Epoch [216/1000]  D_loss: 0.0025  G_loss: 7.0000\n",
            "Epoch [217/1000]  D_loss: 0.0557  G_loss: 7.9097\n",
            "Epoch [218/1000]  D_loss: 0.0466  G_loss: 7.0819\n",
            "Epoch [219/1000]  D_loss: 0.3435  G_loss: 7.9320\n",
            "Epoch [220/1000]  D_loss: 0.2555  G_loss: 6.7928\n",
            "Epoch [221/1000]  D_loss: 0.2004  G_loss: 6.2764\n",
            "Epoch [222/1000]  D_loss: 0.0573  G_loss: 8.1301\n",
            "Epoch [223/1000]  D_loss: 0.5894  G_loss: 6.8304\n",
            "Epoch [224/1000]  D_loss: 0.0012  G_loss: 7.4849\n",
            "Epoch [225/1000]  D_loss: 0.0506  G_loss: 8.3546\n",
            "Epoch [226/1000]  D_loss: 0.1428  G_loss: 5.4569\n",
            "Epoch [227/1000]  D_loss: 0.0386  G_loss: 8.9129\n",
            "Epoch [228/1000]  D_loss: 0.5404  G_loss: 8.0032\n",
            "Epoch [229/1000]  D_loss: 0.3310  G_loss: 7.9044\n",
            "Epoch [230/1000]  D_loss: 0.6683  G_loss: 8.0323\n",
            "Epoch [231/1000]  D_loss: 0.0520  G_loss: 7.0669\n",
            "Epoch [232/1000]  D_loss: 0.7539  G_loss: 8.0090\n",
            "Epoch [233/1000]  D_loss: 0.0670  G_loss: 7.9533\n",
            "Epoch [234/1000]  D_loss: 0.1369  G_loss: 7.2746\n",
            "Epoch [235/1000]  D_loss: 0.2735  G_loss: 8.3844\n",
            "Epoch [236/1000]  D_loss: 0.0400  G_loss: 7.1172\n",
            "Epoch [237/1000]  D_loss: 0.2253  G_loss: 8.8481\n",
            "Epoch [238/1000]  D_loss: 0.2778  G_loss: 7.2305\n",
            "Epoch [239/1000]  D_loss: 0.4712  G_loss: 8.0372\n",
            "Epoch [240/1000]  D_loss: 0.3279  G_loss: 7.2767\n",
            "Epoch [241/1000]  D_loss: 0.3132  G_loss: 8.1549\n",
            "Epoch [242/1000]  D_loss: 0.2110  G_loss: 7.0999\n",
            "Epoch [243/1000]  D_loss: 0.4748  G_loss: 7.1900\n",
            "Epoch [244/1000]  D_loss: 0.3623  G_loss: 8.8473\n",
            "Epoch [245/1000]  D_loss: 0.0053  G_loss: 7.9784\n",
            "Epoch [246/1000]  D_loss: 0.0593  G_loss: 7.9815\n",
            "Epoch [247/1000]  D_loss: 0.3106  G_loss: 7.9042\n",
            "Epoch [248/1000]  D_loss: 0.2779  G_loss: 7.6284\n",
            "Epoch [249/1000]  D_loss: 0.0428  G_loss: 7.7561\n",
            "Epoch [250/1000]  D_loss: 0.0088  G_loss: 8.3564\n",
            "Epoch [251/1000]  D_loss: 0.2162  G_loss: 6.2648\n",
            "Epoch [252/1000]  D_loss: 0.0008  G_loss: 8.6570\n",
            "Epoch [253/1000]  D_loss: 0.8541  G_loss: 7.2245\n",
            "Epoch [254/1000]  D_loss: 0.0207  G_loss: 7.8082\n",
            "Epoch [255/1000]  D_loss: 0.0647  G_loss: 5.9855\n",
            "Epoch [256/1000]  D_loss: 0.2518  G_loss: 9.9975\n",
            "Epoch [257/1000]  D_loss: 0.1781  G_loss: 7.2502\n",
            "Epoch [258/1000]  D_loss: 0.6807  G_loss: 8.8871\n",
            "Epoch [259/1000]  D_loss: 0.2630  G_loss: 9.3442\n",
            "Epoch [260/1000]  D_loss: 0.0119  G_loss: 7.7355\n",
            "Epoch [261/1000]  D_loss: 0.4561  G_loss: 6.7764\n",
            "Epoch [262/1000]  D_loss: 0.6237  G_loss: 8.9070\n",
            "Epoch [263/1000]  D_loss: 0.3460  G_loss: 7.7801\n",
            "Epoch [264/1000]  D_loss: 0.0615  G_loss: 7.4796\n",
            "Epoch [265/1000]  D_loss: 0.4331  G_loss: 8.9643\n",
            "Epoch [266/1000]  D_loss: 0.0905  G_loss: 7.1587\n",
            "Epoch [267/1000]  D_loss: 0.6526  G_loss: 8.4832\n",
            "Epoch [268/1000]  D_loss: 0.1933  G_loss: 5.7740\n",
            "Epoch [269/1000]  D_loss: 0.1903  G_loss: 6.8121\n",
            "Epoch [270/1000]  D_loss: 0.0782  G_loss: 5.8441\n",
            "Epoch [271/1000]  D_loss: 0.2989  G_loss: 7.5624\n",
            "Epoch [272/1000]  D_loss: 0.0002  G_loss: 7.4703\n",
            "Epoch [273/1000]  D_loss: 0.0125  G_loss: 9.6928\n",
            "Epoch [274/1000]  D_loss: 0.0188  G_loss: 7.6619\n",
            "Epoch [275/1000]  D_loss: 0.0341  G_loss: 7.1909\n",
            "Epoch [276/1000]  D_loss: 0.1049  G_loss: 7.0569\n",
            "Epoch [277/1000]  D_loss: 0.2225  G_loss: 7.1246\n",
            "Epoch [278/1000]  D_loss: 0.1845  G_loss: 6.8455\n",
            "Epoch [279/1000]  D_loss: 0.1961  G_loss: 6.1294\n",
            "Epoch [280/1000]  D_loss: 0.5586  G_loss: 9.2749\n",
            "Epoch [281/1000]  D_loss: 0.3022  G_loss: 6.3119\n",
            "Epoch [282/1000]  D_loss: 0.0552  G_loss: 5.7810\n",
            "Epoch [283/1000]  D_loss: 0.0108  G_loss: 5.6868\n",
            "Epoch [284/1000]  D_loss: 0.4336  G_loss: 7.7555\n",
            "Epoch [285/1000]  D_loss: 0.0518  G_loss: 9.2617\n",
            "Epoch [286/1000]  D_loss: 0.7947  G_loss: 7.3312\n",
            "Epoch [287/1000]  D_loss: 0.2188  G_loss: 10.2742\n",
            "Epoch [288/1000]  D_loss: 0.0006  G_loss: 9.2461\n",
            "Epoch [289/1000]  D_loss: 0.3534  G_loss: 7.1461\n",
            "Epoch [290/1000]  D_loss: 0.7854  G_loss: 6.5579\n",
            "Epoch [291/1000]  D_loss: 0.0823  G_loss: 9.1997\n",
            "Epoch [292/1000]  D_loss: 0.1314  G_loss: 9.0381\n",
            "Epoch [293/1000]  D_loss: 0.5997  G_loss: 7.9309\n",
            "Epoch [294/1000]  D_loss: 0.0018  G_loss: 7.0217\n",
            "Epoch [295/1000]  D_loss: 0.5717  G_loss: 9.0582\n",
            "Epoch [296/1000]  D_loss: 0.0668  G_loss: 6.0622\n",
            "Epoch [297/1000]  D_loss: 0.3555  G_loss: 6.8320\n",
            "Epoch [298/1000]  D_loss: 0.2526  G_loss: 9.3915\n",
            "Epoch [299/1000]  D_loss: 0.0013  G_loss: 6.3588\n",
            "Epoch [300/1000]  D_loss: 0.3092  G_loss: 7.3484\n",
            "Epoch [301/1000]  D_loss: 1.3007  G_loss: 8.0692\n",
            "Epoch [302/1000]  D_loss: 0.0040  G_loss: 7.0129\n",
            "Epoch [303/1000]  D_loss: 0.3973  G_loss: 6.3712\n",
            "Epoch [304/1000]  D_loss: 0.8134  G_loss: 6.9500\n",
            "Epoch [305/1000]  D_loss: 0.6587  G_loss: 9.3348\n",
            "Epoch [306/1000]  D_loss: 0.3638  G_loss: 7.2351\n",
            "Epoch [307/1000]  D_loss: 0.0726  G_loss: 7.5509\n",
            "Epoch [308/1000]  D_loss: 0.0189  G_loss: 7.4524\n",
            "Epoch [309/1000]  D_loss: 0.0212  G_loss: 9.0559\n",
            "Epoch [310/1000]  D_loss: 0.5651  G_loss: 10.1876\n",
            "Epoch [311/1000]  D_loss: 0.1011  G_loss: 7.9128\n",
            "Epoch [312/1000]  D_loss: 0.1276  G_loss: 9.0468\n",
            "Epoch [313/1000]  D_loss: 0.0703  G_loss: 7.3418\n",
            "Epoch [314/1000]  D_loss: 0.0776  G_loss: 6.8930\n",
            "Epoch [315/1000]  D_loss: 0.1288  G_loss: 8.4284\n",
            "Epoch [316/1000]  D_loss: 0.3743  G_loss: 6.2616\n",
            "Epoch [317/1000]  D_loss: 0.8390  G_loss: 8.5212\n",
            "Epoch [318/1000]  D_loss: 0.0201  G_loss: 7.0272\n",
            "Epoch [319/1000]  D_loss: 0.0018  G_loss: 8.3985\n",
            "Epoch [320/1000]  D_loss: 0.4130  G_loss: 9.5815\n",
            "Epoch [321/1000]  D_loss: 0.8374  G_loss: 7.8744\n",
            "Epoch [322/1000]  D_loss: 0.1453  G_loss: 7.8576\n",
            "Epoch [323/1000]  D_loss: 0.4394  G_loss: 8.2559\n",
            "Epoch [324/1000]  D_loss: 0.2470  G_loss: 7.2215\n",
            "Epoch [325/1000]  D_loss: 0.2537  G_loss: 10.3348\n",
            "Epoch [326/1000]  D_loss: 0.2345  G_loss: 9.8046\n",
            "Epoch [327/1000]  D_loss: 0.2249  G_loss: 9.0771\n",
            "Epoch [328/1000]  D_loss: 0.0350  G_loss: 9.3470\n",
            "Epoch [329/1000]  D_loss: 0.4227  G_loss: 8.4059\n",
            "Epoch [330/1000]  D_loss: 0.1060  G_loss: 7.1581\n",
            "Epoch [331/1000]  D_loss: 0.5089  G_loss: 6.4657\n",
            "Epoch [332/1000]  D_loss: 0.1971  G_loss: 6.5208\n",
            "Epoch [333/1000]  D_loss: 0.3232  G_loss: 9.7118\n",
            "Epoch [334/1000]  D_loss: 0.6585  G_loss: 5.6606\n",
            "Epoch [335/1000]  D_loss: 0.3967  G_loss: 6.0689\n",
            "Epoch [336/1000]  D_loss: 0.0201  G_loss: 9.2284\n",
            "Epoch [337/1000]  D_loss: 0.2537  G_loss: 8.1701\n",
            "Epoch [338/1000]  D_loss: 0.0278  G_loss: 9.0447\n",
            "Epoch [339/1000]  D_loss: 0.3291  G_loss: 9.8284\n",
            "Epoch [340/1000]  D_loss: 0.0830  G_loss: 6.8110\n",
            "Epoch [341/1000]  D_loss: 1.2249  G_loss: 6.6452\n",
            "Epoch [342/1000]  D_loss: 0.2680  G_loss: 8.3310\n",
            "Epoch [343/1000]  D_loss: 0.0908  G_loss: 8.1802\n",
            "Epoch [344/1000]  D_loss: 0.1107  G_loss: 6.7724\n",
            "Epoch [345/1000]  D_loss: 0.1045  G_loss: 7.2245\n",
            "Epoch [346/1000]  D_loss: 0.4730  G_loss: 8.6589\n",
            "Epoch [347/1000]  D_loss: 0.1033  G_loss: 6.6969\n",
            "Epoch [348/1000]  D_loss: 0.4757  G_loss: 7.0133\n",
            "Epoch [349/1000]  D_loss: 1.2465  G_loss: 7.9260\n",
            "Epoch [350/1000]  D_loss: 0.2221  G_loss: 7.6840\n",
            "Epoch [351/1000]  D_loss: 0.3620  G_loss: 8.2172\n",
            "Epoch [352/1000]  D_loss: 0.8889  G_loss: 8.1477\n",
            "Epoch [353/1000]  D_loss: 0.6403  G_loss: 8.7599\n",
            "Epoch [354/1000]  D_loss: 0.4517  G_loss: 8.5445\n",
            "Epoch [355/1000]  D_loss: 0.7935  G_loss: 7.9896\n",
            "Epoch [356/1000]  D_loss: 0.5081  G_loss: 9.1312\n",
            "Epoch [357/1000]  D_loss: 0.2134  G_loss: 6.8782\n",
            "Epoch [358/1000]  D_loss: 0.1842  G_loss: 7.6726\n",
            "Epoch [359/1000]  D_loss: 0.1403  G_loss: 5.2515\n",
            "Epoch [360/1000]  D_loss: 0.1776  G_loss: 7.1042\n",
            "Epoch [361/1000]  D_loss: 0.0697  G_loss: 8.2832\n",
            "Epoch [362/1000]  D_loss: 0.0229  G_loss: 8.8228\n",
            "Epoch [363/1000]  D_loss: 0.1611  G_loss: 7.5520\n",
            "Epoch [364/1000]  D_loss: 0.1149  G_loss: 6.6726\n",
            "Epoch [365/1000]  D_loss: 0.1916  G_loss: 6.2944\n",
            "Epoch [366/1000]  D_loss: 0.1657  G_loss: 9.7427\n",
            "Epoch [367/1000]  D_loss: 0.2775  G_loss: 6.5570\n",
            "Epoch [368/1000]  D_loss: 0.3120  G_loss: 8.1225\n",
            "Epoch [369/1000]  D_loss: 0.0539  G_loss: 8.4210\n",
            "Epoch [370/1000]  D_loss: 0.3710  G_loss: 7.0674\n",
            "Epoch [371/1000]  D_loss: 0.0452  G_loss: 7.6796\n",
            "Epoch [372/1000]  D_loss: 0.2231  G_loss: 8.5721\n",
            "Epoch [373/1000]  D_loss: 0.2021  G_loss: 7.4933\n",
            "Epoch [374/1000]  D_loss: 0.0212  G_loss: 8.7503\n",
            "Epoch [375/1000]  D_loss: 0.1675  G_loss: 5.8849\n",
            "Epoch [376/1000]  D_loss: 0.4823  G_loss: 6.1145\n",
            "Epoch [377/1000]  D_loss: 0.2705  G_loss: 9.6031\n",
            "Epoch [378/1000]  D_loss: 0.0494  G_loss: 7.5668\n",
            "Epoch [379/1000]  D_loss: 0.8929  G_loss: 8.1936\n",
            "Epoch [380/1000]  D_loss: 0.0840  G_loss: 7.2065\n",
            "Epoch [381/1000]  D_loss: 0.2777  G_loss: 7.5827\n",
            "Epoch [382/1000]  D_loss: 0.0211  G_loss: 5.9056\n",
            "Epoch [383/1000]  D_loss: 0.2593  G_loss: 8.1471\n",
            "Epoch [384/1000]  D_loss: 0.3210  G_loss: 6.3804\n",
            "Epoch [385/1000]  D_loss: 0.5378  G_loss: 7.6705\n",
            "Epoch [386/1000]  D_loss: 0.6682  G_loss: 6.6609\n",
            "Epoch [387/1000]  D_loss: 0.2058  G_loss: 9.1999\n",
            "Epoch [388/1000]  D_loss: 0.0461  G_loss: 6.4146\n",
            "Epoch [389/1000]  D_loss: 0.0196  G_loss: 6.7813\n",
            "Epoch [390/1000]  D_loss: 0.0106  G_loss: 7.9673\n",
            "Epoch [391/1000]  D_loss: 0.0089  G_loss: 6.8709\n",
            "Epoch [392/1000]  D_loss: 0.1006  G_loss: 7.0084\n",
            "Epoch [393/1000]  D_loss: 0.5246  G_loss: 8.0138\n",
            "Epoch [394/1000]  D_loss: 0.0048  G_loss: 9.4120\n",
            "Epoch [395/1000]  D_loss: 0.0447  G_loss: 9.0108\n",
            "Epoch [396/1000]  D_loss: 0.0014  G_loss: 7.3187\n",
            "Epoch [397/1000]  D_loss: 0.2394  G_loss: 6.2207\n",
            "Epoch [398/1000]  D_loss: 0.3357  G_loss: 7.8629\n",
            "Epoch [399/1000]  D_loss: 0.2209  G_loss: 9.0335\n",
            "Epoch [400/1000]  D_loss: 0.8662  G_loss: 7.8889\n",
            "Epoch [401/1000]  D_loss: 0.6083  G_loss: 5.5118\n",
            "Epoch [402/1000]  D_loss: 0.0252  G_loss: 7.3296\n",
            "Epoch [403/1000]  D_loss: 0.3656  G_loss: 6.7489\n",
            "Epoch [404/1000]  D_loss: 0.3059  G_loss: 7.4175\n",
            "Epoch [405/1000]  D_loss: 0.1207  G_loss: 5.9144\n",
            "Epoch [406/1000]  D_loss: 0.1152  G_loss: 7.6541\n",
            "Epoch [407/1000]  D_loss: 1.0682  G_loss: 6.4588\n",
            "Epoch [408/1000]  D_loss: 0.3634  G_loss: 8.0107\n",
            "Epoch [409/1000]  D_loss: 0.0304  G_loss: 9.3384\n",
            "Epoch [410/1000]  D_loss: 0.0215  G_loss: 7.9633\n",
            "Epoch [411/1000]  D_loss: 0.4573  G_loss: 6.0589\n",
            "Epoch [412/1000]  D_loss: 0.6590  G_loss: 8.1720\n",
            "Epoch [413/1000]  D_loss: 0.0228  G_loss: 7.4377\n",
            "Epoch [414/1000]  D_loss: 0.1010  G_loss: 7.5797\n",
            "Epoch [415/1000]  D_loss: 0.1576  G_loss: 8.7273\n",
            "Epoch [416/1000]  D_loss: 0.5393  G_loss: 5.5480\n",
            "Epoch [417/1000]  D_loss: 0.1362  G_loss: 6.1819\n",
            "Epoch [418/1000]  D_loss: 0.3321  G_loss: 7.1592\n",
            "Epoch [419/1000]  D_loss: 0.2690  G_loss: 6.9023\n",
            "Epoch [420/1000]  D_loss: 0.2325  G_loss: 7.4623\n",
            "Epoch [421/1000]  D_loss: 0.0445  G_loss: 9.5431\n",
            "Epoch [422/1000]  D_loss: 0.1143  G_loss: 7.9878\n",
            "Epoch [423/1000]  D_loss: 0.0097  G_loss: 8.4043\n",
            "Epoch [424/1000]  D_loss: 0.3030  G_loss: 6.9059\n",
            "Epoch [425/1000]  D_loss: 0.9376  G_loss: 6.9791\n",
            "Epoch [426/1000]  D_loss: 0.6590  G_loss: 7.3717\n",
            "Epoch [427/1000]  D_loss: 0.0889  G_loss: 7.5356\n",
            "Epoch [428/1000]  D_loss: 0.2568  G_loss: 7.7236\n",
            "Epoch [429/1000]  D_loss: 0.4173  G_loss: 8.7360\n",
            "Epoch [430/1000]  D_loss: 0.1536  G_loss: 8.0226\n",
            "Epoch [431/1000]  D_loss: 0.0225  G_loss: 8.3041\n",
            "Epoch [432/1000]  D_loss: 0.0245  G_loss: 6.3283\n",
            "Epoch [433/1000]  D_loss: 0.0159  G_loss: 8.3355\n",
            "Epoch [434/1000]  D_loss: 0.0268  G_loss: 6.8832\n",
            "Epoch [435/1000]  D_loss: 0.0079  G_loss: 6.0721\n",
            "Epoch [436/1000]  D_loss: 0.6160  G_loss: 10.0829\n",
            "Epoch [437/1000]  D_loss: 0.0514  G_loss: 9.8266\n",
            "Epoch [438/1000]  D_loss: 0.3747  G_loss: 4.7626\n",
            "Epoch [439/1000]  D_loss: 0.2131  G_loss: 7.8682\n",
            "Epoch [440/1000]  D_loss: 0.1863  G_loss: 9.4763\n",
            "Epoch [441/1000]  D_loss: 0.1603  G_loss: 7.8476\n",
            "Epoch [442/1000]  D_loss: 0.1900  G_loss: 7.4455\n",
            "Epoch [443/1000]  D_loss: 0.0016  G_loss: 7.5338\n",
            "Epoch [444/1000]  D_loss: 0.1285  G_loss: 7.5578\n",
            "Epoch [445/1000]  D_loss: 0.1543  G_loss: 6.3239\n",
            "Epoch [446/1000]  D_loss: 0.3875  G_loss: 7.9862\n",
            "Epoch [447/1000]  D_loss: 0.0314  G_loss: 8.3223\n",
            "Epoch [448/1000]  D_loss: 0.0410  G_loss: 7.3539\n",
            "Epoch [449/1000]  D_loss: 0.0078  G_loss: 8.1104\n",
            "Epoch [450/1000]  D_loss: 0.2542  G_loss: 8.2752\n",
            "Epoch [451/1000]  D_loss: 0.0042  G_loss: 8.8254\n",
            "Epoch [452/1000]  D_loss: 0.0105  G_loss: 8.8888\n",
            "Epoch [453/1000]  D_loss: 0.0054  G_loss: 8.6215\n",
            "Epoch [454/1000]  D_loss: 0.2245  G_loss: 7.7694\n",
            "Epoch [455/1000]  D_loss: 0.0405  G_loss: 7.9058\n",
            "Epoch [456/1000]  D_loss: 0.0501  G_loss: 7.6335\n",
            "Epoch [457/1000]  D_loss: 0.0493  G_loss: 10.3941\n",
            "Epoch [458/1000]  D_loss: 0.0171  G_loss: 7.7746\n",
            "Epoch [459/1000]  D_loss: 0.0439  G_loss: 7.0432\n",
            "Epoch [460/1000]  D_loss: 0.2057  G_loss: 8.5135\n",
            "Epoch [461/1000]  D_loss: 0.0011  G_loss: 7.3677\n",
            "Epoch [462/1000]  D_loss: 0.0010  G_loss: 8.9083\n",
            "Epoch [463/1000]  D_loss: 0.0122  G_loss: 8.8681\n",
            "Epoch [464/1000]  D_loss: 0.3277  G_loss: 8.1553\n",
            "Epoch [465/1000]  D_loss: 0.3687  G_loss: 7.8152\n",
            "Epoch [466/1000]  D_loss: 0.0084  G_loss: 9.9896\n",
            "Epoch [467/1000]  D_loss: 0.0890  G_loss: 7.8227\n",
            "Epoch [468/1000]  D_loss: 0.4831  G_loss: 8.5515\n",
            "Epoch [469/1000]  D_loss: 0.0685  G_loss: 9.8187\n",
            "Epoch [470/1000]  D_loss: 0.0335  G_loss: 7.1577\n",
            "Epoch [471/1000]  D_loss: 0.0019  G_loss: 7.4969\n",
            "Epoch [472/1000]  D_loss: 0.0032  G_loss: 7.9392\n",
            "Epoch [473/1000]  D_loss: 0.1444  G_loss: 7.3852\n",
            "Epoch [474/1000]  D_loss: 0.1888  G_loss: 7.8584\n",
            "Epoch [475/1000]  D_loss: 0.0228  G_loss: 7.3132\n",
            "Epoch [476/1000]  D_loss: 0.1743  G_loss: 9.7758\n",
            "Epoch [477/1000]  D_loss: 0.2272  G_loss: 7.1963\n",
            "Epoch [478/1000]  D_loss: 0.0807  G_loss: 9.0771\n",
            "Epoch [479/1000]  D_loss: 0.1315  G_loss: 8.1011\n",
            "Epoch [480/1000]  D_loss: 0.0393  G_loss: 6.9114\n",
            "Epoch [481/1000]  D_loss: 0.1233  G_loss: 8.6791\n",
            "Epoch [482/1000]  D_loss: 0.0405  G_loss: 7.3273\n",
            "Epoch [483/1000]  D_loss: 0.0024  G_loss: 5.9228\n",
            "Epoch [484/1000]  D_loss: 0.1825  G_loss: 9.4697\n",
            "Epoch [485/1000]  D_loss: 0.1681  G_loss: 11.0468\n",
            "Epoch [486/1000]  D_loss: 0.2950  G_loss: 8.3241\n",
            "Epoch [487/1000]  D_loss: 0.3922  G_loss: 8.6121\n",
            "Epoch [488/1000]  D_loss: 0.0193  G_loss: 7.2353\n",
            "Epoch [489/1000]  D_loss: 0.0504  G_loss: 10.1288\n",
            "Epoch [490/1000]  D_loss: 0.0688  G_loss: 8.4741\n",
            "Epoch [491/1000]  D_loss: 0.2242  G_loss: 7.4156\n",
            "Epoch [492/1000]  D_loss: 0.0130  G_loss: 8.8003\n",
            "Epoch [493/1000]  D_loss: 0.0126  G_loss: 10.9592\n",
            "Epoch [494/1000]  D_loss: 0.1833  G_loss: 8.5017\n",
            "Epoch [495/1000]  D_loss: 0.0433  G_loss: 7.9280\n",
            "Epoch [496/1000]  D_loss: 0.1080  G_loss: 8.2417\n",
            "Epoch [497/1000]  D_loss: 0.0063  G_loss: 6.5681\n",
            "Epoch [498/1000]  D_loss: 0.1824  G_loss: 7.3525\n",
            "Epoch [499/1000]  D_loss: 0.0032  G_loss: 7.2482\n",
            "Epoch [500/1000]  D_loss: 0.0055  G_loss: 7.4155\n",
            "Epoch [501/1000]  D_loss: 0.0033  G_loss: 8.9416\n",
            "Epoch [502/1000]  D_loss: 0.0376  G_loss: 7.8692\n",
            "Epoch [503/1000]  D_loss: 0.1360  G_loss: 10.0058\n",
            "Epoch [504/1000]  D_loss: 0.1699  G_loss: 9.3252\n",
            "Epoch [505/1000]  D_loss: 0.0013  G_loss: 9.6196\n",
            "Epoch [506/1000]  D_loss: 0.0011  G_loss: 9.1534\n",
            "Epoch [507/1000]  D_loss: 0.1134  G_loss: 9.7609\n",
            "Epoch [508/1000]  D_loss: 0.1983  G_loss: 5.3866\n",
            "Epoch [509/1000]  D_loss: 0.2151  G_loss: 9.7610\n",
            "Epoch [510/1000]  D_loss: 0.1511  G_loss: 8.1863\n",
            "Epoch [511/1000]  D_loss: 0.0019  G_loss: 7.9889\n",
            "Epoch [512/1000]  D_loss: 0.0060  G_loss: 9.9009\n",
            "Epoch [513/1000]  D_loss: 0.1522  G_loss: 7.0262\n",
            "Epoch [514/1000]  D_loss: 0.1028  G_loss: 8.8387\n",
            "Epoch [515/1000]  D_loss: 0.0044  G_loss: 8.5942\n",
            "Epoch [516/1000]  D_loss: 0.7323  G_loss: 9.2931\n",
            "Epoch [517/1000]  D_loss: 0.0019  G_loss: 9.2445\n",
            "Epoch [518/1000]  D_loss: 0.1020  G_loss: 8.7491\n",
            "Epoch [519/1000]  D_loss: 0.0675  G_loss: 8.4437\n",
            "Epoch [520/1000]  D_loss: 0.0011  G_loss: 9.7816\n",
            "Epoch [521/1000]  D_loss: 0.0700  G_loss: 7.8897\n",
            "Epoch [522/1000]  D_loss: 0.0402  G_loss: 6.2567\n",
            "Epoch [523/1000]  D_loss: 0.0635  G_loss: 8.9721\n",
            "Epoch [524/1000]  D_loss: 0.0094  G_loss: 10.3092\n",
            "Epoch [525/1000]  D_loss: 0.0210  G_loss: 9.4579\n",
            "Epoch [526/1000]  D_loss: 0.1279  G_loss: 8.8889\n",
            "Epoch [527/1000]  D_loss: 0.0675  G_loss: 6.3511\n",
            "Epoch [528/1000]  D_loss: 0.0230  G_loss: 6.9396\n",
            "Epoch [529/1000]  D_loss: 0.2247  G_loss: 10.1512\n",
            "Epoch [530/1000]  D_loss: 0.0177  G_loss: 11.2854\n",
            "Epoch [531/1000]  D_loss: 0.1340  G_loss: 9.6242\n",
            "Epoch [532/1000]  D_loss: 0.0357  G_loss: 6.6672\n",
            "Epoch [533/1000]  D_loss: 0.0323  G_loss: 11.1621\n",
            "Epoch [534/1000]  D_loss: 0.1940  G_loss: 11.4370\n",
            "Epoch [535/1000]  D_loss: 0.0073  G_loss: 8.9791\n",
            "Epoch [536/1000]  D_loss: 0.0104  G_loss: 9.1812\n",
            "Epoch [537/1000]  D_loss: 0.0077  G_loss: 9.6281\n",
            "Epoch [538/1000]  D_loss: 0.0055  G_loss: 10.0995\n",
            "Epoch [539/1000]  D_loss: 0.0227  G_loss: 8.4247\n",
            "Epoch [540/1000]  D_loss: 0.0043  G_loss: 9.7913\n",
            "Epoch [541/1000]  D_loss: 0.0004  G_loss: 10.2155\n",
            "Epoch [542/1000]  D_loss: 0.0039  G_loss: 6.8654\n",
            "Epoch [543/1000]  D_loss: 0.1522  G_loss: 8.0994\n",
            "Epoch [544/1000]  D_loss: 0.0616  G_loss: 8.1348\n",
            "Epoch [545/1000]  D_loss: 0.1457  G_loss: 10.6518\n",
            "Epoch [546/1000]  D_loss: 0.0163  G_loss: 10.3060\n",
            "Epoch [547/1000]  D_loss: 0.6553  G_loss: 9.7725\n",
            "Epoch [548/1000]  D_loss: 0.0985  G_loss: 9.3662\n",
            "Epoch [549/1000]  D_loss: 0.2754  G_loss: 9.3561\n",
            "Epoch [550/1000]  D_loss: 0.0636  G_loss: 8.9426\n",
            "Epoch [551/1000]  D_loss: 0.0005  G_loss: 6.9866\n",
            "Epoch [552/1000]  D_loss: 0.0009  G_loss: 9.9649\n",
            "Epoch [553/1000]  D_loss: 0.2140  G_loss: 10.0911\n",
            "Epoch [554/1000]  D_loss: 0.0308  G_loss: 9.0604\n",
            "Epoch [555/1000]  D_loss: 0.0005  G_loss: 12.4042\n",
            "Epoch [556/1000]  D_loss: 0.0006  G_loss: 11.0732\n",
            "Epoch [557/1000]  D_loss: 0.0507  G_loss: 12.1600\n",
            "Epoch [558/1000]  D_loss: 0.0096  G_loss: 10.7550\n",
            "Epoch [559/1000]  D_loss: 0.1745  G_loss: 8.6816\n",
            "Epoch [560/1000]  D_loss: 0.0003  G_loss: 9.3480\n",
            "Epoch [561/1000]  D_loss: 0.0011  G_loss: 12.4619\n",
            "Epoch [562/1000]  D_loss: 0.0072  G_loss: 9.1626\n",
            "Epoch [563/1000]  D_loss: 0.1346  G_loss: 9.4513\n",
            "Epoch [564/1000]  D_loss: 0.0021  G_loss: 8.4192\n",
            "Epoch [565/1000]  D_loss: 0.0001  G_loss: 10.4398\n",
            "Epoch [566/1000]  D_loss: 0.0484  G_loss: 9.5378\n",
            "Epoch [567/1000]  D_loss: 0.0126  G_loss: 8.7611\n",
            "Epoch [568/1000]  D_loss: 0.3673  G_loss: 11.0710\n",
            "Epoch [569/1000]  D_loss: 0.0083  G_loss: 10.6005\n",
            "Epoch [570/1000]  D_loss: 0.0980  G_loss: 11.3328\n",
            "Epoch [571/1000]  D_loss: 0.0935  G_loss: 7.1857\n",
            "Epoch [572/1000]  D_loss: 0.2158  G_loss: 10.3169\n",
            "Epoch [573/1000]  D_loss: 0.0301  G_loss: 10.5575\n",
            "Epoch [574/1000]  D_loss: 0.0029  G_loss: 11.6965\n",
            "Epoch [575/1000]  D_loss: 0.0006  G_loss: 7.5593\n",
            "Epoch [576/1000]  D_loss: 0.0060  G_loss: 10.3153\n",
            "Epoch [577/1000]  D_loss: 0.0207  G_loss: 9.9468\n",
            "Epoch [578/1000]  D_loss: 0.1040  G_loss: 9.6365\n",
            "Epoch [579/1000]  D_loss: 0.0013  G_loss: 10.2923\n",
            "Epoch [580/1000]  D_loss: 0.0245  G_loss: 10.1296\n",
            "Epoch [581/1000]  D_loss: 0.0220  G_loss: 9.6381\n",
            "Epoch [582/1000]  D_loss: 0.0014  G_loss: 11.7326\n",
            "Epoch [583/1000]  D_loss: 0.0996  G_loss: 8.7160\n",
            "Epoch [584/1000]  D_loss: 0.1189  G_loss: 11.5604\n",
            "Epoch [585/1000]  D_loss: 0.0043  G_loss: 8.8129\n",
            "Epoch [586/1000]  D_loss: 0.0092  G_loss: 10.6895\n",
            "Epoch [587/1000]  D_loss: 0.0009  G_loss: 9.8900\n",
            "Epoch [588/1000]  D_loss: 0.0121  G_loss: 9.7279\n",
            "Epoch [589/1000]  D_loss: 0.0009  G_loss: 11.7005\n",
            "Epoch [590/1000]  D_loss: 0.0126  G_loss: 10.4381\n",
            "Epoch [591/1000]  D_loss: 0.1709  G_loss: 9.7714\n",
            "Epoch [592/1000]  D_loss: 0.0067  G_loss: 9.0485\n",
            "Epoch [593/1000]  D_loss: 0.0149  G_loss: 9.5202\n",
            "Epoch [594/1000]  D_loss: 0.0039  G_loss: 10.7602\n",
            "Epoch [595/1000]  D_loss: 0.0022  G_loss: 10.6063\n",
            "Epoch [596/1000]  D_loss: 0.0006  G_loss: 9.7550\n",
            "Epoch [597/1000]  D_loss: 0.0016  G_loss: 10.9839\n",
            "Epoch [598/1000]  D_loss: 0.0048  G_loss: 9.3453\n",
            "Epoch [599/1000]  D_loss: 0.0002  G_loss: 11.4914\n",
            "Epoch [600/1000]  D_loss: 0.0016  G_loss: 10.9287\n",
            "Epoch [601/1000]  D_loss: 0.0003  G_loss: 9.3717\n",
            "Epoch [602/1000]  D_loss: 0.0117  G_loss: 9.7773\n",
            "Epoch [603/1000]  D_loss: 0.0083  G_loss: 10.3011\n",
            "Epoch [604/1000]  D_loss: 0.1826  G_loss: 10.5109\n",
            "Epoch [605/1000]  D_loss: 0.0205  G_loss: 12.6277\n",
            "Epoch [606/1000]  D_loss: 0.0015  G_loss: 10.7345\n",
            "Epoch [607/1000]  D_loss: 0.0004  G_loss: 9.6841\n",
            "Epoch [608/1000]  D_loss: 0.0047  G_loss: 11.4540\n",
            "Epoch [609/1000]  D_loss: 0.0012  G_loss: 9.0088\n",
            "Epoch [610/1000]  D_loss: 0.0072  G_loss: 10.6469\n",
            "Epoch [611/1000]  D_loss: 0.1063  G_loss: 11.7238\n",
            "Epoch [612/1000]  D_loss: 0.0121  G_loss: 11.5291\n",
            "Epoch [613/1000]  D_loss: 0.0028  G_loss: 9.3173\n",
            "Epoch [614/1000]  D_loss: 0.0264  G_loss: 9.5125\n",
            "Epoch [615/1000]  D_loss: 0.2851  G_loss: 11.8206\n",
            "Epoch [616/1000]  D_loss: 0.0062  G_loss: 9.8946\n",
            "Epoch [617/1000]  D_loss: 0.0871  G_loss: 9.8867\n",
            "Epoch [618/1000]  D_loss: 0.0007  G_loss: 10.2877\n",
            "Epoch [619/1000]  D_loss: 0.0082  G_loss: 9.6652\n",
            "Epoch [620/1000]  D_loss: 0.0075  G_loss: 9.7837\n",
            "Epoch [621/1000]  D_loss: 0.0145  G_loss: 11.8775\n",
            "Epoch [622/1000]  D_loss: 0.0009  G_loss: 7.6484\n",
            "Epoch [623/1000]  D_loss: 0.0481  G_loss: 7.1326\n",
            "Epoch [624/1000]  D_loss: 0.0182  G_loss: 9.0668\n",
            "Epoch [625/1000]  D_loss: 0.1244  G_loss: 11.1936\n",
            "Epoch [626/1000]  D_loss: 0.0033  G_loss: 8.0488\n",
            "Epoch [627/1000]  D_loss: 0.0033  G_loss: 12.5465\n",
            "Epoch [628/1000]  D_loss: 0.0010  G_loss: 10.0467\n",
            "Epoch [629/1000]  D_loss: 0.0038  G_loss: 9.6631\n",
            "Epoch [630/1000]  D_loss: 0.0020  G_loss: 9.5324\n",
            "Epoch [631/1000]  D_loss: 0.0035  G_loss: 9.4545\n",
            "Epoch [632/1000]  D_loss: 0.0002  G_loss: 9.5416\n",
            "Epoch [633/1000]  D_loss: 0.0203  G_loss: 9.7734\n",
            "Epoch [634/1000]  D_loss: 0.0070  G_loss: 10.5300\n",
            "Epoch [635/1000]  D_loss: 0.0001  G_loss: 9.8478\n",
            "Epoch [636/1000]  D_loss: 0.0832  G_loss: 10.0772\n",
            "Epoch [637/1000]  D_loss: 0.0031  G_loss: 10.4021\n",
            "Epoch [638/1000]  D_loss: 0.0046  G_loss: 10.5079\n",
            "Epoch [639/1000]  D_loss: 0.0026  G_loss: 9.1189\n",
            "Epoch [640/1000]  D_loss: 0.0002  G_loss: 7.7141\n",
            "Epoch [641/1000]  D_loss: 0.0694  G_loss: 8.9603\n",
            "Epoch [642/1000]  D_loss: 0.0025  G_loss: 10.5572\n",
            "Epoch [643/1000]  D_loss: 0.0025  G_loss: 10.4745\n",
            "Epoch [644/1000]  D_loss: 0.0083  G_loss: 11.4800\n",
            "Epoch [645/1000]  D_loss: 0.0009  G_loss: 11.0860\n",
            "Epoch [646/1000]  D_loss: 0.0728  G_loss: 9.1215\n",
            "Epoch [647/1000]  D_loss: 0.0071  G_loss: 8.2473\n",
            "Epoch [648/1000]  D_loss: 0.0536  G_loss: 9.2438\n",
            "Epoch [649/1000]  D_loss: 0.0015  G_loss: 10.5335\n",
            "Epoch [650/1000]  D_loss: 0.0033  G_loss: 9.7290\n",
            "Epoch [651/1000]  D_loss: 0.0226  G_loss: 9.7254\n",
            "Epoch [652/1000]  D_loss: 0.0020  G_loss: 9.4082\n",
            "Epoch [653/1000]  D_loss: 0.0001  G_loss: 8.8885\n",
            "Epoch [654/1000]  D_loss: 0.0378  G_loss: 11.0098\n",
            "Epoch [655/1000]  D_loss: 0.0013  G_loss: 10.2243\n",
            "Epoch [656/1000]  D_loss: 0.0029  G_loss: 9.5297\n",
            "Epoch [657/1000]  D_loss: 0.0127  G_loss: 7.4346\n",
            "Epoch [658/1000]  D_loss: 0.0042  G_loss: 10.5015\n",
            "Epoch [659/1000]  D_loss: 0.0875  G_loss: 11.3449\n",
            "Epoch [660/1000]  D_loss: 0.0043  G_loss: 9.7866\n",
            "Epoch [661/1000]  D_loss: 0.0001  G_loss: 8.8131\n",
            "Epoch [662/1000]  D_loss: 0.0112  G_loss: 8.4839\n",
            "Epoch [663/1000]  D_loss: 0.0026  G_loss: 9.9623\n",
            "Epoch [664/1000]  D_loss: 0.0009  G_loss: 9.2416\n",
            "Epoch [665/1000]  D_loss: 0.0676  G_loss: 10.5236\n",
            "Epoch [666/1000]  D_loss: 0.0045  G_loss: 10.5912\n",
            "Epoch [667/1000]  D_loss: 0.0061  G_loss: 11.2198\n",
            "Epoch [668/1000]  D_loss: 0.0866  G_loss: 10.6677\n",
            "Epoch [669/1000]  D_loss: 0.0038  G_loss: 7.4638\n",
            "Epoch [670/1000]  D_loss: 0.0237  G_loss: 7.9997\n",
            "Epoch [671/1000]  D_loss: 0.0051  G_loss: 8.2122\n",
            "Epoch [672/1000]  D_loss: 0.0023  G_loss: 11.5557\n",
            "Epoch [673/1000]  D_loss: 0.0004  G_loss: 8.8376\n",
            "Epoch [674/1000]  D_loss: 0.0010  G_loss: 8.8011\n",
            "Epoch [675/1000]  D_loss: 0.0209  G_loss: 7.9681\n",
            "Epoch [676/1000]  D_loss: 0.0003  G_loss: 8.8393\n",
            "Epoch [677/1000]  D_loss: 0.0110  G_loss: 8.0722\n",
            "Epoch [678/1000]  D_loss: 0.0062  G_loss: 10.9146\n",
            "Epoch [679/1000]  D_loss: 0.0235  G_loss: 9.7893\n",
            "Epoch [680/1000]  D_loss: 0.0920  G_loss: 8.8687\n",
            "Epoch [681/1000]  D_loss: 0.0018  G_loss: 11.2846\n",
            "Epoch [682/1000]  D_loss: 0.0043  G_loss: 10.1724\n",
            "Epoch [683/1000]  D_loss: 0.0077  G_loss: 9.5833\n",
            "Epoch [684/1000]  D_loss: 0.0307  G_loss: 8.3056\n",
            "Epoch [685/1000]  D_loss: 0.0215  G_loss: 7.9252\n",
            "Epoch [686/1000]  D_loss: 0.0008  G_loss: 9.4970\n",
            "Epoch [687/1000]  D_loss: 0.1230  G_loss: 9.0438\n",
            "Epoch [688/1000]  D_loss: 0.0402  G_loss: 10.2248\n",
            "Epoch [689/1000]  D_loss: 0.0361  G_loss: 8.0914\n",
            "Epoch [690/1000]  D_loss: 0.0005  G_loss: 8.9928\n",
            "Epoch [691/1000]  D_loss: 0.0027  G_loss: 8.9189\n",
            "Epoch [692/1000]  D_loss: 0.0008  G_loss: 7.5895\n",
            "Epoch [693/1000]  D_loss: 0.0074  G_loss: 10.5012\n",
            "Epoch [694/1000]  D_loss: 0.0003  G_loss: 10.0195\n",
            "Epoch [695/1000]  D_loss: 0.0031  G_loss: 9.5333\n",
            "Epoch [696/1000]  D_loss: 0.0091  G_loss: 11.8093\n",
            "Epoch [697/1000]  D_loss: 0.0002  G_loss: 9.6117\n",
            "Epoch [698/1000]  D_loss: 0.0047  G_loss: 10.3671\n",
            "Epoch [699/1000]  D_loss: 0.0133  G_loss: 9.7631\n",
            "Epoch [700/1000]  D_loss: 0.0009  G_loss: 8.8883\n",
            "Epoch [701/1000]  D_loss: 0.0019  G_loss: 8.8726\n",
            "Epoch [702/1000]  D_loss: 0.0007  G_loss: 8.6077\n",
            "Epoch [703/1000]  D_loss: 0.0015  G_loss: 9.8026\n",
            "Epoch [704/1000]  D_loss: 0.0019  G_loss: 10.1554\n",
            "Epoch [705/1000]  D_loss: 0.0007  G_loss: 9.5998\n",
            "Epoch [706/1000]  D_loss: 0.0025  G_loss: 9.5625\n",
            "Epoch [707/1000]  D_loss: 0.0018  G_loss: 8.8663\n",
            "Epoch [708/1000]  D_loss: 0.0068  G_loss: 8.2219\n",
            "Epoch [709/1000]  D_loss: 0.0013  G_loss: 9.8433\n",
            "Epoch [710/1000]  D_loss: 0.0213  G_loss: 9.7040\n",
            "Epoch [711/1000]  D_loss: 0.0029  G_loss: 10.3649\n",
            "Epoch [712/1000]  D_loss: 0.0039  G_loss: 8.1126\n",
            "Epoch [713/1000]  D_loss: 0.0003  G_loss: 9.3943\n",
            "Epoch [714/1000]  D_loss: 0.0007  G_loss: 8.8922\n",
            "Epoch [715/1000]  D_loss: 0.0076  G_loss: 8.7395\n",
            "Epoch [716/1000]  D_loss: 0.0003  G_loss: 10.0877\n",
            "Epoch [717/1000]  D_loss: 0.0005  G_loss: 9.6685\n",
            "Epoch [718/1000]  D_loss: 0.0048  G_loss: 9.0426\n",
            "Epoch [719/1000]  D_loss: 0.0003  G_loss: 8.0391\n",
            "Epoch [720/1000]  D_loss: 0.0007  G_loss: 9.1768\n",
            "Epoch [721/1000]  D_loss: 0.0220  G_loss: 9.2628\n",
            "Epoch [722/1000]  D_loss: 0.0151  G_loss: 9.1649\n",
            "Epoch [723/1000]  D_loss: 0.0006  G_loss: 9.4695\n",
            "Epoch [724/1000]  D_loss: 0.0012  G_loss: 8.9929\n",
            "Epoch [725/1000]  D_loss: 0.0250  G_loss: 9.3452\n",
            "Epoch [726/1000]  D_loss: 0.0033  G_loss: 8.2520\n",
            "Epoch [727/1000]  D_loss: 0.0009  G_loss: 7.8561\n",
            "Epoch [728/1000]  D_loss: 0.0017  G_loss: 9.9758\n",
            "Epoch [729/1000]  D_loss: 0.0040  G_loss: 8.3929\n",
            "Epoch [730/1000]  D_loss: 0.0037  G_loss: 8.5253\n",
            "Epoch [731/1000]  D_loss: 0.0048  G_loss: 9.0111\n",
            "Epoch [732/1000]  D_loss: 0.0050  G_loss: 9.1912\n",
            "Epoch [733/1000]  D_loss: 0.0009  G_loss: 9.9868\n",
            "Epoch [734/1000]  D_loss: 0.0097  G_loss: 8.5043\n",
            "Epoch [735/1000]  D_loss: 0.0017  G_loss: 8.0453\n",
            "Epoch [736/1000]  D_loss: 0.0060  G_loss: 9.4609\n",
            "Epoch [737/1000]  D_loss: 0.0004  G_loss: 9.8617\n",
            "Epoch [738/1000]  D_loss: 0.0012  G_loss: 8.9588\n",
            "Epoch [739/1000]  D_loss: 0.0029  G_loss: 8.5299\n",
            "Epoch [740/1000]  D_loss: 0.0069  G_loss: 8.9532\n",
            "Epoch [741/1000]  D_loss: 0.0027  G_loss: 7.8576\n",
            "Epoch [742/1000]  D_loss: 0.0024  G_loss: 9.6432\n",
            "Epoch [743/1000]  D_loss: 0.0116  G_loss: 9.8424\n",
            "Epoch [744/1000]  D_loss: 0.0242  G_loss: 8.4434\n",
            "Epoch [745/1000]  D_loss: 0.0051  G_loss: 9.9543\n",
            "Epoch [746/1000]  D_loss: 0.0030  G_loss: 6.9080\n",
            "Epoch [747/1000]  D_loss: 0.0050  G_loss: 9.3220\n",
            "Epoch [748/1000]  D_loss: 0.0030  G_loss: 8.9947\n",
            "Epoch [749/1000]  D_loss: 0.0091  G_loss: 8.5182\n",
            "Epoch [750/1000]  D_loss: 0.0012  G_loss: 8.1269\n",
            "Epoch [751/1000]  D_loss: 0.0086  G_loss: 8.6608\n",
            "Epoch [752/1000]  D_loss: 0.0046  G_loss: 8.5971\n",
            "Epoch [753/1000]  D_loss: 0.0023  G_loss: 9.4863\n",
            "Epoch [754/1000]  D_loss: 0.0023  G_loss: 8.5511\n",
            "Epoch [755/1000]  D_loss: 0.0006  G_loss: 7.7037\n",
            "Epoch [756/1000]  D_loss: 0.0046  G_loss: 8.6423\n",
            "Epoch [757/1000]  D_loss: 0.0075  G_loss: 8.2656\n",
            "Epoch [758/1000]  D_loss: 0.0013  G_loss: 7.3043\n",
            "Epoch [759/1000]  D_loss: 0.0012  G_loss: 8.2655\n",
            "Epoch [760/1000]  D_loss: 0.0012  G_loss: 7.9474\n",
            "Epoch [761/1000]  D_loss: 0.0028  G_loss: 7.0838\n",
            "Epoch [762/1000]  D_loss: 0.0035  G_loss: 7.9494\n",
            "Epoch [763/1000]  D_loss: 0.0182  G_loss: 7.9711\n",
            "Epoch [764/1000]  D_loss: 0.0067  G_loss: 7.9334\n",
            "Epoch [765/1000]  D_loss: 0.0108  G_loss: 9.0367\n",
            "Epoch [766/1000]  D_loss: 0.0043  G_loss: 7.9332\n",
            "Epoch [767/1000]  D_loss: 0.0044  G_loss: 7.1183\n",
            "Epoch [768/1000]  D_loss: 0.0129  G_loss: 7.7870\n",
            "Epoch [769/1000]  D_loss: 0.0104  G_loss: 9.1263\n",
            "Epoch [770/1000]  D_loss: 0.0062  G_loss: 8.8686\n",
            "Epoch [771/1000]  D_loss: 0.0002  G_loss: 6.7332\n",
            "Epoch [772/1000]  D_loss: 0.0115  G_loss: 6.5290\n",
            "Epoch [773/1000]  D_loss: 0.0017  G_loss: 8.4395\n",
            "Epoch [774/1000]  D_loss: 0.0206  G_loss: 7.9990\n",
            "Epoch [775/1000]  D_loss: 0.0085  G_loss: 8.2296\n",
            "Epoch [776/1000]  D_loss: 0.0027  G_loss: 7.7650\n",
            "Epoch [777/1000]  D_loss: 0.0041  G_loss: 7.8504\n",
            "Epoch [778/1000]  D_loss: 0.0010  G_loss: 7.5229\n",
            "Epoch [779/1000]  D_loss: 0.0146  G_loss: 8.8229\n",
            "Epoch [780/1000]  D_loss: 0.0039  G_loss: 6.7570\n",
            "Epoch [781/1000]  D_loss: 0.0157  G_loss: 7.7273\n",
            "Epoch [782/1000]  D_loss: 0.0025  G_loss: 7.6498\n",
            "Epoch [783/1000]  D_loss: 0.0048  G_loss: 8.2399\n",
            "Epoch [784/1000]  D_loss: 0.0093  G_loss: 7.4488\n",
            "Epoch [785/1000]  D_loss: 0.0027  G_loss: 6.7454\n",
            "Epoch [786/1000]  D_loss: 0.0023  G_loss: 9.1249\n",
            "Epoch [787/1000]  D_loss: 0.0015  G_loss: 7.9122\n",
            "Epoch [788/1000]  D_loss: 0.0083  G_loss: 7.8289\n",
            "Epoch [789/1000]  D_loss: 0.0561  G_loss: 7.9021\n",
            "Epoch [790/1000]  D_loss: 0.0096  G_loss: 9.2301\n",
            "Epoch [791/1000]  D_loss: 0.0025  G_loss: 7.6537\n",
            "Epoch [792/1000]  D_loss: 0.0042  G_loss: 7.6379\n",
            "Epoch [793/1000]  D_loss: 0.0020  G_loss: 7.8010\n",
            "Epoch [794/1000]  D_loss: 0.0051  G_loss: 8.2055\n",
            "Epoch [795/1000]  D_loss: 0.0132  G_loss: 8.5327\n",
            "Epoch [796/1000]  D_loss: 0.0018  G_loss: 8.0673\n",
            "Epoch [797/1000]  D_loss: 0.0032  G_loss: 7.5909\n",
            "Epoch [798/1000]  D_loss: 0.0023  G_loss: 7.5589\n",
            "Epoch [799/1000]  D_loss: 0.0039  G_loss: 7.9330\n",
            "Epoch [800/1000]  D_loss: 0.0124  G_loss: 8.5571\n",
            "Epoch [801/1000]  D_loss: 0.0115  G_loss: 7.5850\n",
            "Epoch [802/1000]  D_loss: 0.0007  G_loss: 7.4534\n",
            "Epoch [803/1000]  D_loss: 0.0039  G_loss: 6.5732\n",
            "Epoch [804/1000]  D_loss: 0.0032  G_loss: 8.4986\n",
            "Epoch [805/1000]  D_loss: 0.0056  G_loss: 8.1503\n",
            "Epoch [806/1000]  D_loss: 0.0029  G_loss: 8.3911\n",
            "Epoch [807/1000]  D_loss: 0.0046  G_loss: 8.3413\n",
            "Epoch [808/1000]  D_loss: 0.0021  G_loss: 7.7177\n",
            "Epoch [809/1000]  D_loss: 0.0007  G_loss: 7.6297\n",
            "Epoch [810/1000]  D_loss: 0.0173  G_loss: 7.2610\n",
            "Epoch [811/1000]  D_loss: 0.0063  G_loss: 8.0539\n",
            "Epoch [812/1000]  D_loss: 0.0021  G_loss: 7.9276\n",
            "Epoch [813/1000]  D_loss: 0.0044  G_loss: 7.8098\n",
            "Epoch [814/1000]  D_loss: 0.0015  G_loss: 8.3500\n",
            "Epoch [815/1000]  D_loss: 0.0103  G_loss: 7.0654\n",
            "Epoch [816/1000]  D_loss: 0.0065  G_loss: 8.1012\n",
            "Epoch [817/1000]  D_loss: 0.0049  G_loss: 7.6791\n",
            "Epoch [818/1000]  D_loss: 0.0066  G_loss: 7.5829\n",
            "Epoch [819/1000]  D_loss: 0.0030  G_loss: 7.6270\n",
            "Epoch [820/1000]  D_loss: 0.0014  G_loss: 8.1285\n",
            "Epoch [821/1000]  D_loss: 0.0017  G_loss: 7.8452\n",
            "Epoch [822/1000]  D_loss: 0.0814  G_loss: 8.5333\n",
            "Epoch [823/1000]  D_loss: 0.0048  G_loss: 6.7877\n",
            "Epoch [824/1000]  D_loss: 0.0054  G_loss: 8.1359\n",
            "Epoch [825/1000]  D_loss: 0.0016  G_loss: 7.3190\n",
            "Epoch [826/1000]  D_loss: 0.0213  G_loss: 7.7151\n",
            "Epoch [827/1000]  D_loss: 0.0013  G_loss: 7.9673\n",
            "Epoch [828/1000]  D_loss: 0.0032  G_loss: 8.1396\n",
            "Epoch [829/1000]  D_loss: 0.0020  G_loss: 7.4766\n",
            "Epoch [830/1000]  D_loss: 0.0043  G_loss: 7.8792\n",
            "Epoch [831/1000]  D_loss: 0.0055  G_loss: 8.2579\n",
            "Epoch [832/1000]  D_loss: 0.0064  G_loss: 6.6484\n",
            "Epoch [833/1000]  D_loss: 0.0019  G_loss: 8.4818\n",
            "Epoch [834/1000]  D_loss: 0.0022  G_loss: 7.5098\n",
            "Epoch [835/1000]  D_loss: 0.0022  G_loss: 7.2927\n",
            "Epoch [836/1000]  D_loss: 0.0042  G_loss: 8.3482\n",
            "Epoch [837/1000]  D_loss: 0.0016  G_loss: 7.7337\n",
            "Epoch [838/1000]  D_loss: 0.0076  G_loss: 7.8001\n",
            "Epoch [839/1000]  D_loss: 0.0020  G_loss: 7.8030\n",
            "Epoch [840/1000]  D_loss: 0.0026  G_loss: 7.5029\n",
            "Epoch [841/1000]  D_loss: 0.0047  G_loss: 7.7198\n",
            "Epoch [842/1000]  D_loss: 0.0011  G_loss: 7.8617\n",
            "Epoch [843/1000]  D_loss: 0.0012  G_loss: 6.8650\n",
            "Epoch [844/1000]  D_loss: 0.0046  G_loss: 7.0004\n",
            "Epoch [845/1000]  D_loss: 0.0039  G_loss: 8.0701\n",
            "Epoch [846/1000]  D_loss: 0.0126  G_loss: 8.0638\n",
            "Epoch [847/1000]  D_loss: 0.0015  G_loss: 7.2269\n",
            "Epoch [848/1000]  D_loss: 0.0014  G_loss: 5.9795\n",
            "Epoch [849/1000]  D_loss: 0.0057  G_loss: 7.6451\n",
            "Epoch [850/1000]  D_loss: 0.0034  G_loss: 7.9074\n",
            "Epoch [851/1000]  D_loss: 0.0075  G_loss: 6.6549\n",
            "Epoch [852/1000]  D_loss: 0.0023  G_loss: 7.1629\n",
            "Epoch [853/1000]  D_loss: 0.0039  G_loss: 7.2469\n",
            "Epoch [854/1000]  D_loss: 0.0073  G_loss: 6.0404\n",
            "Epoch [855/1000]  D_loss: 0.0034  G_loss: 7.7022\n",
            "Epoch [856/1000]  D_loss: 0.0144  G_loss: 7.3331\n",
            "Epoch [857/1000]  D_loss: 0.0107  G_loss: 6.0336\n",
            "Epoch [858/1000]  D_loss: 0.0042  G_loss: 6.7757\n",
            "Epoch [859/1000]  D_loss: 0.0313  G_loss: 7.8776\n",
            "Epoch [860/1000]  D_loss: 0.0024  G_loss: 7.5513\n",
            "Epoch [861/1000]  D_loss: 0.0012  G_loss: 7.2250\n",
            "Epoch [862/1000]  D_loss: 0.0031  G_loss: 6.7210\n",
            "Epoch [863/1000]  D_loss: 0.0024  G_loss: 6.7499\n",
            "Epoch [864/1000]  D_loss: 0.0072  G_loss: 7.3001\n",
            "Epoch [865/1000]  D_loss: 0.0055  G_loss: 7.9353\n",
            "Epoch [866/1000]  D_loss: 0.0062  G_loss: 6.5457\n",
            "Epoch [867/1000]  D_loss: 0.0026  G_loss: 7.9200\n",
            "Epoch [868/1000]  D_loss: 0.0135  G_loss: 8.2834\n",
            "Epoch [869/1000]  D_loss: 0.0035  G_loss: 7.6683\n",
            "Epoch [870/1000]  D_loss: 0.0015  G_loss: 7.5738\n",
            "Epoch [871/1000]  D_loss: 0.0081  G_loss: 7.6586\n",
            "Epoch [872/1000]  D_loss: 0.0072  G_loss: 6.6916\n",
            "Epoch [873/1000]  D_loss: 0.0121  G_loss: 7.5534\n",
            "Epoch [874/1000]  D_loss: 0.0054  G_loss: 7.2025\n",
            "Epoch [875/1000]  D_loss: 0.0056  G_loss: 7.7232\n",
            "Epoch [876/1000]  D_loss: 0.0047  G_loss: 7.7687\n",
            "Epoch [877/1000]  D_loss: 0.0032  G_loss: 7.2450\n",
            "Epoch [878/1000]  D_loss: 0.0057  G_loss: 7.7850\n",
            "Epoch [879/1000]  D_loss: 0.0012  G_loss: 7.8006\n",
            "Epoch [880/1000]  D_loss: 0.0208  G_loss: 7.1646\n",
            "Epoch [881/1000]  D_loss: 0.0014  G_loss: 6.1280\n",
            "Epoch [882/1000]  D_loss: 0.0017  G_loss: 7.3205\n",
            "Epoch [883/1000]  D_loss: 0.0011  G_loss: 8.0590\n",
            "Epoch [884/1000]  D_loss: 0.0013  G_loss: 8.1967\n",
            "Epoch [885/1000]  D_loss: 0.0036  G_loss: 7.6101\n",
            "Epoch [886/1000]  D_loss: 0.0036  G_loss: 7.6121\n",
            "Epoch [887/1000]  D_loss: 0.0033  G_loss: 8.1754\n",
            "Epoch [888/1000]  D_loss: 0.0014  G_loss: 7.2692\n",
            "Epoch [889/1000]  D_loss: 0.0012  G_loss: 7.2535\n",
            "Epoch [890/1000]  D_loss: 0.0013  G_loss: 7.1000\n",
            "Epoch [891/1000]  D_loss: 0.0018  G_loss: 7.4165\n",
            "Epoch [892/1000]  D_loss: 0.0023  G_loss: 6.4222\n",
            "Epoch [893/1000]  D_loss: 0.0049  G_loss: 7.3580\n",
            "Epoch [894/1000]  D_loss: 0.0027  G_loss: 6.7289\n",
            "Epoch [895/1000]  D_loss: 0.0028  G_loss: 7.1408\n",
            "Epoch [896/1000]  D_loss: 0.0022  G_loss: 6.4012\n",
            "Epoch [897/1000]  D_loss: 0.0056  G_loss: 7.7195\n",
            "Epoch [898/1000]  D_loss: 0.0011  G_loss: 7.7580\n",
            "Epoch [899/1000]  D_loss: 0.0059  G_loss: 7.6514\n",
            "Epoch [900/1000]  D_loss: 0.0034  G_loss: 7.1617\n",
            "Epoch [901/1000]  D_loss: 0.0030  G_loss: 7.2726\n",
            "Epoch [902/1000]  D_loss: 0.0138  G_loss: 7.3103\n",
            "Epoch [903/1000]  D_loss: 0.0016  G_loss: 7.7150\n",
            "Epoch [904/1000]  D_loss: 0.0109  G_loss: 7.2241\n",
            "Epoch [905/1000]  D_loss: 0.0028  G_loss: 7.3954\n",
            "Epoch [906/1000]  D_loss: 0.0036  G_loss: 7.0664\n",
            "Epoch [907/1000]  D_loss: 0.0045  G_loss: 7.9761\n",
            "Epoch [908/1000]  D_loss: 0.0020  G_loss: 6.9901\n",
            "Epoch [909/1000]  D_loss: 0.0018  G_loss: 6.8690\n",
            "Epoch [910/1000]  D_loss: 0.0062  G_loss: 7.7227\n",
            "Epoch [911/1000]  D_loss: 0.0009  G_loss: 7.7587\n",
            "Epoch [912/1000]  D_loss: 0.0198  G_loss: 7.4657\n",
            "Epoch [913/1000]  D_loss: 0.0010  G_loss: 7.1306\n",
            "Epoch [914/1000]  D_loss: 0.0019  G_loss: 7.8110\n",
            "Epoch [915/1000]  D_loss: 0.0035  G_loss: 7.5691\n",
            "Epoch [916/1000]  D_loss: 0.0009  G_loss: 7.6807\n",
            "Epoch [917/1000]  D_loss: 0.0019  G_loss: 8.1749\n",
            "Epoch [918/1000]  D_loss: 0.0026  G_loss: 7.7819\n",
            "Epoch [919/1000]  D_loss: 0.0047  G_loss: 7.3091\n",
            "Epoch [920/1000]  D_loss: 0.0104  G_loss: 7.4409\n",
            "Epoch [921/1000]  D_loss: 0.0071  G_loss: 7.2332\n",
            "Epoch [922/1000]  D_loss: 0.0078  G_loss: 7.4030\n",
            "Epoch [923/1000]  D_loss: 0.0034  G_loss: 7.6588\n",
            "Epoch [924/1000]  D_loss: 0.0032  G_loss: 6.9779\n",
            "Epoch [925/1000]  D_loss: 0.0055  G_loss: 7.2522\n",
            "Epoch [926/1000]  D_loss: 0.0010  G_loss: 7.6439\n",
            "Epoch [927/1000]  D_loss: 0.0057  G_loss: 6.6147\n",
            "Epoch [928/1000]  D_loss: 0.0035  G_loss: 6.4127\n",
            "Epoch [929/1000]  D_loss: 0.0061  G_loss: 6.4412\n",
            "Epoch [930/1000]  D_loss: 0.0050  G_loss: 6.8407\n",
            "Epoch [931/1000]  D_loss: 0.0062  G_loss: 6.4231\n",
            "Epoch [932/1000]  D_loss: 0.0024  G_loss: 7.3560\n",
            "Epoch [933/1000]  D_loss: 0.0021  G_loss: 7.8293\n",
            "Epoch [934/1000]  D_loss: 0.0037  G_loss: 6.1603\n",
            "Epoch [935/1000]  D_loss: 0.0044  G_loss: 7.1714\n",
            "Epoch [936/1000]  D_loss: 0.0043  G_loss: 7.7453\n",
            "Epoch [937/1000]  D_loss: 0.0012  G_loss: 7.4052\n",
            "Epoch [938/1000]  D_loss: 0.0023  G_loss: 7.5490\n",
            "Epoch [939/1000]  D_loss: 0.0010  G_loss: 7.7004\n",
            "Epoch [940/1000]  D_loss: 0.0050  G_loss: 6.5539\n",
            "Epoch [941/1000]  D_loss: 0.0020  G_loss: 7.2304\n",
            "Epoch [942/1000]  D_loss: 0.0025  G_loss: 6.5765\n",
            "Epoch [943/1000]  D_loss: 0.0031  G_loss: 7.6025\n",
            "Epoch [944/1000]  D_loss: 0.0015  G_loss: 7.6970\n",
            "Epoch [945/1000]  D_loss: 0.0011  G_loss: 7.1523\n",
            "Epoch [946/1000]  D_loss: 0.0023  G_loss: 6.5313\n",
            "Epoch [947/1000]  D_loss: 0.0028  G_loss: 7.0746\n",
            "Epoch [948/1000]  D_loss: 0.0021  G_loss: 6.7626\n",
            "Epoch [949/1000]  D_loss: 0.0053  G_loss: 7.5254\n",
            "Epoch [950/1000]  D_loss: 0.0071  G_loss: 6.8716\n",
            "Epoch [951/1000]  D_loss: 0.0044  G_loss: 6.3615\n",
            "Epoch [952/1000]  D_loss: 0.0063  G_loss: 6.9120\n",
            "Epoch [953/1000]  D_loss: 0.0026  G_loss: 6.6523\n",
            "Epoch [954/1000]  D_loss: 0.0027  G_loss: 7.4114\n",
            "Epoch [955/1000]  D_loss: 0.0027  G_loss: 6.6398\n",
            "Epoch [956/1000]  D_loss: 0.0042  G_loss: 7.4749\n",
            "Epoch [957/1000]  D_loss: 0.0004  G_loss: 7.3531\n",
            "Epoch [958/1000]  D_loss: 0.0050  G_loss: 6.3364\n",
            "Epoch [959/1000]  D_loss: 0.0179  G_loss: 7.0634\n",
            "Epoch [960/1000]  D_loss: 0.0031  G_loss: 6.8411\n",
            "Epoch [961/1000]  D_loss: 0.0016  G_loss: 7.2413\n",
            "Epoch [962/1000]  D_loss: 0.0053  G_loss: 6.3978\n",
            "Epoch [963/1000]  D_loss: 0.0029  G_loss: 6.7864\n",
            "Epoch [964/1000]  D_loss: 0.0029  G_loss: 6.7199\n",
            "Epoch [965/1000]  D_loss: 0.0123  G_loss: 6.8233\n",
            "Epoch [966/1000]  D_loss: 0.0045  G_loss: 7.3191\n",
            "Epoch [967/1000]  D_loss: 0.0028  G_loss: 7.4398\n",
            "Epoch [968/1000]  D_loss: 0.0026  G_loss: 6.8601\n",
            "Epoch [969/1000]  D_loss: 0.0149  G_loss: 7.5265\n",
            "Epoch [970/1000]  D_loss: 0.0016  G_loss: 6.0774\n",
            "Epoch [971/1000]  D_loss: 0.0016  G_loss: 7.6947\n",
            "Epoch [972/1000]  D_loss: 0.0028  G_loss: 6.6739\n",
            "Epoch [973/1000]  D_loss: 0.0131  G_loss: 7.2394\n",
            "Epoch [974/1000]  D_loss: 0.0026  G_loss: 7.4887\n",
            "Epoch [975/1000]  D_loss: 0.0025  G_loss: 6.6490\n",
            "Epoch [976/1000]  D_loss: 0.0023  G_loss: 6.6365\n",
            "Epoch [977/1000]  D_loss: 0.0012  G_loss: 7.2919\n",
            "Epoch [978/1000]  D_loss: 0.0033  G_loss: 6.7658\n",
            "Epoch [979/1000]  D_loss: 0.0019  G_loss: 6.1939\n",
            "Epoch [980/1000]  D_loss: 0.0172  G_loss: 8.9489\n",
            "Epoch [981/1000]  D_loss: 0.0062  G_loss: 6.5566\n",
            "Epoch [982/1000]  D_loss: 0.0031  G_loss: 6.9708\n",
            "Epoch [983/1000]  D_loss: 0.0024  G_loss: 7.3093\n",
            "Epoch [984/1000]  D_loss: 0.0193  G_loss: 7.3080\n",
            "Epoch [985/1000]  D_loss: 0.0039  G_loss: 7.1408\n",
            "Epoch [986/1000]  D_loss: 0.0008  G_loss: 7.5989\n",
            "Epoch [987/1000]  D_loss: 0.0030  G_loss: 7.5472\n",
            "Epoch [988/1000]  D_loss: 0.0021  G_loss: 7.0134\n",
            "Epoch [989/1000]  D_loss: 0.0012  G_loss: 6.9901\n",
            "Epoch [990/1000]  D_loss: 0.0064  G_loss: 8.0597\n",
            "Epoch [991/1000]  D_loss: 0.0014  G_loss: 7.1179\n",
            "Epoch [992/1000]  D_loss: 0.0030  G_loss: 7.8672\n",
            "Epoch [993/1000]  D_loss: 0.0042  G_loss: 6.6582\n",
            "Epoch [994/1000]  D_loss: 0.0120  G_loss: 6.9064\n",
            "Epoch [995/1000]  D_loss: 0.0012  G_loss: 7.9508\n",
            "Epoch [996/1000]  D_loss: 0.0055  G_loss: 6.8947\n",
            "Epoch [997/1000]  D_loss: 0.0037  G_loss: 7.4405\n",
            "Epoch [998/1000]  D_loss: 0.0018  G_loss: 8.1098\n",
            "Epoch [999/1000]  D_loss: 0.0011  G_loss: 6.6595\n",
            "Epoch [1000/1000]  D_loss: 0.0040  G_loss: 8.3945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and save fake EEG samples after training\n",
        "G.eval()\n",
        "num_samples = 10000  # or however many you want to generate\n",
        "z = torch.randn(num_samples, latent_dim).to(device)\n",
        "with torch.no_grad():\n",
        "    fake_data = G(z).cpu().view(num_samples, 8, 9, 372)  # shape: [10000, 8, 9, 372]\n",
        "\n",
        "# Save each fake sample as .npy file in Google Drive\n",
        "output_folder = \"/content/drive/MyDrive/fake_brain_atlas\"  # Update this path to your Google Drive\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "for i, sample in enumerate(fake_data):\n",
        "    path = os.path.join(output_folder, f\"fake_{i+1:02d}_brain_cognition_atlas.npy\")\n",
        "    np.save(path, sample.numpy())\n",
        "\n",
        "print(f\"Saved {num_samples} fake samples to {output_folder}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEbQLzGkYD6N",
        "outputId": "ea8f8505-6c6a-4967-be5c-431885f6f8d0"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 10000 fake samples to /content/drive/MyDrive/fake_brain_atlas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the path to one of your saved files (make sure the file exists in your Google Drive)\n",
        "file_path = \"/content/drive/MyDrive/fake_brain_atlas/fake_01_brain_cognition_atlas.npy\"  # Update the file name accordingly\n",
        "\n",
        "# Load the file\n",
        "sample_data = np.load(file_path)\n",
        "\n",
        "# Print the shape of the loaded data\n",
        "print(f\"Shape of the loaded sample: {sample_data.shape}\")\n",
        "\n",
        "# Display the first few values (for example)\n",
        "print(\"First few values of the sample:\")\n",
        "print(sample_data[:5, :5, :5])  # Displaying the first 5 values of the 3D array\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc5GF0MdY_2o",
        "outputId": "bfa9d45d-0031-4118-b9cb-c7d920c6c581"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the loaded sample: (8, 9, 372)\n",
            "First few values of the sample:\n",
            "[[[ 0.02337002 -0.74485135  0.59951764  0.7319162  -0.7138317 ]\n",
            "  [ 0.6006163   0.44792652  0.3079675   0.87103516  0.5453192 ]\n",
            "  [ 0.69594955  0.3818473   0.79272574  0.0796722   0.5344146 ]\n",
            "  [-0.12609702  0.9245155  -0.20564018 -0.55361265 -0.05784824]\n",
            "  [-0.12263639  0.4968264  -0.4051038   0.14364415  0.61722773]]\n",
            "\n",
            " [[ 0.23962218 -0.35407376  0.37675914 -0.38762096 -0.11929367]\n",
            "  [-0.6127381  -0.86815757  0.8488765   0.3805332   0.6294726 ]\n",
            "  [ 0.51687825 -0.19811913  0.05990627 -0.25976738 -0.77655506]\n",
            "  [-0.352736    0.5396191  -0.19128254  0.61011094 -0.45948824]\n",
            "  [ 0.4546485   0.5552575   0.47844374 -0.43137166 -0.6790498 ]]\n",
            "\n",
            " [[ 0.41270518 -0.45639747  0.02706678 -0.02496133  0.5300721 ]\n",
            "  [-0.16690736 -0.94558924 -0.5946794  -0.0040481  -0.92751086]\n",
            "  [ 0.22139382  0.35483187  0.33131617  0.21906167 -0.76221585]\n",
            "  [ 0.6992738   0.64433545  0.8776825   0.18944556 -0.06385395]\n",
            "  [-0.6944372  -0.30361846  0.51492167  0.37298664 -0.922677  ]]\n",
            "\n",
            " [[-0.36264655 -0.4271248  -0.7263274  -0.15940847 -0.5762827 ]\n",
            "  [-0.05036669 -0.67381746 -0.04865398 -0.06299338  0.6787569 ]\n",
            "  [ 0.5927718   0.5197344   0.28588217 -0.3329826   0.45803538]\n",
            "  [ 0.4304883   0.3904301  -0.41919264 -0.44844857 -0.7927123 ]\n",
            "  [-0.21750171 -0.09482962  0.4843408  -0.73249155  0.0867186 ]]\n",
            "\n",
            " [[-0.3304169   0.26492712  0.5376054  -0.02391265  0.42085317]\n",
            "  [-0.10655858  0.18275476  0.06765119  0.05011407  0.73310894]\n",
            "  [ 0.17629008 -0.5323917  -0.33935446  0.81748676  0.33849376]\n",
            "  [-0.254601    0.6184371   0.04973941 -0.59609675  0.50575686]\n",
            "  [ 0.73803884 -0.18456097  0.6177832  -0.38899595  0.3320843 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load one of the files to inspect\n",
        "sample_data = np.load(\"/content/drive/MyDrive/Brain_Cognition_Atlas/1_20160518_Brain_Cognition_Atlas.npy\")\n",
        "\n",
        "print(f\"Data shape: {sample_data.shape}\")\n",
        "print(f\"Data type: {type(sample_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weJSYU1SZnKH",
        "outputId": "29fbdb1f-e9b9-4865-a3f0-83c6dc2e5a54"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (8, 9, 372)\n",
            "Data type: <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Constants\n",
        "mspca_dir = '/content/drive/MyDrive/MSPCA_outputs'\n",
        "features_output_dir = '/content/drive/MyDrive/SODP_SDC_features'\n",
        "labels_output_dir = '/content/drive/MyDrive/SODP_SDC_labels'\n",
        "os.makedirs(features_output_dir, exist_ok=True)\n",
        "os.makedirs(labels_output_dir, exist_ok=True)\n",
        "\n",
        "def compute_sodp(eeg_signal):\n",
        "    \"\"\"Compute x(n) = EEG(n+1) - EEG(n), y(n) = EEG(n+2) - EEG(n+1)\"\"\"\n",
        "    x = eeg_signal[1:-1] - eeg_signal[:-2]\n",
        "    y = eeg_signal[2:] - eeg_signal[1:-1]\n",
        "    return x, y\n",
        "\n",
        "def compute_sdc(x, y):\n",
        "    \"\"\"Compute SDC as sqrt(x^2 + y^2) and return total sum\"\"\"\n",
        "    return np.sum(np.sqrt(x**2 + y**2))\n",
        "\n",
        "# Iterate over all MSPCA files\n",
        "for file in os.listdir(mspca_dir):\n",
        "    if not file.endswith('_MSPCA.npy'):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(mspca_dir, file)\n",
        "    data = np.load(file_path, allow_pickle=True).item()\n",
        "\n",
        "    sodp_sdc_features = []\n",
        "    labels = data.get('labels', None)  # Extract the labels (assuming they are under the 'labels' key)\n",
        "\n",
        "    # Check if labels exist, otherwise skip the file\n",
        "    if labels is None:\n",
        "        print(f\"Skipping {file}: No labels found\")\n",
        "        continue\n",
        "\n",
        "    # Iterate through the EEG data and compute SODP and SDC for each channel\n",
        "    for key, eeg_data in sorted(data.items()):\n",
        "        # Skip 'labels' key, as it's not EEG data\n",
        "        if key == 'labels':\n",
        "            continue\n",
        "\n",
        "        # eeg_data: Check if it's a 1D or 2D array (signal or windows of signals)\n",
        "        if isinstance(eeg_data, np.ndarray):\n",
        "            if eeg_data.ndim == 1:\n",
        "                # Single signal (1D array)\n",
        "                x, y = compute_sodp(eeg_data)\n",
        "                sdc = compute_sdc(x, y)\n",
        "                sodp_sdc_features.append(sdc)\n",
        "\n",
        "            elif eeg_data.ndim == 2:\n",
        "                # Multiple windows of signals (2D array)\n",
        "                for window in eeg_data:\n",
        "                    x, y = compute_sodp(window)\n",
        "                    sdc = compute_sdc(x, y)\n",
        "                    sodp_sdc_features.append(sdc)\n",
        "        else:\n",
        "            print(f\"Skipping {key}: Expected NumPy array, found {type(eeg_data)}\")\n",
        "\n",
        "    # Convert to array: shape = (num_samples,) → SDC values\n",
        "    features_array = np.array(sodp_sdc_features)\n",
        "\n",
        "    # Save features and labels in separate files\n",
        "    features_save_path = os.path.join(features_output_dir, file.replace('_MSPCA.npy', '_SODP_SDC_features.npy'))\n",
        "    labels_save_path = os.path.join(labels_output_dir, file.replace('_MSPCA.npy', '_labels.npy'))\n",
        "\n",
        "    np.save(features_save_path, features_array)\n",
        "    np.save(labels_save_path, labels)\n",
        "\n",
        "    print(f\"Saved features for {file} → {features_save_path}\")\n",
        "    print(f\"Saved labels for {file} → {labels_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99sHSdF6btX9",
        "outputId": "9522b6e2-6717-4401-ffd2-6af003e6f604"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved features for 1_20160518_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/1_20160518_SODP_SDC_features.npy\n",
            "Saved labels for 1_20160518_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/1_20160518_labels.npy\n",
            "Saved features for 14_20151205_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/14_20151205_SODP_SDC_features.npy\n",
            "Saved labels for 14_20151205_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/14_20151205_labels.npy\n",
            "Saved features for 5_20160406_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/5_20160406_SODP_SDC_features.npy\n",
            "Saved labels for 5_20160406_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/5_20160406_labels.npy\n",
            "Saved features for 6_20150507_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/6_20150507_SODP_SDC_features.npy\n",
            "Saved labels for 6_20150507_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/6_20150507_labels.npy\n",
            "Saved features for 8_20151103_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/8_20151103_SODP_SDC_features.npy\n",
            "Saved labels for 8_20151103_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/8_20151103_labels.npy\n",
            "Saved features for 13_20151115_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/13_20151115_SODP_SDC_features.npy\n",
            "Saved labels for 13_20151115_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/13_20151115_labels.npy\n",
            "Saved features for 2_20150915_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/2_20150915_SODP_SDC_features.npy\n",
            "Saved labels for 2_20150915_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/2_20150915_labels.npy\n",
            "Saved features for 4_20151111_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/4_20151111_SODP_SDC_features.npy\n",
            "Saved labels for 4_20151111_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/4_20151111_labels.npy\n",
            "Saved features for 11_20150916_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/11_20150916_SODP_SDC_features.npy\n",
            "Saved labels for 11_20150916_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/11_20150916_labels.npy\n",
            "Saved features for 10_20151014_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/10_20151014_SODP_SDC_features.npy\n",
            "Saved labels for 10_20151014_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/10_20151014_labels.npy\n",
            "Saved features for 15_20150508_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/15_20150508_SODP_SDC_features.npy\n",
            "Saved labels for 15_20150508_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/15_20150508_labels.npy\n",
            "Saved features for 9_20151028_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/9_20151028_SODP_SDC_features.npy\n",
            "Saved labels for 9_20151028_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/9_20151028_labels.npy\n",
            "Saved features for 12_20150725_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/12_20150725_SODP_SDC_features.npy\n",
            "Saved labels for 12_20150725_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/12_20150725_labels.npy\n",
            "Saved features for 3_20150919_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/3_20150919_SODP_SDC_features.npy\n",
            "Saved labels for 3_20150919_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/3_20150919_labels.npy\n",
            "Saved features for 7_20150715_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_features/7_20150715_SODP_SDC_features.npy\n",
            "Saved labels for 7_20150715_MSPCA.npy → /content/drive/MyDrive/SODP_SDC_labels/7_20150715_labels.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2  # For bilinear interpolation\n",
        "\n",
        "# Constants\n",
        "features_dir = '/content/drive/MyDrive/SODP_SDC_features'\n",
        "output_dir = '/content/drive/MyDrive/Brain_Cognition_Atlas(new)'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Constants for image dimensions\n",
        "height = 8\n",
        "width = 9\n",
        "grid_size = height * width  # Total number of cells in the 8x9 grid\n",
        "\n",
        "def bilinear_interpolate(image, target_shape=(8, 9)):\n",
        "    \"\"\"Apply bilinear interpolation to resize the image to target shape.\"\"\"\n",
        "    return cv2.resize(image, target_shape, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "def create_8x9_image(features):\n",
        "    \"\"\"Project the SODP-SDC features onto an 8x9 image.\"\"\"\n",
        "    # Reshape the features to fit the 8x9 grid\n",
        "    image = np.zeros((height, width))  # Initialize an empty image with zeros (padding)\n",
        "\n",
        "    # Check if features is a scalar, and wrap it in a list if it is\n",
        "    if isinstance(features, np.float64) or isinstance(features, float):\n",
        "        features = [features]  # Convert scalar to list for processing\n",
        "\n",
        "    # Ensure the number of features fits within the grid size (8x9 = 72)\n",
        "    features = features[:grid_size]  # Truncate extra features if there are more than 72\n",
        "\n",
        "    # Map the feature to the image grid. Assuming simple linear mapping for illustration.\n",
        "    for i in range(len(features)):\n",
        "        row = i // width  # Mapping to 8 rows\n",
        "        col = i % width   # Mapping to 9 columns\n",
        "        image[row, col] = features[i]\n",
        "\n",
        "    # Apply bilinear interpolation to ensure smooth transitions between missing values\n",
        "    return bilinear_interpolate(image)\n",
        "\n",
        "def process_file(file_path):\n",
        "    \"\"\"Process a single SODP-SDC feature file to create the brain-cognition atlas.\"\"\"\n",
        "    # Load features\n",
        "    features = np.load(file_path)  # Load SODP-SDC features for a given file\n",
        "\n",
        "    # Create a list to store the 8x9 images\n",
        "    image_stack = []\n",
        "\n",
        "    # Check if features are multidimensional (time windows)\n",
        "    if features.ndim == 1:\n",
        "        features = [features]  # Wrap in list if only one time window is present\n",
        "\n",
        "    # For each time window, project features to 8x9 image\n",
        "    for time_window_features in features:\n",
        "        image = create_8x9_image(time_window_features)\n",
        "        image_stack.append(image)\n",
        "\n",
        "    # Stack all the 8x9 images to form a 3D array (stack of time windows)\n",
        "    image_stack = np.stack(image_stack, axis=-1)\n",
        "\n",
        "    # Save the resulting 3D brain-cognition atlas\n",
        "    output_path = os.path.join(output_dir, os.path.basename(file_path).replace('SODP_SDC_features.npy', 'brain_cognition_atlas.npy'))\n",
        "    np.save(output_path, image_stack)\n",
        "    print(f\"Saved brain cognition atlas for {file_path} → {output_path}\")\n",
        "\n",
        "# Iterate through all feature files and process them\n",
        "for file in os.listdir(features_dir):\n",
        "    if file.endswith('_SODP_SDC_features.npy'):\n",
        "        file_path = os.path.join(features_dir, file)\n",
        "        process_file(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Z92MoYf7aw",
        "outputId": "ef052eb2-3746-4a0c-c0d1-534716e56e67"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/1_20160518_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/1_20160518_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/14_20151205_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/14_20151205_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/5_20160406_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/5_20160406_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/6_20150507_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/6_20150507_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/8_20151103_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/8_20151103_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/13_20151115_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/13_20151115_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/2_20150915_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/2_20150915_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/4_20151111_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/4_20151111_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/11_20150916_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/11_20150916_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/10_20151014_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/10_20151014_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/15_20150508_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/15_20150508_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/9_20151028_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/9_20151028_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/12_20150725_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/12_20150725_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/3_20150919_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/3_20150919_brain_cognition_atlas.npy\n",
            "Saved brain cognition atlas for /content/drive/MyDrive/SODP_SDC_features/7_20150715_SODP_SDC_features.npy → /content/drive/MyDrive/Brain_Cognition_Atlas(new)/7_20150715_brain_cognition_atlas.npy\n"
          ]
        }
      ]
    }
  ]
}