{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanuja1708/EEG-emotions/blob/main/latest_with_ui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXBncFvInhqD",
        "outputId": "ff10b107-fd45-4937-bbc0-014e64ca1215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0KRequirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Collecting pykalman\n",
            "  Downloading pykalman-0.10.1-py2.py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Collecting scikit-base<0.13.0 (from pykalman)\n",
            "  Downloading scikit_base-0.12.2-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Downloading pykalman-0.10.1-py2.py3-none-any.whl (248 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.5/248.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_base-0.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-base, pykalman, flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25 pykalman-0.10.1 scikit-base-0.12.2\n",
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (2.0.2)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install flask\n",
        "!npm install -g localtunnel\n",
        "!pip install flask flask-ngrok tensorflow joblib pykalman scipy\n",
        "!pip install PyWavelets\n",
        "\n",
        "from flask import Flask, jsonify\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "from scipy.signal import butter, filtfilt\n",
        "from pykalman import KalmanFilter\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "# Setup for your EEG signal processing\n",
        "sampling_rate_original = 2500\n",
        "sampling_rate_final = 200\n",
        "freq_bands = {\n",
        "    \"Delta\": (1, 4),\n",
        "    \"Theta\": (4, 8),\n",
        "    \"Alpha\": (8, 14),\n",
        "    \"Beta\": (14, 31),\n",
        "    \"Gamma\": (31, 50)\n",
        "}\n",
        "kf = KalmanFilter(\n",
        "    transition_matrices=[1],\n",
        "    observation_matrices=[1],\n",
        "    transition_covariance=0.2 * np.eye(1),\n",
        "    observation_covariance=0.5 * np.eye(1),\n",
        "    initial_state_mean=[20],\n",
        "    initial_state_covariance=5 * np.eye(1)\n",
        ")\n",
        "\n",
        "def bandpass_filter(data, lowcut, highcut, fs):\n",
        "    nyq = 0.5 * fs\n",
        "    b, a = butter(4, [lowcut / nyq, highcut / nyq], btype='band')\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def downsample(signal, original_fs, target_fs):\n",
        "    factor = int(original_fs / target_fs)\n",
        "    return signal[::factor]\n",
        "\n",
        "def generate_eeg():\n",
        "    t = np.linspace(0, 1, sampling_rate_original)\n",
        "    eeg = []\n",
        "    for _ in range(62):\n",
        "        alpha = np.random.uniform(40, 80) * np.sin(2 * np.pi * np.random.uniform(8, 13) * t)\n",
        "        beta = np.random.uniform(15, 35) * np.sin(2 * np.pi * np.random.uniform(14, 30) * t)\n",
        "        delta = np.random.uniform(1, 5) * np.sin(2 * np.pi * np.random.uniform(1, 3) * t)\n",
        "        theta = np.random.uniform(1, 5) * np.sin(2 * np.pi * np.random.uniform(4, 7) * t)\n",
        "        gamma = np.random.uniform(1, 5) * np.sin(2 * np.pi * np.random.uniform(31, 50) * t)\n",
        "        noise = np.random.normal(0, 3, t.shape)\n",
        "        signal = delta + theta + alpha + beta + gamma + noise\n",
        "        eeg.append(np.clip(signal, -70, 70))\n",
        "    return np.array(eeg)\n",
        "\n",
        "def extract_de_lds():\n",
        "    eeg_data = generate_eeg()\n",
        "    de_feats = []\n",
        "    for ch in eeg_data:\n",
        "        ch = downsample(ch, sampling_rate_original, sampling_rate_final)\n",
        "        band_feats = []\n",
        "        for _, (low, high) in freq_bands.items():\n",
        "            filtered = bandpass_filter(ch, low, high, sampling_rate_final)\n",
        "            var = np.var(filtered)\n",
        "            de = 0.5 * np.log(2 * np.pi * np.e * var + 1e-8)\n",
        "            band_feats.append(de)\n",
        "        smoothed, _ = kf.filter(np.array(band_feats).reshape(-1, 1))\n",
        "        de_feats.append(smoothed.flatten())\n",
        "    de_feats = np.array(de_feats)\n",
        "    de_min, de_max = np.min(de_feats), np.max(de_feats)\n",
        "    scaled = 15 + (de_feats - de_min) / (de_max - de_min + 1e-8) * (27 - 15)\n",
        "    return scaled.flatten()\n",
        "\n",
        "@app.route('/predict', methods=['GET'])\n",
        "def predict():\n",
        "    sample = extract_de_lds().reshape((1, 1, 310))\n",
        "    model = load_model('/content/eeg_emotion_gan_model.h5')\n",
        "    encoder = joblib.load('/content/eeg_label_encoder.pkl')\n",
        "    prediction = model.predict(sample)\n",
        "\n",
        "    # print(f\"Model Prediction: {prediction}\")\n",
        "\n",
        "    label = encoder.inverse_transform(np.argmax(prediction, axis=1))[0]\n",
        "    print(f\"Predicted Emotion: {label}\")\n",
        "    return jsonify({'emotion': label})\n",
        "\n",
        "# Run LocalTunnel to expose the Flask API with a custom subdomain for static URL\n",
        "def run_localtunnel():\n",
        "    subdomain = \"eegemotion\"  # Replace with your desired subdomain\n",
        "    process = subprocess.Popen(['lt', '--port', '5000', '-s', subdomain], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    for line in process.stdout:\n",
        "        print(line.decode('utf-8').strip())  # Show the URL output from LocalTunnel\n",
        "\n",
        "# Start the LocalTunnel process and Flask app\n",
        "if __name__ == \"__main__\":\n",
        "    from threading import Thread\n",
        "    # Run LocalTunnel in a separate thread\n",
        "    thread = Thread(target=run_localtunnel)\n",
        "    thread.start()\n",
        "    app.run(port=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34hRORVYny2D",
        "outputId": "c2380293-395f-4f53-d3e9-f18e9cc3f9aa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "your url is: https://eegemotion.loca.lt\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7881d0083740> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 16:21:42] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Emotion: Anger\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7881d0163920> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 16:24:20] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Emotion: Happy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 16:24:27] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Emotion: Sad\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 16:25:31] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Emotion: Disgust\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 16:25:37] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Emotion: Sad\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 16:26:48] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Emotion: Anger\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 16:33:57] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Emotion: Anger\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 16:34:48] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Emotion: Neutral\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 16:34:54] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Emotion: Fear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "# Setup for your EEG signal processing\n",
        "sampling_rate_original = 2500\n",
        "sampling_rate_final = 200\n",
        "freq_bands = {\n",
        "    \"Delta\": (1, 4),\n",
        "    \"Theta\": (4, 8),\n",
        "    \"Alpha\": (8, 14),\n",
        "    \"Beta\": (14, 31),\n",
        "    \"Gamma\": (31, 50)\n",
        "}\n",
        "kf = KalmanFilter(\n",
        "    transition_matrices=[1],\n",
        "    observation_matrices=[1],\n",
        "    transition_covariance=0.2 * np.eye(1),\n",
        "    observation_covariance=0.5 * np.eye(1),\n",
        "    initial_state_mean=[20],\n",
        "    initial_state_covariance=5 * np.eye(1)\n",
        ")\n",
        "\n",
        "def bandpass_filter(data, lowcut, highcut, fs):\n",
        "    nyq = 0.5 * fs\n",
        "    b, a = butter(4, [lowcut / nyq, highcut / nyq], btype='band')\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def downsample(signal, original_fs, target_fs):\n",
        "    factor = int(original_fs / target_fs)\n",
        "    return signal[::factor]\n",
        "\n",
        "def generate_eeg():\n",
        "    t = np.linspace(0, 1, sampling_rate_original)\n",
        "    eeg = []\n",
        "    for _ in range(62):\n",
        "        alpha = np.random.uniform(40, 80) * np.sin(2 * np.pi * np.random.uniform(8, 13) * t)\n",
        "        beta = np.random.uniform(15, 35) * np.sin(2 * np.pi * np.random.uniform(14, 30) * t)\n",
        "        delta = np.random.uniform(1, 5) * np.sin(2 * np.pi * np.random.uniform(1, 3) * t)\n",
        "        theta = np.random.uniform(1, 5) * np.sin(2 * np.pi * np.random.uniform(4, 7) * t)\n",
        "        gamma = np.random.uniform(1, 5) * np.sin(2 * np.pi * np.random.uniform(31, 50) * t)\n",
        "        noise = np.random.normal(0, 3, t.shape)\n",
        "        signal = delta + theta + alpha + beta + gamma + noise\n",
        "        eeg.append(np.clip(signal, -70, 70))\n",
        "    return np.array(eeg)\n",
        "\n",
        "def extract_de_lds(eeg_data):\n",
        "    de_feats = []\n",
        "    for ch in eeg_data:\n",
        "        ch = downsample(ch, sampling_rate_original, sampling_rate_final)\n",
        "            eeg_data = [bandpass_filter(ch, 1, 50, sampling_rate_final) for ch in eeg_data]\n",
        "\n",
        "        band_feats = []\n",
        "        for _, (low, high) in freq_bands.items():\n",
        "            filtered = bandpass_filter(ch, low, high, sampling_rate_final)\n",
        "            var = np.var(filtered)\n",
        "            de = 0.5 * np.log(2 * np.pi * np.e * var + 1e-8)\n",
        "            band_feats.append(de)\n",
        "        smoothed, _ = kf.filter(np.array(band_feats).reshape(-1, 1))\n",
        "        de_feats.append(smoothed.flatten())\n",
        "    de_feats = np.array(de_feats)\n",
        "    de_min, de_max = np.min(de_feats), np.max(de_feats)\n",
        "    scaled = 15 + (de_feats - de_min) / (de_max - de_min + 1e-8) * (27 - 15)\n",
        "    return scaled.flatten()\n",
        "\n",
        "@app.route('/predict', methods=['GET'])\n",
        "def predict():\n",
        "    eeg_data = generate_eeg()  # Generate EEG data for emotion prediction\n",
        "    sample = extract_de_lds(eeg_data).reshape((1, 1, 310))\n",
        "\n",
        "    model = load_model('/content/eeg_emotion_gan_model.h5')\n",
        "    encoder = joblib.load('/content/eeg_label_encoder.pkl')\n",
        "    prediction = model.predict(sample)\n",
        "\n",
        "    label = encoder.inverse_transform(np.argmax(prediction, axis=1))[0]\n",
        "    print(f\"Predicted Emotion: {label}\")\n",
        "\n",
        "    # Send both emotion prediction and EEG data to front-end\n",
        "    return jsonify({\n",
        "        'emotion': label,\n",
        "        'eeg_data': eeg_data.tolist()  # Send the raw EEG data to display as a waveform\n",
        "    })\n",
        "\n",
        "# Run LocalTunnel to expose the Flask API with a custom subdomain for static URL\n",
        "def run_localtunnel():\n",
        "    subdomain = \"eegemotion\"  # Replace with your desired subdomain\n",
        "    process = subprocess.Popen(['lt', '--port', '5000', '-s', subdomain], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    for line in process.stdout:\n",
        "        print(line.decode('utf-8').strip())  # Show the URL output from LocalTunnel\n",
        "\n",
        "# Start the LocalTunnel process and Flask app\n",
        "if __name__ == \"__main__\":\n",
        "    from threading import Thread\n",
        "    # Run LocalTunnel in a separate thread\n",
        "    thread = Thread(target=run_localtunnel)\n",
        "    thread.start()\n",
        "    app.run(port=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYaJNynYQcdT",
        "outputId": "fcd2f3c8-8363-4a43-8494-cffab6db1a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://eegemotion.loca.lt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 20:37:18] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: Fear\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/May/2025 20:50:07] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: Sad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with mspca and welch\n",
        "\n",
        "from flask import Flask, jsonify\n",
        "import numpy as np\n",
        "from scipy.signal import butter, filtfilt, welch\n",
        "from pykalman import KalmanFilter\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "import pywt\n",
        "from sklearn.decomposition import PCA\n",
        "import subprocess\n",
        "from threading import Thread\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# --- EEG Config ---\n",
        "sampling_rate_original = 2500\n",
        "sampling_rate_final = 200\n",
        "segment_length = sampling_rate_final * 1\n",
        "\n",
        "freq_bands = {\n",
        "    \"Delta\": (1, 4),\n",
        "    \"Theta\": (4, 8),\n",
        "    \"Alpha\": (8, 14),\n",
        "    \"Beta\": (14, 31),\n",
        "    \"Gamma\": (31, 50)\n",
        "}\n",
        "\n",
        "# --- Kalman Filter Setup ---\n",
        "kf = KalmanFilter(\n",
        "    transition_matrices=[1],\n",
        "    observation_matrices=[1],\n",
        "    transition_covariance=0.2 * np.eye(1),\n",
        "    observation_covariance=0.5 * np.eye(1),\n",
        "    initial_state_mean=[20],\n",
        "    initial_state_covariance=5 * np.eye(1)\n",
        ")\n",
        "\n",
        "# --- Filtering & Downsampling ---\n",
        "def bandpass_filter(data, lowcut, highcut, fs):\n",
        "    nyq = 0.5 * fs\n",
        "    b, a = butter(4, [lowcut / nyq, highcut / nyq], btype='band')\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def downsample(signal, original_fs, target_fs):\n",
        "    factor = int(original_fs / target_fs)\n",
        "    return signal[::factor]\n",
        "\n",
        "# --- EEG Generator ---\n",
        "def generate_eeg():\n",
        "    t = np.linspace(0, 1, sampling_rate_original)\n",
        "    eeg = []\n",
        "    for _ in range(62):\n",
        "        alpha = np.random.uniform(40, 80) * np.sin(2 * np.pi * np.random.uniform(8, 13) * t)\n",
        "        beta = np.random.uniform(15, 35) * np.sin(2 * np.pi * np.random.uniform(14, 30) * t)\n",
        "        delta = np.random.uniform(1, 5) * np.sin(2 * np.pi * np.random.uniform(1, 3) * t)\n",
        "        theta = np.random.uniform(1, 5) * np.sin(2 * np.pi * np.random.uniform(4, 7) * t)\n",
        "        gamma = np.random.uniform(1, 5) * np.sin(2 * np.pi * np.random.uniform(31, 50) * t)\n",
        "        noise = np.random.normal(0, 3, t.shape)\n",
        "        signal = delta + theta + alpha + beta + gamma + noise\n",
        "        eeg.append(np.clip(signal, -70, 70))\n",
        "    return np.array(eeg)\n",
        "\n",
        "# --- MSPCA Denoising ---\n",
        "def mspca_denoise(eeg_data, wavelet='db4', level=4):\n",
        "    denoised_eeg = []\n",
        "    for channel in eeg_data:\n",
        "        coeffs = pywt.wavedec(channel, wavelet, level=level)\n",
        "        denoised_coeffs = []\n",
        "        for c in coeffs:\n",
        "            if len(c) > 1:\n",
        "                c = c.reshape(-1, 1)\n",
        "                pca = PCA(n_components=1)\n",
        "                transformed = pca.fit_transform(c)\n",
        "                reconstructed = pca.inverse_transform(transformed).flatten()\n",
        "                denoised_coeffs.append(reconstructed)\n",
        "            else:\n",
        "                denoised_coeffs.append(c)\n",
        "        denoised_signal = pywt.waverec(denoised_coeffs, wavelet)\n",
        "        denoised_eeg.append(denoised_signal[:len(channel)])\n",
        "    return np.array(denoised_eeg)\n",
        "\n",
        "# --- DE_LDS with Welch & MSPCA ---\n",
        "def extract_de_lds(eeg_data):\n",
        "    eeg_data = [downsample(ch, sampling_rate_original, sampling_rate_final) for ch in eeg_data]\n",
        "    eeg_data = [bandpass_filter(ch, 1, 50, sampling_rate_final) for ch in eeg_data]\n",
        "    eeg_data = mspca_denoise(np.array(eeg_data))\n",
        "\n",
        "    de_features = []\n",
        "    for ch_data in eeg_data:\n",
        "        freqs, psd = welch(ch_data, fs=sampling_rate_final, nperseg=min(200, segment_length))\n",
        "        band_de = []\n",
        "        for _, (low, high) in freq_bands.items():\n",
        "            band_power = psd[(freqs >= low) & (freqs <= high)]\n",
        "            band_de_value = 0.5 * np.log(2 * np.pi * np.e * (np.mean(band_power) + 1e-6))\n",
        "            band_de.append(band_de_value)\n",
        "        smoothed_de, _ = kf.filter(np.array(band_de).reshape(-1, 1))\n",
        "        de_features.append(smoothed_de.flatten())\n",
        "    de_features = np.array(de_features)\n",
        "\n",
        "    # Normalize\n",
        "    de_min, de_max = np.min(de_features), np.max(de_features)\n",
        "    scaled = 15 + (de_features - de_min) / (de_max - de_min + 1e-8) * (27 - 15)\n",
        "    return scaled.flatten()\n",
        "\n",
        "# --- API Route ---\n",
        "@app.route('/predict', methods=['GET'])\n",
        "def predict():\n",
        "    eeg_data = generate_eeg()\n",
        "    sample = extract_de_lds(eeg_data).reshape((1, 1, 310))\n",
        "\n",
        "    model = load_model('/content/eeg_emotion_gan_model.h5')\n",
        "    encoder = joblib.load('/content/eeg_label_encoder.pkl')\n",
        "    prediction = model.predict(sample)\n",
        "    label = encoder.inverse_transform(np.argmax(prediction, axis=1))[0]\n",
        "\n",
        "    print(f\"Predicted Emotion: {label}\")\n",
        "    return jsonify({'emotion': label, 'eeg_data': eeg_data.tolist()})\n",
        "\n",
        "# --- LocalTunnel Setup ---\n",
        "def run_localtunnel():\n",
        "    subdomain = \"eegemotion\"\n",
        "    process = subprocess.Popen(['lt', '--port', '5000', '-s', subdomain], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    for line in process.stdout:\n",
        "        print(line.decode('utf-8').strip())\n",
        "\n",
        "# --- Run App ---\n",
        "if __name__ == \"__main__\":\n",
        "    from threading import Thread\n",
        "    # Run LocalTunnel in a separate thread\n",
        "    thread = Thread(target=run_localtunnel)\n",
        "    thread.start()\n",
        "    app.run(port=5000)"
      ],
      "metadata": {
        "id": "sqvjgwizJYUh",
        "outputId": "883cbb54-541a-4248-d652-1c73b4bb193b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://eegemotion.loca.lt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 695ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 15:41:43] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: Fear\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 15:45:36] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: Sad\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 15:51:40] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: Sad\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 500ms/step\n",
            "Predicted Emotion: Happy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 15:52:35] \"GET /predict HTTP/1.1\" 200 -\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b99987df380> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 15:53:08] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: Anger\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b99a0130900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 16:03:02] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: Anger\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [12/May/2025 16:04:19] \"GET /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: Anger\n"
          ]
        }
      ]
    }
  ]
}